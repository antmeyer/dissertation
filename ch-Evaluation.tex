\chapter{DUAL-PARADIGM EVALUATION}
\label{ch:eval}

\section{Introduction}

%\paragraph{Evaluation.} 
Unsupervised learning systems like Multimorph are inherently difficult to evaluate. 
%This is because, supervised learning, the correct answers--e.g., the correct
%categories for the input examples are basically unknown. They are not supplied to the learner, at any rate. 
%This is because an unsupervised learning system is not provided with explicit learning targets.
Arguably, the main advantage of unsupervised learning is that it can discover novel and potentially useful categories and associations \citep{parsons:2004}. On the other hand, it is very difficult to evaluate previously unknown categories according to precision and recall. % and other measures that rely ``answer keys." 
The previously unknown categories in the present study were to be morphological 
categories that were perhaps going to embody in some way Aronoff's notion of the morphome \citep{aronoff:1994}.  But we did not know the precise form these categories would take. 

%---recall from REF that we use the term morph to avoid suggesting equivalence with he were unknown because we had  units were not conventional morphemes 
%or morphosyntactic categories. 

%Instead, MCMM-generated clusters corresponded roughly to Aronoff's 
%\emph{morphomes} \citep{aronoff:1994}, 
which can be described as 
\emph{pre-morphosyntactic} units, i.e., units that have been assembled from 
phonemes, but have not yet been assigned 
a syntactic or semantic meaning. I use the term \emph{morph} to refer to such units in what follows, since it is a less loaded term than either
either \emph{morpheme} or \emph{morphome}.
%\emph{morphome}, since MCMM-generated clusters may not correspond 
%precisely to morphomes in every case (see section REF). %~\ref{sec:targets}).

Thus, the evaluation itself presents an important research question, namely the question 
of how to evaluate the output of an unsupervised morphological clustering algorithm, 
particularly one that considers only features of \emph{word-internal form}, having no 
access to morphosyntactic features that exist outside of the word, e.g., person, gender, and 
number of surrounding words.

%Multimorph is an unsupervised machine learning system, which makes it 
%intrinsically difficult to evaluate, and thus no single 
%evaluation method is likely to be perfect. 

I thus devised a dual-paradigm method for evaluating Multimorph's output, i.e., an approach consisting of two complementary methods, %I would thus rather not rely on a single method.
%not only because is it an unsupervised learning system, which are notoriously challenging to evaluate,  it learns unconventional morphological units, namely morphs. Indeed, it would be very difficult to come up with a gold standard for the morphs because we do not know what the morphs are supposed to look like; i.e., the ``right answers" are not obvious at the outset. 
%In particular, I will use 
one \emph{intrinsic} and one \emph{extrinsic}. We shall discuss these two components in what follows.

Finally, the evaluation also included a \emph{qualitative} component, which served to complement the quantitative 
%utput of the MCMM was evaluated \emph{qualitatively}. The  which is intended to complement the quantitative
 intrinsic an extrinsic methods.


\section{Qualitative Evaluation}
The qualitative component of the evaluation consisted mainly in directly (or ``manually") examining the clusters generated by Multimorph's MCMM generated. These clusters are words grouped together according to some shared elements of form. These shared elements are often immediately obvious to the human eye, but even in such cases they can become diluted or lost in automated quantitative assessment.   %shared elements of form are immediately obvious to the human eye, but they can easily become lost in automated 
%A qualitative componentmponent is advantageous is that the automaticl,kl quantative procedures may not catch everything. 
%Humans also have linguistic intuition, and since we are dealing with linguistic data, which can be very useful in interpreting a cluster. Since we are dealing with unsupervised learning, there is basically no telling what the clusters will come to represent. The qualitative component is meant to address this essentially lack of predictability.

\section{Quantitative Evaluation}
\subsection{Intrinsic Evaluation}
An \emph{intrinsic} evaluation regards a system as a stand-alone system and its output as an end unto itself. An intrinsic evaluation thus judges a system's output directly as opposed to indirectly via the performance of a downstream system. However, intrinsic evaluations still require some \emph{externally-generated} standard against which to measure a system's performance. Such a standard is called a \emph{gold standard} and generally consists of a set of ``correct answers'' that humans have had some part in creating.
%That is, an intrinsic evaluation thus evaluates a system directly, on its on terms; any other system is irrelevant. 
%disregards 
% as independent and isolated from other systems, examining its 
%output directly. That is, the system is evaluated as a stand-alone application, on its
%on terms, as it were,
%not as one embedded in a larger system.

The present study's intrinsic evaluation component thus required a \emph{gold-standard categorization} against which to evaluate Multimorph's clusterings,\footnote{Note the use of the word {categorization} instead of \emph{clustering} in the term \emph{gold-standard categorization}. 
By convention, gold-standard clusters are not actually called clusters, but rather 
but (gold-standard) \emph{categories}}, and thus measure the precision and recall of Multimorph clusterings. 
%We thus compare system-generated clusterings to a gold-standard categorization.}

In supervised learning, the training process requires that each learning example (or data point) by a target label, i.e., the ``correct answer,'' as it were. %old-standard (or target) category. Supervised systems need the target labels to learn.
 These target labels later serve as the gold-standard labels evaluating the trained model. In supervised learning, therefore, it is trivial to obtain a gold-standard for evaluation; if a model was trained at all, then the gold standard already exists. By contrast, in unsupervised learning, obtaining a gold-standard dataset can be much less straightforward. One problem, as mentioned above, is that unsupervised learning is by nature open-ended; it is supposed result in the discovery of categories and relationships.  

Another difficulty (where obtaining a gold standard is concerned) is the 
form-centric nature of Multimorph's learning domain. Multimorph learns morphs, 
which are autonomous morphological categories, as discussed in chapter~\ref{autonomous}. 
They mediate between phonology and morphosyntax, but are at the same time independent 
of both of these levels of structure. It is thus not always clear what the morphs are in a 
given data set. In As intermediate units that are unconstrained by meaning, they tend 
to be whatever they need to be in order to bridge the gap between phonological units 
and meaning (syntax and semantics).  There are thus not many morphomic lexicons 
or corpora wherein autonomous morphological units are annotated. There are no 
such resources for Hebrew.

There is in fact no ideal choice for a gold-standard dataset for Multimorph, 
not at the present time, at any rate. One of motivating factors behind this 
thesis is that not much is currently know about the autonomous morphologies 
of particular languages, and that unsupervised learning can contribute to the 
process of discovering autonomous morphological categories. At the same time, 
however, this means that gold-standard datasets are not going to be forthcoming. 
I chose to include this intrinsic evaluation component largely as means 
to supplement and support the highly experimental extrinsic evaluation 
component.  It was an exercise in maximizing the utility of less-than-ideal data, data that was not really
suitable to the present study.

As described in chapter~\ref{ch:experi}, the source of Multimorph's input words was the 
Berman Longitudinal Corpus. Each word in this corpus is both (quasi-)phonetically transcribed as well as morphologically annotated, as described in chapter~\ref{ch:experi} (see in particular section~\ref{sec:morph-annotation}).
%i.e., labeled with morphological categories. 
%Each word thus comes with a 
%morphological analysis consisting of one or more morphological categories (see \ref{sec:morph-annotation}).
The first step in 
building the gold-standard data was to extract each word's morphological analysis 
or analyses (section~\ref{sec:extr}). 

\begin{figure}[t]
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
hes\a'{i}gu \> pos:v\&root:nsg\&ptn:hifil\&tense:past\&pers:3\&gen:unsp\&num:pl \\
hev\a'{a}nt \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:2\&gen:fm\&num:sg \\
hev\a'{a}nti \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:1\&gen:unsp\&num:sg \\
hev\a'{e}\textipa{P}nu \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:1\&gen:unsp\&num:pl \\
hev\a'{e}\textipa{P}t \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:2\&gen:fm\&num:sg \\
hev\a'{e}\textipa{P}ta \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:2\&gen:ms\&num:sg \\
hev\a'{e}\textipa{P}tem \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:2\&gen:ms\&num:pl \\
hev\a'{e}\textipa{P}ti \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:1\&gen:unsp\&num:sg \\
hev\a'{i}n \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:3\&gen:ms\&num:sg \\
hev\a'{i}na \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:3\&gen:fm\&num:sg \\
\vdots \> \vdots \\
\end{tabbing}
\caption{Morphological analyses extracted from the Berman Longitudinal Corpus.}
\label{fig:analyses}
\end{mdframed}
\end{figure}

%I stated in chapter~\ref{autonomous}
%The intrinsic component of this study's evaluation compares an MCMM-generated clustering to a 
%\emph{gold-standard categorization},  computing its precision and recall according to the extent to which it matches the gold-standard categorization. 

An MCMM's output consists of 
%Given the final valuations in the MCMM matrices $\mathbf{M}$ and $\mathbf{C}$,
the matrices $\mathbf{M}$ and $\mathbf{C}$. As described in chapter~\ref{sec:MCMM}, the $\mathbf{M}$ contains a cluster-membership vector for each data point (i.e., word), while the matrix $\mathbf{C}$ contains each cluster's average vector, i.e., a vector whose active features are the active features across all of the cluster's members have in common. Put another way, a cluster is a set of data points, each of which can be described as a set of active features. Though this set differs from data point to data point, even with a cluster, co-members share some active features; that is, there is a non-empty intersection.  A cluster's centroid is the intersection of the active feature sets among a cluster's members.
(See the example in section~\ref{subsec:example}.) 

A cluster's centroid thus concisely expresses the relationship between the cluster's members, which is, ultimately, the cluster's very meaning. Cluster centroids can therefore be used as cluster labels. In our case, of course, a centroid is a vector consisting of $J$ feature activities, each in the interval $[0,1]$. The $J$ features in centroids correspond to the $J$ features in the data points, and each activity is of course a number, particular one in the interval $[0,1]$. 
A number of steps are necessary to convert a numerical, multi-dimensional centroid vector to a unit of morphological structure. This process is described in section REF.

%        self.clusters_justWords = []
%        self.Rv = Rv
%        self.Mv = Mv
%        self.Cv = Cv
%        cdef INT i, k, j, n
%        cdef INT I = self.Rv.shape[0]
%        cdef INT K = self.Mv.shape[1]
%        cdef INT J = self.Cv.shape[0]
%        #cdef object clusters_m, clusters_mr, clusters_mc, clusters_mcr
%        #clusters_m = [[] for k in range(K)]
%        #clusters_mr = [[] for k in range(K)]
%        clusters_mc = [[] for k in range(K)]
%        #clusters_mcr = [[] for k in range(K)]
%        self.clusters_justWords = [[] for k in range(K)]
%        cdef bint membership = 0
%        cdef object word_and_val
%        cdef FLOAT mc = 0.0
%        # "wordList" is a list of the text-formatted words corresponding to the datapoints (feature vectors).
%        # the activations matrix M contains I rows (# of data points) and K columns (# of clusters).
%        # the k-th column in M corresponds to the k-th column in C.
%        for i in range(I):
%            # Iterate over the row M[i], which is a row of k elements
%            # Each k represents a cluster
%            for k in range(K):
%                # for each index k, find the kth column in the
%                # J x K matrix C.
%                # Then proceed down the j row indices in this column
%                # until a [j,k] cell is found that meets the membership
%                # criteria. Only one such cell is needed.
%
%                # if self.Mv[i,k] >= thresh:
%                #     word_and_val = (wordList[i], "{:.4f}".format(self.Mv[i,k]))
%                #     if word_and_val in clusters_m[k]:
%                #         pass
%                #     else:
%                #         clusters_mcr[k].append(word_and_val)
%                #     if wordList[i] in self.clusters_justWords[k]:
%                #         pass
%                #     else:
%                #         self.clusters_justWords[k].append(wordList[i])
%
%                # if self.Mv[i,k] >= thresh:
%                #     for j in range(J):
%                #         if (self.Rv[i,j] >= 0.5):
%                #             membership = 1
%                #             break
%                # if membership == 1:
%                #     membership = 0
%                #     word_and_val = (wordList[i], "{:.4f}".format(self.Mv[i,k]))
%                #     if word_and_val in clusters_mr[k]:
%                #         pass
%                #     else:
%                #         clusters_mr[k].append(word_and_val)
%                #     if wordList[i] in self.clusters_justWords[k]:
%                #         pass
%                #     else:
%                #         self.clusters_justWords[k].append(wordList[i])
%
%                mc = 0.0
%                for $ \gets j in range(J):
%                    mc = self.Mv[i,k]*self.Cv[j,k]
%                    if (mc >= thresh):
%                        membership = 1
%                        break
%                if membership == 1:
%                    membership = 0
%                    word_and_val = (wordList[i], "{:.4f}".format(mc))
%                    if word_and_val in clusters_mc[k]:
%                        pass
%                    else:
%                        clusters_mc[k].append(word_and_val)
%                    if wordList[i] in self.clusters_justWords[k]:
%                        pass
%                    else:
%                        self.clusters_justWords[k].append(wordList[i])
\begin{algorithm}
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead 
\KwIn{$\mathbf{M}$,  $\mathbf{C}$, input words $\mathbf{w}$, \text{threshold} $\theta$, $I$ $J$,$K$} 
\KwOut{$\mathbf{U}$, a list of $K$ cluster member lists, i.e., $\{\mathbf{u}_k$ for $k \in K\}$, where each $\mathbf{u}_k$ is the list of the $k$th cluster's (active) member words}
%
%belong to that cluster.
%$\text{clusters} \gets \big[\,[\,\,]_0, [\,\,]_1, [\,\,]_2, \,\dots, \, [\,\,]_K \,\big]$\;
Initialize cluster member lists: $\mathbf{U} = \{\mathbf{u}_0, \mathbf{u}_1, \mathbf{u}_2,\ldots, \mathbf{u}_K\}$ where each $\mathbf{u}_k \gets \emptyset$\;
\For {$ i \gets 0$ \textbf{to} $I$}{
	%$w \gets \text{words}[i]$\;
	\For {$ k  \gets 0$ \textbf{to} $K$}{
	%$\text{Clusters}[k] \gets \emptyset_k$\;
	%$\text{vote}_{k,j} \gets 0$\;
	$j \gets 0$\;
	%$\text{threshold-met} \gets \textbf{False}$\;
		%\# Make sure there is at least one $c_{j,k}$ such that $m_{i,k}c_{j,k}\geq \theta$,\;
		%\# where $\text{vote}_{k,j} = m_{i,k}c_{j,k}$\;
		%\While {$m_{i,k}c_{j,k} < \theta$ \textbf{and} $j < \textsc{Number-of-Rows}(\textbf{C})$}{
		\While {$m_{i,k}c_{j,k} < \theta$ \textbf{and} $j < J$}{  \label{line:vote-thresh}
		%\While {$m_{i,k}c_{j,k} < \theta$ \textbf{and} $j < \textsc{length}({\textbf{C}_{(\bullet,k)})$}{
		%{$\text{vote}_{k,j} < \theta$ \textbf{and} $j < \textsc{Number-of-Rows}(\textbf{C})$}{  
			%$\text{vote}_{k,j} \gets m_{i,k}c_{j,k}$\;
			$j \gets j + 1$\;
			%$\text{threshold-met} \gets \textbf{True}$\;
		}
	%\If{$\text{threshold-met}$}{
	\If{$m_{i,k}c_{j,k} \geq \theta$}{
		%$\text{threshold-met} \gets \textbf{False}$\;
		%$\text{Clusters}[k] \gets \text{Clusters}[k] \cup w_i$\;
		$\mathbf{u}_k \gets \mathbf{u}_k \cup w_i$\; \label{line:append}
	}
	}
}
\Return $\mathbf{U}$\;
\caption{\textsc{Clusters-and-Members}}
\label{alg:members}
\end{algorithm}

The purpose of the while loop beginning on line~\ref{line:vote-thresh} is to impose the following constraint on cluster membership: In order for a word to be a member of a given cluster, the cluster must be responsible for at least one active surface unit in the word's reconstruction vector.
 Recall from chapter~\ref{ch:MCMM} that \emph{votes} in an MCMM are products of the form $m_{i,k}c_{j,k}$, and that the Noisy-OR mixing function requires at least one `\textsc{yes}' vote in order to activate a given reconstruction unit. For Multimorph's purposes, a vote is treated as a `\textsc{yes}' vote as long as it satisfies the criterion 
 \begin{equation}\label{eq:vote-criterion}
 m_{i,k}c_{j,k} \geq \theta_{\text{mc}}
 \end{equation}
 where $\theta_{\text{mc}}$ is a threshold. 
 The while loop on \ref{line:vote-thresh} always begins with a particular $k$ index supplied 
 by the loop within which it is nested. We will call this $k$ the index of the current cluster. 
 The while loop proceeds to increment the index $j$ and thus descend the cells of the ($k$th) 
 column in $\mathbf{C}$ (i.e., the current cluster's centroid vector). Each new $j$ corresponds 
 to a different cell, i.e., a different  $c_{j,k}$, and thus a different vote $m_{i,k}c_{j,k}$. 
 The procedure visits each cell in turn until it finds a `\textsc{yes}' vote, i.e., a $c_{j,k}$ 
 such that the vote $m_{i,k}c_{j,k}$ exceeds the threshold $\theta_{\text{mc}}$. The 
 procedure only needs to find one `\textsc{yes}' to verify that there is at least one \emph{active}
 surface unit associated with the current cluster. Thus, the while loop terminates upon finding 
 such a vote, whereupon
the current ($i$th) word is appended to the active members of the current cluster (line~\ref{line:append}). 
The procedure iterates through the entire input wordlist and appends words to clusters member lists 
whenever criterion \eqref{eq:vote-criterion} is satisfied. It outputs each cluster's list of active member words.
 
 
% \theta$ $1$ (or close to $1$) for at least one $k \in K$ in order for $r_{i,j}$ to be $1$ (or close to $1$). The constraint imposed on line~\ref{line:vote-thresh} thus ensures that 
%\begin{algorithm}
%\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead 
%\KwIn{A set $C = \{c_1, c_2, \ldots, c_r\}$ of denominations of coins, where $c_i > c_2 > \ldots > c_r$ and a positive number $n$}
%\KwOut{A list of coins $d_1,d_2,\ldots,d_k$, such that $\sum_{i=1}^k d_i = n$ and $k$ is minimized}
%$C \gets \emptyset$\;
%\For{$i \gets 1$ \textbf{to} $r$}{
%  \While{$n \geq c_i$} {
%    $C \gets C \cup \{c_i\}$\;
%    $n \gets n - c_i$\;
%  }
%}
%\Return{$C$}\;
%\caption{{\sc Change} Makes change using the smallest number of coins}
%\label{algo:change}
%\end{algorithm}

In this way, the $\mathbf{M}$ and $\mathbf{C}$ matrices together provide all the information necessary 
to derive a list of word-to-cluster mappings: The $\mathbf{M}$ 
matrix tells us the indices of the clusters to which a given data point (or word) 
belongs, and the $\mathbf{C}$ matrix provides a meaningful cluster label for each 
cluster index. Each word-to-cluster mapping thus 
consists of a word followed by a list of cluster labels.  
as in ``runs: run, -s,"  where \textit{runs} is the word, and \textit{run} and \textit{-s} 
are the labels of the clusters to which \textit{runs} belongs. These labels also represent 
morphs. That is, e.g., \emph{-s} represents a cluster whose words end with the morph 
\emph{-s}. 

%To evaluate a list of mappings like ``runs: run, -s," we need an analogous 
%gold-standard list, one that supplies, for each word, a list of gold-standard \emph{categories}.
%This is to say that we need a gold-standard morphological analysis for each word. 
%We needed different gold standard lists for the orthographic and transcribed datasets, 
%since these datasets come from different sources, and their respective gold standards must 
%be obtained through separate means.

\subsection{Gold Standard Categories}

\subsubsection{Verbs} 

\begin{figure}[tb!]
 \begin{center}
 %\footnotesize
 \setlength{\extrarowheight}{8pt}
 \begin{tabular}{lcccc}
 \toprule
    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
    & \textit{Sg} & \textit{Pl} & \textit{Sg} & \textit{Pl} \\
\cmidrule{2-3}  \cmidrule{4-5}
   \textit{1.m/f} & hi-tx\'{i}l-ti & hitx\'{i}l-nu & \textipa{P}etx\'{i}l & na-tx\'{i}l\\
   \hline
   \textit{2.m} &  hitx\'{i}l-ta & hitx\'{i}l-tem  &  ta-tx\'{i}l & \\
   \textit{2.f} & hitx\'{i}l-t & hitx\'{i}l-ten &  ta-txil\'{i} & \raisebox{1.5ex}[0pt]{ta-txil-\'{u}}\\
   \hline
   \textit{3.m} & hi-tx\'{i}l &   &  ya-tx\'{i}l &\\
   \textit{3.f} & hi-txil-\'{a} & \raisebox{1.5ex}[0pt]{hi-txil-\'{u}} &  ta-tx\'{i}l & \raisebox{1.5ex}[0pt]{ya-txil-\'{u}} \\
   \bottomrule
 \end{tabular}
 \caption{Past and future-tense paradigms for the root \textit{t.x.l} in the \textit{Hif'il} binyan.}
 \label{fig:paradigms}
 \end{center}
 \end{figure}
 
% \begin{figure}[tb!]
% \begin{center}
% %\footnotesize
% \begin{tabular}{lcccc}
% \toprule
%    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
%    \cmidrule{2-3}  \cmidrule{4-5}
%    & \textit{Sg.} & \textit{Pl.} & \textit{Sg.} & \textit{Pl.} \\
%\cmidrule{2-3}  \cmidrule{4-5}
%   \textit{1.m/f} & gam\'ar-ti & garm\'ar-nu & \textipa{P}etx\'{i}l & na-tx\'{i}l\\
%   \hline
%   \textit{2.m} & gam\'ar-ta & gam\'ar-tem  &  tigm\'{o}r & \\
%   \textit{2.f} & \raisebox{1.5ex}[0pt]{htxil-t} & htxil-tn &  ta-txil\'{i} & \raisebox{1.5ex}[0pt]{ta-txil-\'{u}\\
%   \hline
%   \textit{3.m} & gam\'ar &   &  yi-gm\'{o}r & yi-gmer\'{u} \\
%   %\cline{1-2}
%   \textit{3.f} & gamr-\'{a} & \raisebox{1.5ex}[0pt]{gamr-\'{u}} &  ta-tx\'{i}l & \raisebox{1.5ex}[0pt]{ya-txil-\'{u}} \\
%   \bottomrule
% \end{tabular}
% \caption{Past and future-tense paradigms for the root \textit{t.x.l} in the \textit{Hif'il} binyan.}
% \label{fig:paradigms}
% \end{center}
% \end{figure}
 The BLC uses the traditional conjugational categories for verbs, i.e., categories corresponding to the cells in a conjugation paradigm.
 % %For every cell in figure~\ref{fig:paradigms}, there is a 
% \textsc{mila-ma} represents verbal inflections for person, number, and gender in %composite 
% \texttt{person/gender/number} (or \texttt{PGN}) features taking values like \texttt{3p/F/Sg} and \texttt{2p/M/Sg}. However, there is not always a one-to-one correspondence between such category labels and the actual distinctions in form. Consider, for example, the verb \textit{ttxil} `she/you will begin'. In the absence of context, this form is entirely ambiguous; it can be either \textsc{2.m.sg} or \textsc{3.f.sg}. \textsc{mila-ma} produces a separate analysis for each possibility, giving rise to two distinct \texttt{PGN} categories, namely \texttt{PGN:2/M/Sg} and \texttt{PGN:3/F/Sg}, where only one form exists. This is similar to the problem of negative and unmarked categories: an MCCM has no means of making a distinction where none exists in the feature representations.

\texttt{fut\%(2\%M)|(23\%F)}
\texttt{hifil\%prefix\_stem}
\texttt{past\%2\%M\%Sg}
\texttt{fut\%1\%Sg}
\texttt{qal\%prefix\_stem}
\texttt{nifal\%suffix\_stem}
\texttt{fut\%3\%M}
\texttt{past\%3\%F\%Sg}
\texttt{fut\%3\%M}
\texttt{past\%2\%M\%Sg}
\texttt{past\%3\%Pl}
\texttt{past\%2\%F\%Sg}
\texttt{past\%1\%Sg}
\texttt{past\%2\%F\%Sg}
\texttt{(past\%3\%Pl)|(fut\%23\%Pl)}

      \begin{table}[h!]
      \small
      \begin{center}
      \begin{tabular}{ccc}
      %\multicolumn{3}{c}{\textit{mzkir}  `he reminds'} \\
      \toprule
       & \texttt{sg} & \texttt{pl} \\
      \midrule
      \texttt{m} & ma-zk\'{i}r & ma-zkir-\'{i}m \\
      \hline
      \texttt{f} & ma-zk\'{i}r-\'{a} & ma-zkir-\'{o}t \\
      \bottomrule
      \end{tabular}
      \end{center}
      \label{tab:hifil-part}
      \caption{Inflections of {\textit{mzkir}  `he reminds'}}
    \end{table}
    


%A_{m,n} = 
% \begin{pmatrix}
%  a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
%  a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
%  \vdots  & \vdots  & \ddots & \vdots  \\
%  a_{m,1} & a_{m,2} & \cdots & a_{m,n} 
% \end{pmatrix}
 
 
\subsubsection{Nominals}

Figure~
\begin{figure}{ht!}
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
pas \> pos:n\&gen:ms\&num:sg\&stat:u\\
pas\a'{e}y \> pos:n\&gen:ms\&num:pl\&stat:cstr pos:n\&gen:ms\&num:pl\&stat:cstr\\
pas\a'{i}m \> pos:n\&\&gen:ms\&num:pl\&stat:abs\\
%qi\v{s}q\a'{u}\v{s} \> pos:n\&gen:ms\&num:sg\&stat:u\\
%qi\v{s}u\textipa{P}\a'{i}m \> pos:n\&root:q\v{s}	extipa{P}\&ptn:CiCuC\&gen:ms\&num:pl\&stat:abs\\
%qi\v{s}u\UTF{1E6D}\a'{i}m \> pos:n\&root:q\v{s}\UTF{1E6D}\&ptn:CiCuC\&gen:ms\&num:pl\&stat:abs\\
%qi\v{s}\a'{u}\textipa{P} \> pos:n\&root:q\v{s}\textipa{P}\&ptn:CiCuC\&gen:ms\&num:sg\&stat:u\\
%qi\v{s}\a'{u}\UTF{1E6D} \> pos:n\&root:q\v{s}\UTF{1E6D}\&ptn:CiCuC\&gen:ms\&num:sg\&stat:u\\
qvuc\a'{a} \> pos:n\&root:qbc\&ptn:CCuCa\&gen:fm\&num:sg\&stat:u\\
qvuc\a'{o}t \> pos:n\&root:qbc\&ptn:CCuCa\&gen:fm\&num:pl\&stat:u \\
zriq\a'{a} \> pos:n\&root:zrq\&ptn:CCiCa\&gen:fm\&num:sg\&stat:u \\
zriq\a'{a}t	 \> pos:n\&root:zrq\&ptn:CCiCa\&gen:fm\&num:sg\&stat:cstr \\
%qlaf \> pos:n\&root:qlp\&ptn:CCaC\&gen:ms\&num:sg\&stat:u\\
%qlaf\a'{i}m \> pos:n\&root:qlp\&ptn:CCaC\&gen:ms\&num:pl\&stat:abs\\
%siman\a'{i}m \> pos:n\&root:sym\&ptn:CiCan\&gen:ms\&num:pl\&stat:abs\\
%sim\a'{a}n \> pos:n\&root:sym\&ptn:CiCan\&gen:ms\&num:sg\&stat:u\\
%sin\a'{a}r \> pos:n\&root:snr\&ptn:CiCaC\&gen:ms\&num:sg\&stat:u\\
%sin\a'{o}r \> pos:n\&root:snr\&ptn:CiCoC\&gen:ms\&num:sg\&stat:u\\
%sipr\a'{a} \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:3\&gen:fm\&num:sg\\
%sipr\a'{u} \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:3\&gen:unsp\&num:pl\\
%sipur\a'{e}y \> pos:n\&root:spr\&ptn:CiCuC\&gen:ms\&num:pl\&stat:cstr\\
%sipur\a'{i}m \> pos:n\&root:spr\&ptn:CiCuC\&gen:ms\&num:pl\&stat:abs\\
%sip\a'{a}rnu \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:1\&gen:unsp\&num:pl\\
%sip\a'{a}rt \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:2\&gen:fm\&num:sg\\
%sip\a'{a}rta \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:2\&gen:ms\&num:sg\\
%sip\a'{a}rti \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:1\&gen:unsp\&num:sg\\
%sip\a'{e}r \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:3\&gen:ms\&num:sg\\
%wilon\a'{o}t \> pos:n\&root:wln\&ptn:CiCoCa\&gen:fem\&num:pl\&stat:u\\
%wil\a'{o}n \> pos:n\&root:wyl\&ptn:CiCon\&gen:ms\&num:sg\&stat:u\\
%tmun\a'{a} \> pos:n\&root:mnh\&ptn:CCuCa\&gen:fm\&num:sg\&stat:u\\
%tmun\a'{o}t \> pos:n\&root:mnh\&ptn:CCuCa\&gen:fm\&num:pl\&stat:u\\
\end{tabbing}
\caption{BLC morphological analyses for nouns}
\label{fig:blc-nouns}
\end{mdframed}
\end{figure}

Hebrew nominals, i.e., adjectives and nouns, inflect for gender and number. However, as discussed in chapter~\ref{autonomous}, particularly section~\ref{sec:heb-example}, the Hebrew inflectional endings for nominals are \emph{fusional}; it is impossible to separate the  masculine plural suffix \textit{-im} into 
distinct singular and plural components. The same is true of the feminine plural suffix \textit{-ot}. (See section~\ref{sec:heb-example} for the full argument.)  Thus,\dots
We modify the BLC's atomic categories so that they better reflect Hebrew's fusional suffixes. That is, we merge \texttt{masculine} and \texttt{plural} into the single category \texttt{M\%Pl}. Likewise, \texttt{feminine} and \texttt{plural} become \texttt{F\%Pl}, and \texttt{feminine} and \texttt{singular} become \texttt{F\%Sg}.

The forms of Hebrew nouns also depend on whether they are in the \emph{absolute state} or the \emph{construct state}, as illustrated in table~\ref{tab:cstr-endings}. The absolute state is the canonical state, the state of a noun in isolation, whereas the construct state is the state that licenses a noun to link up to another noun and form a compound, as in \textit{pas\'{e}y raq\'{e}vet} `ribbons-of train' or `train tracks'. The noun \textit{pas\'{e}y} exhibits the construct ending for masculine plural nouns, namely \textit{-\'{e}y}. 
In table~\ref{tab:cstr-endings}, we see that the \textit{-\'{e}y} construct ending corresponds to the absolute ending -\textit{-\'{i}m}. Similarly, the ending \textit{-\'{a}t} feminine singular construct endingcorresponds to the absolute \textit{-\'{a}}. 
%However, the  Thewith another noun to form a compound noun. When isolated, nouns are in the \emph{absolute} state. For example, \dots
%\begin{exe}
%noun(construct)  noun(absolute)
%\end{exe}
%\dots Table~\ref{tab:cstr-endings} shows the absolute and construct forms for both masculine noun and a feminine %noun. 
However, the other two inflectional categories, namely the masculine singular and the feminine plural, have identical absolute and construct forms. Or, to put it another way, the construct state has no identifiable exponent in these two categories


%   \begin{table}[h!]
%      \small
%      \centering
%      \begin{tabular}{ccccc} 
% \toprule
% &  \multicolumn{2}{|c|}{masc. noun} & \multicolumn{2}{|c|}{fem. noun}
% &  sg. & pl.\\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%masc. & pas & pasim \\
%  fem. &	qvuc\'{a}	& qvuc\'ot \\	
%    \bottomrule
%    \end{tabular}
%    \label{tab:cstr-endings}
%    \caption{Fusional noun endings.}
%    \end{table}
    
 
   \begin{table}[h!]
      \small
      \centering
      \setlength{\extrarowheight}{8pt}
      \begin{tabular}{ccccc} 
 \toprule
 &  \multicolumn{2}{c}{masc. noun} & \multicolumn{2}{c}{fem. noun} \\
 &  sg. & pl. & sg. & pl. \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    absolute & pas & pasim  & qvuc\'{a}	& qvuc\'{o}t \\
    construct & pas & pasey	& qvuc\'{a}t	& qvuc\'{o}t\\
    \bottomrule
    \end{tabular}
    \label{tab:cstr-endings}
    \caption{Masculine and feminine construct forms.}
    \end{table}
    
%\textipa{Q}alul\a'{a} \> pos:adj\&root:\textipa{Q}ll\&ptn:CaCuC\&gen:fm\&num:sg\&src:deverb\\
%\textipa{Q}alul\a'{i}m \> pos:adj\&root:\textipa{Q}ll\&ptn:CaCuC\&gen:ms\&num:pl\&src:deverb\\

\begin{figure}[t]
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
nexmad\a'{a} \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:fm\&num:sg\&src:deverb\\
nexmad\a'{i}m \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:ms\&num:pl\&src:deverb\\
nexmad\a'{o}t \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:fm\&num:pl\&src:deverb\\
nexm\a'{a}d \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:ms\&num:sg\&src:deverb\\
\end{tabbing}
\caption{Morphological analyses extracted from the Berman Longitudinal Corpus. These entries are the inflections
of the adjective \textit{nexmad} `pleasant.'}
\label{fig:analyses}
\end{mdframed}
\end{figure}

   \begin{table}[h!]
      \small
      \centering
       \setlength{\extrarowheight}{8pt}
      \begin{tabular}{ccc} 
 \toprule
 & sg. & pl. \\
\cmidrule{2-3}
    masc. & nexmad & nexmad-\'{i}m \\
    fem. &  nexmad-\'{i} & nexmad-\'{o}t \\
    \bottomrule
    \end{tabular}
    \label{tab:cstr-endings}
    \caption{Inflections of the adjective \textit{nexmad} `pleasant'}
    \end{table}

%\begin{table}[ht]
%\begin{center}
%\subtable[maqomi `local' \label{subtab:fusion:1}]{
%\setlength{\extrarowheight}{8pt}
%{\begin{tabular}{lcc}
%\  & masc & fem  \\
%\hline 
%sg & maqom-i & meqom-i-t  \\
%pl & meqom-iy-im & meqom-iy-ot  \\
%\end{tabular}}
%}
%\subtable[gadol `big' \label{subtab:fusion:2}]{
%\setlength{\extrarowheight}{8pt}
%{\begin{tabular}{lcc}
%\  & masc & fem  \\
%\hline 
%sg & gadol & gdol-a  \\
%pl & gdol-im & gdol-ot  \\
%\end{tabular}}
%}
%\label{tab:fusion}
%\caption{Fusional suffixes in Hebrew nominals}
%\end{center}
%\end{table}

\begin{table}[ht]
\centering
\setlength{\extrarowheight}{8pt}
\begin{tabular}{lcc}
\toprule
& sg. & pl.  \\
\cmidrule(lr){2-3}
masc. & maqom-i &  meqom-iy-im\\
fem. & meqom-i-t  & meqom-iy-ot  \\
\bottomrule
\end{tabular}
\label{tab:der-adjs}
\caption{Inflections of the derivational adjective \textit{maqom\'{i}} `local,' derived from the noun for \textit{maqom} `place'}
\end{table}

%\texttt{cstr\%f\%mass}
%\texttt{cstr\%f\%sg}
%\texttt{cstr\%m\%mass}
%\texttt{cstr\%m\%pl}
%\texttt{cstr\%m\%sg}
%\textit{state} \% \textit{gender} \% \textit{number}
%\textit{adj}\%\textit{m}\%\textit{sg}

%\begin{exe}
%\ex \label{ex:cstr:pasey}
%	\textsf{pas\'{e}y+ha+rak\textipa{\'{e}}vet} \\
%	\textsf{n:det|+n|pas\&gen:ms\&num:pl\&stat:comp-\'{e}y+det|ha+n|rak\textipa{\'{e}}vet}
%\ex \label{ex:cstr:shaat} 
%	\textsf{\v{s}\textglotstop{at}+sip\'{u}r .} \\
%	\textsf{n|+n|\v{s}a\textipa{Q}\'a\&gen:fm\&num:sg\&stat:comp-\'at+n|sip\'ur .}
%\ex \label{ex:cstr:bdiqat} 
%	\textsf{bdiq\'{a}t+\textipa{P}ozn\'{a}yim} \\ 
%	\textsf{n|bdiq\'{a}\&root:tbd\&ptn:tbd\&gen:fm\&num:sg\&stat:bound-\'{a}t}
%\end{exe}

\paragraph{Participles.}
All binyanim except the qal and nifal use the prefix stem to form the participle. To deal with the qal and nifal participles, we add the constraints qal:participle and nifal:participle.
%def analysisFilter(analyses_str):
%	new_analyses = list()
%	analyses=analyses_str.split()
%	if "pos:part" in analyses_str:
%		####print "**",analyses_str
%		for analysis in analyses:
%			if "pos:v" in analysis:
%				continue
%			elif "pos:adj" in analysis:
%				continue
%			elif "pos:n" in analysis:
%				continue
%			else: new_analyses.append(analysis)
%		return new_analyses
%	else:
%		return analyses
%	elif pos == "part":
%		#pat = ur"(ptn:)([a-zC]+)(&tense:)(past)(&pers:3&gen:f&num:sg)(&|$)"
%		#pat = ur"(pos:)(part)(&[^\s]*)(&ptn:)([a-zC]*)(&[^\s]*)(&gen:)([mfu])(&num:)([sgplundoma]+)(&|$)"
%		#pat = ur"(pos:)(part)(&root:[^\s]+&ptn:)([a-zC]*)(&[^\s]*)(gen:)([mascfemunsp]+)(&num:)([sgplundoma]+)(&|$)"
%		pat = ur"(pos:)(part)([^\s]*&root:[^\s]+)(&ptn:)([a-zC]*)(&[^\s]*)(gen:)([mascfemunsp]+)(&num:)([sgplundoma]+)(&|$)"
%		re_part = re.compile(pat, re.UNICODE)
%		#analysis = re_part.sub(ur"\5%prefix_stem&\8%\10\11\6", analysis)
%		if ptn == "qal":
%			#1 (pos:)
%			#2 (part)
%			#3 ([^\s]*&ptn:)
%			#4 ([a-zC]*) the actual pattern
%			#5 (&[^\s]*)
%			#6 (gen:) 
%			#7 ([mascfemunsp]+)  # gender val
%			#8 (&num:)
%			#9 ([sgplundoma]+)    # number val
%			#10(&|$)
%			analysis = re_part.sub(ur"\3&\5%participle&\8%\10\6", analysis)
%			# stemFeature = binyan + "%participle"
%			# processedFeatures.append(stemFeature)
%		elif ptn == "nifal":
%			analysis = re_part.sub(ur"\3&\5%suffix_stem&\8%\10\6", analysis)
%			# stemFeature = binyan + "%suffix_stem"
%			# processedFeatures.append(stemFeature)
%		else:
%			analysis = re_part.sub(ur"\3&\5%prefix_stem&\8%\10\6", analysis)
			
			
\subsubsection{Pronominals}

\begin{table}[ht]
\centering
\setlength{\extrarowheight}{8pt}
\begin{tabular}{llcc}
\toprule
     & & sg. & pl. \\
     \midrule
    1 & m/f &  \textipa{P}an\'{i} & \textipa{P}an\'{a}xnu / \textipa{P}\'{a}nu \\
    \midrule
   & masc &  \textipa{P}at\'{a}  &  \textipa{P}at\'{e}m\\
   \raisebox{1.5ex}[0pt]{2} & fem & \textipa{P}at  &  \textipa{P}at\'{e}n\\
   \midrule
   & masc & hu\textipa{P}  & hem\\    
   \raisebox{1.5ex}[0pt]{3} & fem  & hi\textipa{P} &  hen \\
    \bottomrule
\end{tabular}
\label{tab:pers-pronouns}
\caption{Personal pronouns}
\end{table}


%\begin{tabbing}
%\hspace{1in} \= \hspace{5.5in} \kill
%\v{s}ehem \> pre:sh\&pre:sh\&pos:pro\_person\&pers:3\&gen:ms\&num:pl pre:sh\&pos:pro\_person\&pers:3\&gen:ms\&num:pl\\
%\v{s}ehen \> pre:sh\&pre:sh\&pos:pro\_person\&pers:3\&gen:fm\&num:pl\\
%\v{s}e\textipa{P}an\a'{i} \> pre:sh\&pos:pro\_person\&pers:1\&num:sg pre:sh\&pos:pro\_person\&pers:1\&num:sg pre:sh\&pos:pro\_person\&pers:1\&num:sg pre:sh\&pre:sh\&pos:pro\_person\&pers:1\&num:sg\\
%\v{s}e\textipa{P}at\UTF{00E1} \> pre:sh\&root:\textipa{P}yt\&ptn:CaCa\&pre:sh\&pos:pro\_person\&pers:2\&gen:ms\&num:sg\\
%\v{s}e\textipa{P}at\UTF{00E9}m \> pre:sh\&root:\textipa{P}ty\&ptn:CaCy\&pre:sh\&pos:pro\_person\&pers:2\&gen:ms\&num:pl\\
%\textipa{P}an\UTF{00E1}xnu \> pos:pro\_person\&pers:1\&num:pl\\
%\textipa{P}an\a'{i} \> pos:pro\_person\&pers:1\&num:sg\\
%\textipa{P}at\UTF{00E9}m \> pos:pro\_person\&pers:2\&gen:ms\&num:pl\\
%\textipa{P}at\UTF{00E9}n \> pos:pro\_person\&pers:2\&gen:fm\&num:pl\\
%hu\textipa{P} \> pos:pro\_person\&pers:3\&gen:ms\&num:sg \\
%hi\textipa{P} \> pos:pro\_person\&pers:3\&gen:fm\&num:sg \\
%\end{tabbing}
%
%		pat = ur"(\t|&)(pos:)([^\s]*pro[^\s]*)(&[^\s]*pers:)([123]+)(&gen:)([fsemunsp]+)(&num:)([sgplduoun]+)(&|$)"
%		re_pro = re.compile(pat, re.UNICODE)
%		# 1. (\t|&)
%		# 2. (pos:)
%		# 3. ([^\s]*pro[^\s]*)
%		# 4. (&[^\s]*pers:)
%		# 5. ([123]+) ACTUAL PERSON
%		# 6. (&gen:)
%		# 7. ([fsemunsp]+) ACTUAL GENDER
%		# 8. (&num:)
%		# 9. ([sgplduoun]+) ACTUAL NUMBER
%		# 10. (&|$)
%		analysis = re_pro.sub(ur"\1\3\%\5\%\7\&\3\%\5\%\7\%\9\10", analysis)
%		*pro*\%[pers]\%[gen]  \&    *pro*\%[pers]\%[gen]\%[num]
	
%	pat = ur"(^.*)(pro:)([^&]+)(&|$)((?:&[^\s]*$)|$)"
%	re_protype = re.compile(pat, re.UNICODE)
%	if re_protype.search(analysis) != None and re_protype.search(analysis) != "":
%		pro\_type = re\_protype.sub(ur"\3", analysis)
%	analysis = analysis.replace("xxxxxxxxxx:", "pro\_suf\_state&pro\_suf:")
	
%\textipa{Q}al\a'{a}yw \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:3\&gen:ms\&num:sg pos:n\&gen:ms\&num:pl\&poss:3mascSG\\
%\textipa{Q}al\a'{e} \> pos:n\&gen:ms\&num:sg\&stat:u\\
%\textipa{Q}al\a'{e}yha \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:3\&gen:fm\&num:sg pos:n\&gen:ms\&num:pl\&poss:3femSG\\
%\textipa{Q}al\a'{e}ynu \> pos:n\&gen:ms\&num:pl\&poss:1PL\\
%\textipa{Q}al\a'{e}y\textsubdot{k}a \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:2\&gen:ms\&num:sg pos:n\&gen:ms\&num:pl\&poss:2mascSG\\
\begin{figure}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
\textipa{P}ot\a'{a}h \> pos:acc+pro\&pers:3\&gen:fm\&num:sg\\
\textipa{P}ot\a'{a}m \> pos:acc+pro\&pers:3\&gen:ms\&num:pl\\
\textipa{P}ot\a'{a}n \> pos:acc+pro\&pers:3\&gen:fm\&num:pl\\
\textipa{P}ot\a'{a}nu \> pos:acc+pro\&pers:1\&gen:unsp\&num:pl\\
\textipa{P}ot\a'{a}\textsubdot{k} \> pos:acc+pro\&pers:2\&gen:fm\&num:sg\\
\textipa{P}ot\a'{i} \> pos:acc+pro\&pers:1\&gen:unsp\&num:sg\\
\end{tabbing}
\caption{The marker of definite-direct objects \textipa{P}et combines with pronominal suffixes
to create (in effect) accusative pronouns}
\label{fig:acc-examples}
\end{figure}

	prep+pro\%1             
	prep+pro\%1\%pl          
	prep+pro\%1\%sg          
	prep+pro\%2\%f           
	prep+pro\%2\%f\%sg        
	prep+pro\%2\%m           
	prep+pro\%2\%m\%pl        
	prep+pro\%2\%m\%sg        
	prep+pro\%3\%f           
	prep+pro\%3\%f\%pl        
	prep+pro\%3\%f\%sg        
	prep+pro\%3\%m           
	prep+pro\%3\%m\%pl        
	prep+pro\%3\%m\%sg

	
%\begin{tabbing}
%\hspace{1in} \= \hspace{5.5in} \kill
%hazo\textipa{P}t \> pre:ha\&pro_dem\&pers:3\&gen:fm\&num:sg\\
%hem \> pos:cop\&tense:present\&pers:3\&gen:ms\&num:pl pos:pro_person\&pers:1\&num:sg pos:pro_person\&pers:3\&gen:ms\&num:pl pos:part\&root:rcy\&ptn:qal\&gen:ms\&num:pl\\
%hem\v{s}\a'{e}\textsubdot{k} \> pos:n\&num:sg\&stat:u\\
%hen \> pos:cop\&tense:present\&pers:3\&gen:fm\&num:pl pos:pro_person\&pers:3\&gen:fm\&num:pl\\
%\textipa{P}\a'{a}nu \> pro_person\&pers:1\&num:pl 
%haze \> pre:ha\&pro\_dem\&pers:3\&gen:ms\&num:sg\\
%\v{s}em\a'{a}\v{s}ehu \> pre:sh\&root:m\v{s}h\&ptn:CaCeC\&pre:sh\&pro\_indef\&gen:ms\&num:sg\\
%\v{s}em\a'{i}\v{s}ehu \> pre:sh\&root:m\v{s}h\&ptn:CiCeC\&pre:sh\&pro\_indef\&pers:3\&gen:ms\&num:sg\\
%\end{tabbing}			

\subsubsection{Particles}
A distinct gold-standard category was created for each atomic prefixal particle, as well as three additional categories for the composite
prefixes \textit{la-} (`to the'), \textit{ba-} (`in the'), and \textit{ka-} (`as the'), which result from the blending of the prefixal prepositions \textit{le-} (`to'),\textit{be-} (`in'), and \textit{ke-} (`as') with the definite-article prefix \textit{ha-}.
and \text{be-} `in'. 

\begin{table}[t]
\begin{tabular}{ccc}
\toprule
Prefix & Gloss & Gold-Std Category \\
\midrule
\textit{be-} & `in'	& pre:be   \\                    
\textit{ba-}	& & pre:be\~pre:ha    \\                              
\textit{ke-}	&`as'  & pre:ke \\                
\textit{ka-}	& `as the'& pre:ke\~pre:ha \\          
\textit{le-}	& `to' & pre:le  \\               
\textit{la-}	& `to the' & pre:le\~pre:ha \\
\textit{ha-}	& `the' & pre:ha  \\      
\textit{mi(n)-} & & pre:mi  \\               
\textit{\v{s}e)-}	& & pre:sh \\
\textit{we)-} & `and' & pre:we \\
\bottomrule
\end{tabular}
\end{table}
%		pat = ur"(pos:)(adj)([^\s]*)(\&gen:)([a-z+])(\&num:)([a-z:]+)(\&stat:)(cstr)"
%		re_cstr = re.compile(pat, re.UNICODE)
%			
%		if gen == "f" and num == "pl":
%			analysis=re_cstr.sub(ur"\2\&\5\%\7\3", analysis)
%			analysis=re_cstr.sub(ur"\2\&\5\%\7\3", analysis)
%		else:
%			analysis=re_cstr.sub(ur"\2\&\9\%\5\%\7\3", analysis)
%			adj\&cstr%[gen]%[num]
%			analysis=re_cstr.sub(ur"\2\&\9\%\5\%\7\3", analysis)
%		1 (pos:)
%		2 (adj)
%		3 ([^\s]*)
%		4 (&gen:)
%		5 ([a-z+])
%		6 (&num:)
%		7 ([a-z:]+)
%		8 (&stat:)
%		9 (cstr)
%		
%		pat = ur"(pos:)(n)([^\s]*)(\&gen:)([a-z])(\&num:)([a-z:]+)(\&stat:)(cstr)"
%		re_cstr = re.compile(pat, re.UNICODE)
%		
%		if gen == "f" and num == "pl":
%			analysis=re_cstr.sub(ur"\5\%\7\3", analysis)
%			analysis=re_cstr.sub(ur"\5%\7\3", analysis)
%		gen([a-z+])
%		num([a-z:]+)
%		([^\s]*)
%		else:
%			analysis=re_cstr.sub(ur"\9\%\5\%\7\3", analysis)
%			analysis=re_cstr.sub(ur"\9\%\5\%\7\3", analysis)
%		(cstr)	
%		gen([a-z+])
%		num([a-z:]+)
%		([^\s]*)
%
%		pat = ur"(pos:)(n)([^\s]*)(&gen:)([a-z]+)(&num:)([a-z:]+)(&stat:u)?"
%		# 1: (pos:)
%		# 2: (n)
%		# 3: ([^\s]*)
%		# 4: (&gen:)
%		# 5: ([a-z]+)
%		# 6: (&num:)
%		# 7: ([a-z:]+)
%		# 8: (&stat:u)?
%		re_abs = re.compile(pat, re.UNICODE)
%		analysis=re_abs.sub(ur"\5%\7\3", analysis)
%		gen([a-z:]+)num([a-z:]+)([^\s]*)
%		
%		#print "analysis nominal", 2, ": ", analysis
%		pat = ur"(pos:)(adj)([^\s]*)(&gen:)([fmunsp]+)(&num:)([a-z:]+)(&stat:u)?"
%		# 1: (pos:)
%		# 2: (adj)
%		# 3: ([^\s]*)
%		# 4: (&gen:)
%		# 5: ([fmunsp]+)
%		# 6: (&num:)
%		# 7: ([a-z:]+)
%		# 8: (&stat:u)
%		re_abs = re.compile(pat, re.UNICODE)
%		#analysis=re_abs.sub(ur"\2&\5\%\7\3", analysis)
%		analysis=re_abs.sub(ur"\2&\5\%\7\3", analysis)
%		adj&[gen]\%[num]([^\s]*)	
To measure precision and recall, one needs a gold standard. Supervised
%% learners train on a body of human-labeled examples, and thus come with
%% a ready made gold standard, namely some subset of the human-labeled
%% examples themselves. But unsupervised learning algorithms like the
%% MCMM do not train on pre-labeled data, and so they come with no
%% obvious gold standard against which their output can be extrinsically
%% evaluated.
%
%% \marginpar{MD: Should we mention transliterating Hebrew into Latin?
%%   It doesn't really affect the algorithm, though.}
%
%Our data is the same Hebrew word list used by \cite{daya-et-al:2008}
%in their study of automatic root identification.
%This list comprises 6888 unique words with an average length of 5.4 characters. About two-thirds are derived from consonantal roots.
%The dataset gives root annotations for the words that have roots, but specifies no other morphological information. 
%
%We thus use the output of an automatic
%finite-state morphological analyzer, namely the MILA Morphological
%Analysis tool (\textsc{mila-ma}) \citep{hebrew-resources:2008}.
%% \textsc{mila-ma} is in essence a finite-state transducer, a variation
%% of the finite-state automaton. 
%Because its morphological knowledge has been manually coded and its
%output is deterministic, \textsc{mila-ma} provides a fairly close
%approximation to human annotation. This is especially true in the case of the present study. That is, because we consider only the absolute, context-independent analyses of \emph{word
%types}, the problem of disambiguation, the usual source of uncertainty, does not arise.
%
%%group together features from different analyses.
%
%To obtain the the gold-standard dataset for a given experiment, 
%we run \textsc{mila-ma} on every word in the MCMM's output. 
%\textsc{mila-ma} gives
%at least one analysis for each word, and more than one for ambiguous words.
%For example, the form \textit{mniyim} can be
%a \textit{Hif`il} participle (`motivating'), a noun (`motivation'), or a noun bearing the prefixal preposition
%\textit{m-} (`from movement').
%% For each word, \textsc{mila-ma} produced at least one analysis, and
%% multiple analyses if the word in question is morphologically
%% ambiguous.
%\textsc{mila-ma} outputs a separate analysis for each of these possibilities; the analysis corresponding to the third possibility is displayed in 
%% possibilities;
%figure~\ref{fig:mila-output}, with brackets delimiting features and values.
%% .  As illustrated, a \textsc{mila-ma} analysis is string of categories
%% delimited by \texttt{[+}.  Each category in turn consists of a feature
%% and value delimited by \texttt{]}. 
%
%Then, in our evaluation of the MCMM's output, we examine each cluster that contains \textit{mniyim} (if any). In each case, we compare \textit{mniyim}'s \textsc{mila-ma} analyses with those of the other words in its cluster to determine the extent to which \textit{mniyim} has been grouped with similar words. Each cluster should correspond to a single predominant property. Therefore, the ideal case, at least according to our gold-standard, is that \textit{mniyim} should appear in three distinct clusters, one for each of the three \textsc{mila-ma} analyses for\textit{mniyim}.
%%other masculine plural nouns, for example, other words that begin
%%prepositional prefixes, other \textit{Hif'il} participles, or it appears in With some modifications (described next),
%%these feature-value pairs become our gold categories.
%%mniyim	[	+participle][+id]205[+undotted]hniy
%%  [+transliterated]hniy[+root]nwy[+binyan]+Hif'il
%%  [+register]+formal[+tense]+beinoni[+person]+any
%%  [+gender]+masculine[+number]+plural
%%  [+construct]+false[+definiteness]+false
%\begin{figure}[htb!]
%\footnotesize
%\begin{verbatim}
%mniyim	[+preposition]m[+noun][+id]7888
%  [+undotted]niy[+transliterated]niy
%  [+gender]+masculine[+number]+plural
%  [+definiteness]+false[+register]+formal
%  [+construct]+false
%\end{verbatim}
%%	mniyim	[	+noun][+id]6406[+undotted]mniy[+transliterated]mniy[+gender]+masculine[+number]+plural[+definiteness]+false[+register]+formal[+construct]+false
%%	mniyim	[+preposition]m[+noun][+id]7888[+undotted]niy[+transliterated]niy[+gender]+masculine[+number]+plural[+definiteness]+false[+register]+formal[+construct]+false
%\caption{One of the three analyses that \textsc{mila-ma} outputs for the word \textit{mniyim}}
%\label{fig:mila-output}
%\end{figure}
%
%\subsection{Category Mappings}
%\label{subsec:mappings}
%\textsc{mila-ma} outputs 22 possible feature labels. Four of these
%(\texttt{id}, \texttt{undotted}, \texttt{transliterated},
%\texttt{register}) are irrelevant to the present study and are thus discarded.  Each of the
%18 remaining features has at least two values,
%%(\textsc{true} and \textsc{false}), 
%and some have many more.  
%Each feature-value combination is a distinct gold-standard category.
%% Since \textsc{mila-ma} is run on the MCMM's output to obtain the gold
%% standard categories, the size of the gold standard varies with number
%% of words that the MCMM manages to cluster in a given experiment. 
%For a sense of the number of categories involved, consider that in one experiment, 
%our model assigned 4244 of the 6888 words to one or more clusters.
%%one or more clusters.
%Not counting the four irrelevant features mentioned above,
%\textsc{mila-ma}'s 
% output for these 4244 words contained 778 distinct feature-value
%combinations, i.e., categories.
%
%However, many of these original \textsc{mila-ma} categories are
%ill-suited to the purpose of evaluating an MCMM. 
%%We provide a sketch here
%%of our modifications to \textsc{mila-ma}'s category set.  
%%Space precludes a complete discussion, but it should be noted that 
%%while some mappings make the task easier, 
%%It should be noted that these linguistically-motivated mappings make the evaluation more rigorous:
%%By removing very broad categories, such as \texttt{verb} or
%%\texttt{construct:false}, we decrease the chances of receiving credit
%%for accidental clusterings.
% For example, \textsc{mila-ma}'s output contains negative categories
% like \texttt{definiteness:false} and \texttt{construct:false},
% %and \texttt{person/number/gender:-},
% but an MCMM is only capable of recognizing
% positive attributes, i.e., attributes that are actually present in
% forms. 
%% In other words, an MCMM can group together marked forms on the
%% grounds that they share a marker, but there would simply be no 
%% grounds for grouping forms that are not united by a common marker.
% %unmarked forms, since unmarked forms by definition have no formal
% %element in common. 
% Thus, while an MCMM might succeed in grouping
% \texttt{definiteness:true} forms on the basis of their shared
% definite prefix \textit{h-}, it would have no means of discovering 
% a category like
% \texttt{definiteness:false}, whose members would have no common marker.
% %for which there is no marker.
% %because the forms tha since there is no positive
% %attribute that unites them.
% 
% We thus modify \textsc{mila-ma}'s category set in certain ways. 
% The following is a summary of our modifications.
% It should be noted that in most cases these linguistically-motivated mappings make the evaluation more rigorous:
%By removing very broad categories, such as \texttt{verb} or
%\texttt{construct:false}, we decrease the chances of receiving credit
%for accidental clusterings.  
%% %For example, Hebrew makes words
%% %definite by attaching the prefix "h-". Thus, any word that has this prefix will belong to the 
%% %"definiteness:true" category, and likewise all words in the "definiteness:true" will begin with an "h-". Now, it is possible for the mcmm to cluster together all words that begin with "h-", for this is a positive formal property. But what about the "definiteness:false" words? These words all lack the definite marker, so there will be nothing that is the same across all "definiteness:false", and there will thus be no grounds for grouping them together. Note also: having negative categories inflates the results.
%% Other morphological categories may not seem at first glance seem to be negatively defined, but are nonetheless unmarked. For example, Hebrew \textsc{3.m.sg} verb forms are ``distinguished," as it were, by the absence of a marker (see the paradigm in figure \ref{fig:paradigms}).
%
%% \begin{itemize}
%% \itemsep0em
%%\item 
%%For POS labels, we retain \texttt{adverb}s, \texttt{adjective}s,
%%and \texttt{numeral}s, ignoring other categories because they are super-categories (e.g.,
%%\texttt{verb} covers seven \texttt{binyan} categories (verb classes))
%%or lack systematic marking (e.g., \texttt{preposition}).
%%%\item 
%%Atomic categories are combined where appropriate, to capture
%%  fusional morphology (e.g., \texttt{masculine} +
%%  \texttt{plural} $\mapsto$ \texttt{M\%Pl}).
%%%\item
%%Verb inflections are mapped to composite categories 
%%%based on overlapping prefixes and suffixes 
%%(cf. non-3rd-singular verbs in English).
%%  % (e.g., \texttt{{future\%(2\%M)\textbar(2\textbar3\%F)}}), akin to
%%  % non-3rd-singular verbs in English.
%%%\item 
%%For verb stems, the \texttt{binyan} and
%%  \texttt{tense} features are mapped from 21 combinations into 15
%%  features 
%%  %encoding \texttt{binyan} and 
%%  incorporating \texttt{stem\_type}.
%%%\item 
%%For participles, adjective and noun analyses are discarded
%%  (cf. \textit{-ing} forms in English).
%%%\item 
%%We use \texttt{rootless:Nominal} 
%%%  and \texttt{rootless}, 
%%  to capture the regularities of loan words.
%%%  (e.g., use of vowel letters)
%%%  , more frequent usage of \textit{v} (\textit{tet})).
%%%\item 
%%Negative categories (e.g., \texttt{construct:false}) and
%%  unmarked forms (e.g., \texttt{M/Sg} in nominals) are discarded.
%%%\end{itemize}
%
% \paragraph{POS labels.}
% Of \textsc{mila-ma}'s POS labels, we retain only \texttt{adverb}, \texttt{adjective}, and \texttt{numeral} (including cardinal and ordinal subtypes), which often bear distinctive markers. We discard each of the other POS categories for one of the following reasons: 
% \begin{itemize}
%   \item The category is a super-category of other categories and is therefore redundant. For example, \texttt{verb} is a super-category of the seven \textit{binyan} categories (verb classes). If a word is a member of any binyan, it is necessarily also a member of the \texttt{verb} category.
%   \item The category lacks a distinctive form (i.e., it is not systematically marked). Examples include \texttt{preposition} and \texttt{properName}.
%   \item The category is associated with many marginally systematic markers, but it has no predominant marker; e.g., there are many patterns by which nouns are derived, but no predominant pattern.
%%   \item The category does exhibit systematic marking, but its markers coincide with those of another category. For example, numerals bear the same markings as found on nouns and adjectives. 
% \end{itemize}
%
% \paragraph{Negative categories and unmarked forms.} 
% %As discussed above \dots
%
% As mentioned above, an MCMM can only recognize the presence of attributes, not their absence.
% We thus discard negatively defined categories such as \texttt{definiteness:false} and \texttt{construct:false}, as such categories tend to indicate the \emph{absence} of some specific marker. 
% Note again that we do not make the evaluation easier by discarding such categories because in most cases, given a positive and a negative category, the majority of words will trivially belong to the negative category.
% 
% A category does not have to negative to be unmarked, however. The \texttt{M/Sg} in nominals and \texttt{3/M/Sg} in verbs, for example, are unmarked forms. In general, we discard any category that is not associated with an overt marker. 
%
% % . In other words, an MCMM can group together marked forms on the basis
% % of some shared marker, but 
%% [it has no basis for grouping together unmarked forms, since unmarked
%% forms have no formal element in common.  Thus, while the MCMM might
%% succeed in grouping \texttt{definiteness:true} forms together on the
%% basis of the shared prefix \textit{h-}, it has no means to group
%% together \texttt{definiteness:false} words, since no positive
%% attribute unites them.]
% %
%% In general, we discard categories that are not associated with an overt
%% marker. These include \texttt{M/Sg} in nominals, the \texttt{M/Sg} and
%% \texttt{F/Pl} nominal construct states,
%% % (in that they are, in the absence of context, indistinguishable from
%% % their absolute-state counterparts),
%% and \texttt{3/M/Sg} in verbs.
%
%% % %We thus mapped the raw categories to a modified category set better suited to the present study.
% \begin{figure}[tb!]
% \begin{center}
% %\footnotesize
% \begin{tabular}{lcccc}
%    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
%    & \textit{Sg} & \textit{Pl} & \textit{Sg} & \textit{Pl} \\
%   \hline
%   \textit{1.m/f} & htxil-ti & htxil-nw & a-txil & n-txil\\
%   \hline
%   \textit{2.m} & & htxil-tm  &  t-txil & \\
%   \textit{2.f} & \raisebox{1.5ex}[0pt]{htxil-t} & htxil-tn &  t-txil-i & \raisebox{1.5ex}[0pt]{t-txil-w}\\
%   \hline
%   \textit{3.m} & htxil &   &  itxil &\\
%   %\cline{1-2}
%   \textit{3.f} & htxil-h & \raisebox{1.5ex}[0pt]{htxil-w} &  t-txil & \raisebox{1.5ex}[0pt]{i-txil-w} \\
% \end{tabular}
% \caption{Past and future-tense paradigms for the root \textit{t.x.l} in the \textit{Hif'il} binyan.}
% \label{fig:paradigms}
% \end{center}
% \end{figure}
%
% \paragraph{Atomic categories.} 
% %For verbs, \textsc{mila-ma} combines person, number, and gender into a composite \texttt{person/gender/number} feature taking values such as \texttt{3/F/Sg}. However, 
% For nouns, adjectives, and participles, \textsc{mila-ma} expresses gender and number as separate atomic categories, e.g. \texttt{gender:masculine} and \texttt{number:plural}, even though Hebrew is fusional in its gender/number
% markings (see section~\ref{sec:intro}). Most masculine singular nominal forms are unmarked in Hebrew, while most masculine 
% nominals take the ending \textit{-im}. 
% On the other hand, most feminine plurals take the ending \textit{-wt}. 
% Since the masculine plural \textit{-im} has nothing in common with either the masculine singular or the feminine plural forms, it cannot be split into distinct masculine and plural components. We modify \textsc{mila-ma}'s atomic categories so that they better reflect Hebrew's fusional suffixes. That is, we merge \texttt{masculine} and \texttt{plural} into the single category \texttt{M\%Pl}. Likewise, \texttt{feminine} and \texttt{plural} become \texttt{F\%Pl}, and \texttt{feminine} and \texttt{singular} become \texttt{F\%Sg}.
%
%
% \paragraph{Verb Inflections.}
% %For every cell in figure~\ref{fig:paradigms}, there is a 
% \textsc{mila-ma} represents verbal inflections for person, number, and gender in %composite 
% \texttt{person/gender/number} (or \texttt{PGN}) features taking values like \texttt{3p/F/Sg} and \texttt{2p/M/Sg}. However, there is not always a one-to-one correspondence between such category labels and the actual distinctions in form. Consider, for example, the verb \textit{ttxil} `she/you will begin'. In the absence of context, this form is entirely ambiguous; it can be either \textsc{2.m.sg} or \textsc{3.f.sg}. \textsc{mila-ma} produces a separate analysis for each possibility, giving rise to two distinct \texttt{PGN} categories, namely \texttt{PGN:2/M/Sg} and \texttt{PGN:3/F/Sg}, where only one form exists. This is similar to the problem of negative and unmarked categories: an MCCM has no means of making a distinction where none exists in the feature representations.
% %The strictly word-internal feature sets that we use in our experiments provide the MCMM with no means of mapping a single form to more than one category.
% 
% We thus map \textsc{mila-ma}'s verb-inflection categories to composite categories based on overlapping prefixes and suffixes. For example, compare the future-tense forms \textit{t-txil-i} (\textsc{2.f.sg}) and \textit{t-txil} (\textsc{2.m.sg}/\textsc{3.f.sg}) (see figure~\ref{fig:paradigms}). Except for the addition of the \textit{-i} suffix, the \textsc{2.f.sg} form is identical to the \textsc{2.m.sg}/\textsc{3.f.sg} form. Noting both the overlap in prefixes and the difference in suffixes (i.e., \textit{-i} vs. $\emptyset$), we create the categories \texttt{future\%(2\%M)\textbar(2\textbar3\%F)} and \texttt{future\%2\%F\%Sg)}\footnote{
%The \texttt{\%} sign delimits tense, person, gender, and number, and the symbol \texttt{\textbar} means `or'.}
%The former corresponds to the prefix \textit{t-} and the latter to the suffix \textit{-i}. Both are thus necessary to represent the 
%\textsc{2.f.sg} future.
%% We define both past and future-tense inflectional features because past tense and future tense differ systematically in form.
%% In the past tense, \textsc{2.m.sg}, \textsc{3.f.sg}, and \textsc{2.f.sg} each map to a separate category, 
%% namely, \texttt{past\%2\%F\%Sg}, \texttt{past\%2\%M\%Sg}, and \texttt{past\%3\%F\%Sg}, respectively. 
%% \marginpar{Note that the 2ms and 2fs past-tense forms are orthographically identical, so
%% they should be combined in our category set. I overlooked this.}
%% We similarly derive the following categories:
%% %the \textsc{mila-ma} categories
%% \begin{itemize}
%% \item \texttt{past\%3\%F\%Sg} and \texttt{future\%3\%Sg} (We define no feature for the 3.m.sg past tense, since it is the unmarked form.)
%% \item \texttt{past\%1\%Sg}, \texttt{past\%1\%Pl}, \texttt{future\%1\%Sg}, and \texttt{future\%1\%Pl}
%% \item \texttt{past:2\%F\%Sg}, \texttt{past:2\%M\%Sg}
%% \marginpar{We still need past:2\%M\%Pl and past:2\%F\%Pl}
%% \item \texttt{(past\%3\%Pl)\textbar(future\%2\textbar3\%Pl)}
%% \end{itemize}
%% %\texttt{3p/MF/Pl}, \texttt{PGN:3p/MF/Pl}, \texttt{PGN:2p/MF/Pl}, \texttt{tense:past}, \texttt{tense:future} $\to$ 
%% %\texttt{(past\%3\%Pl)\textbar(future\%(2|3)\%Pl}
%% %\begin{itemize}
%% %\item 
%% %\item
%% %\item
%% %\end{itemize}
%% %\texttt{future:1p/MF/Sg}
%% %\texttt{future:1p/MF/Pl}
%% %\texttt{future:2p/F/Sg}
%% %\texttt{future:2p/MF/Pl}
%% %\texttt{future:} 
%% %\texttt{participle_prefix}
%% %	future:(2%M)|(2|3%F)
%% %	future:1%Pl
%% %	future:1%Sg
%% %	future:2%Sg
%% %	future:3%M
%% %	participle_prefix
%% %	past:1%Pl
%% %	past:1%Sg
%% %	past:2%F%Sg
%% %	past:2%M%Sg
%% %	past:3%F%Sg
%% %	past|future:2|3%Pl
%	
% \paragraph{Verb stems.}
% The forms of Hebrew verb stems vary along two dimensions:
% \begin{description}
% \item \textit{binyan:} There are seven \textit{binyanim}, or verb classes, each having a certain vowel pattern and taking a particular set of inflectional affixes (although person-gender-number markings are to some extent consistent across the binyanim).
% \item \textit{stem type:} Each binyan has a ``suffix stem" and a ``prefix stem." The former appears in paradigms dominated by suffixes (mainly the past tense), and the latter in paradigms dominated by prefixes (e.g., the future tense and (in most binyanim) the participle).
% \end{description} 
%% (1) There are the seven binyanim, and 
%% (2) each binyan generally has both a ``suffix" stem and a ``prefix" stem. 
%% The former is used
%% in the past tense, which has suffixes but no prefixes.
%% The latter is used in the future tenses, which takes prefixes in addition to suffixes.
% The suffix and prefix stems of the \textit{Hif'il} binyan are evident in figure \ref{fig:paradigms}; respectively, these are 
% %\verb*|h  i | 
% $h\Box \Box i \Box$
% (in the past tense) 
% and 
% %\verb*|  i | 
% $\Box \Box i \Box$
% (in the future tense).
%% \textit{h\underline{\hspace{1.5ex}} \underline{\hspace{1.5ex}} i \underline{\hspace{1.5ex}}} and \textit{\_\_i\_}, respectively.
% In every binyan except the \textit{Pa'al} and \textit{Nif'al}, the prefix stem is also used in the participle (i.e., present tense). The \textit{Pa'al} participle has its own unique stem, 
% and the \textit{Nif'al} participle uses the \textit{Nif'al} suffix stem.
% 
% The main idea is that tenses are in part identified by their stems; each tense uses a particular type of stem. 
% However, the shapes of these stem types depend on the binyan.
% \textsc{mila-ma} has separate \texttt{binyan} and \texttt{tense} feature types. 
% The former takes seven values, and the latter three.
% Since stem shape depends on \texttt{binyan} and \texttt{tense} jointly, we first combined these feature types, yielding 21 composite 
% textit{binyan}\texttt{:}\textit{tense} categories. However, noting that the future tense and participle often share the same stem type,
% we mapped these 21 categories to 15 \textit{binyan}\texttt{:}\textit{stem\_type} categories. (There are 15 instead of 14 because an additional category is necessary for the unique \textit{Pa'al} participle stem.) 
%
%%\footnotesize
%%\begin{tabular}{lccc}
%%	\textit{Pa'al} & \texttt{Pa'al:prefix\_stem} &
%%	\texttt{Pa'al:suffix\_stem} &
%%	\texttt{Pa'al:participle} \\
%%	\textit{Nif'al} & \texttt{Nif'al:prefix\_stem} &
%%	\texttt{Nif'al:suffix\_stem} & \\
%%	\textit{Pi'el} & \texttt{Pi'el:prefix\_stem} &
%%	\texttt{Pi'el:suffix\_stem} & \\
%%	\textit{Pu'al} & \texttt{Pu'al:prefix\_stem} &
%%	\texttt{Pu'al:suffix\_stem} & \\
%%	\textit{Hif'al} & \texttt{Hif'il:prefix\_stem} &
%%	\texttt{Hif'il:suffix\_stem} & \\
%%	\textit{Huf'al} & \texttt{Huf'al:prefix\_stem} &
%%	\texttt{Huf'al:suffix\_stem} & \\
%%	\textit{Hitpa'el} & \texttt{Hitpa'el:prefix\_stem} &
%%	\texttt{Hitpa'el:suffix\_stem} & \\
%%\end{tabular}	
%
% \paragraph{Participles.}
% In Hebrew, a participle, can function as a noun, an adjective, or as a participle/present-tense verb. Thus, whenever a word is a participle, \textsc{mila-ma} outputs not just a participle analysis, but also a noun analysis and an adjective analysis.
% %at least three separate analyses---one for each of these possibilities.
%%  whenever a word is a participle: one for the noun reading, one for the adjective reading, and one for participle/present-tense verb reading. However, we cannot expect the MCMM to find three distinct categories in a single form in the absence of the necessary contextual evidence for these categories.
% Moreover, we note that if word \textit{w} is a participle, then it is also (at least potentially) an adjective and a noun. But the converse is not true; that is, \textit{w} may be an adjective or a noun without also being a participle.
% Thus, whenever \textsc{mila-ma} gives participle, adjective, and noun analyses for a word, we discard the adjective and noun analyses---i.e., the \emph{entire} analyses, not just the \texttt{adjective} and \texttt{noun} categories, but all categories composing these analyses---because the participle analysis implies the other two.   
% %If a word w is a participle, then it is also an adjective. But the converse is not true. Consider the words bwgd, awhl, lwmd. These are all participles, but they are also all adjectives. But if there are two categories here, there should also be two clusters. And yet we cannot expect the algorithm to produce two identical clusters, one denoting 
% %"adjective" and the other "participle". This would be redundant, whereas the mcmm is seeking to compress the original data.
%
%\paragraph{Roots.}
%
%  \textsc{mila-ma} provides roots only for verbs, even though
%  nouns, adjectives, and adverbs often
%  contain roots. 
%  We retain \textsc{mila-ma}'s root categories, but union them with
%  the annotations
%%accompanying our data, i.e., the annotations of
%of \cite{daya-et-al:2008}. 
%
%% Root categories are thus the union of these two sets of word-to-root
%% mappings.
%
%\paragraph{Rootless words.} 
% There are two classes of words in Hebrew that generally lack roots: function words and loan words. The latter are usually nominals, e.g. \textit{svwdnt} `student' and \textit{ainvlqvwali} `intellectual'.
% Loan words are to some extent marked orthographically:
% \begin{itemize} 
% \item Whenever a pair of letters is homophonous, loan words systematically prefer one letter over the other. 
% \item For instance, loan words always use \textit{v} (\textit{tet}) 
% instead of \textit{t} (\textit{tav}) for the sound /t/, \textit{s} 
% (\textit{samex}) instead of \textit{e} (\textit{sin}) for the sound /s/, 
% and \textit{q} instead of \textit{k} for the sound /k/. 
%\item Most loan words are of English origin, and thus, due to the phonotactic patterns of English, certain character sequences such as \textit{sv} (i.e., /st/) occur over and over again. 
%\item %Moreover, 
%Loan words use the so-called \textit{vowel letters} to represent vowels more frequently and regularly than do native words, and unlike native words, they regularly use the letter \textit{a} (\textit{alef}, historically a glottal stop) to demarcate vowel-initial syllables, which were unknown in past forms of Hebrew. 
% \item %Finally, 
% Loan words tend to be longer than native words.
% \end{itemize}
%Function words, in contrast, such as particles and prepositions, are native to Hebrew and thus do not exhibit these distinctive orthographic patterns.
% To see if the MCMM is able to recognize any of the regularities observed in borrowed nominals, we introduce the category \texttt{rootless:Nominal}. 
% %We introduce the category \texttt{rootless} for all other rootless words.
%
%\paragraph{Definite prefix.}
%When the definite article prefix \textit{h-} follows the prefixal prepositions \textit{b-} (`in'), \textit{l-} (`to'), or \textit{k-} (`as, like'), it is deleted. Consequently, definite and indefinite forms are indistinguishable when they begin with any of these three prefixal prepositions. For example, 
%\texttt{bbit} can be `in the house' or `in a house' (because underlying \textit{b-h-bit} $\to$ \textit{bbit}). Because of this ambiguity,
%\textsc{mila-ma} outputs both the \texttt{definiteness:true} and \texttt{definiteness:false} analyses in such cases.
%We discard \texttt{definiteness:true} in these cases, since the deletion of the definite marker makes \texttt{definiteness:true} an unmarked category.
%The category \texttt{definiteness:false} is also discarded, since we discard all negative categories (as discussed above). 
%%The remaining categories in these two analyses may be discarded, modified, or left intact.
%% although the word in question remains definite. 
%
%\paragraph{Compound prefixes.}
%Modern Hebrew grammars and language-eduation textbooks often treat the prefix \textit{ke-} `when, while' as a single unanalyzable unit (cf. English words like \textit{into} and \textit{whenever}, which, while not prefixed clitics, are nonetheless compound function morphemes). 
%This is indeed the treatment adopted by \textsc{mila-ma}. However, the more traditional treatment is that \textit{ke-} is composed of the ``atomic" prefixal clitics \textit{k-} `as' and \text{e-} `that'. Without features pertaining to syntactic context, an MCMM would lack the evidence for merging \textit{k-} and \text{e-} into a single morpheme. We thus map the feature \texttt{temporalSubConj:ke} to the features 
%\texttt{prefix:k} and \texttt{prefix:e} wherever \texttt{temporalSubConj:ke} occurs in an analysis. We handle other compound prefixes similarly.


%\texttt{prefix:ke} $\mapsto$ \texttt{prefix:k} and \texttt{prefix:e}.
% (Notice that a lot of properties are packed into the suffix \textit{-s}).
% Let us consider  
% briefly the nature of an MCMM's output. One can represent a 
% clustering (or categorization) as a table in which the rows 
% correspond to the clustered (or categorized) items, and 
% the columns to the clusters (or categories) themselves. Now, 
% in the proposed dissertation, the items is feature-vector 
% representations of words, and the clusters is morphs. Each 
% $\text{cell}_{i,k}$ contains either 1 or 0: 1 if $\text{word}_i$ has 
% morph $k$, and 0 if it does not. 
%%What we have just described is essentially the MCMM's $\mathb{M}$ matrix. 
% Each 1 thus represents the presence of a particular morph. 
 
% If we wanted to get away from the matrix format, i.e., the grid 
% of 1's and 0's, we could replace each 1 with its morph and eliminate the 0's altogether. 
% Suppose, for example, 
% that $\text{word}_{1253}$ is \textit{runs}, which has two morphs, namely the stem 
% \textit{run} and the suffix \textit{-s} 3rd-person (\textsc{3p}), present-tense (\textsc{Pres}) singular (\textsc{Sg}) s.
% (Notice that a lot of properties are packed into the suffix \textit{-s}). 
% Suppose further that \textit{run} is cluster 87 and \textit{-s} is cluster 6, and the overall clustering 
% has a total of $K=500$ clusters, in which case $\mathbf{M}$ has 500 columns, and
% row 1253 has 498 zeros and only two 1's, one at column 6 and the other at column 87.
%We can replace the 
%1 in column 6 with \textit{-s} and the 1 in column 87 with \textit{run} 
%and discard the zeros, yielding ``runs: run, -s," which is tantamount a 
%morphological segmentation. 
%---
%Notice that the suffix \textit{-s} maps to three 
%``atomic" morphosyntactic categories, 
%namely 3rd-person (\textsc{3p}), present-tense (\textsc{pres}), 
%and singular (\textsc{sg}). 
%My system cannot learn abstract morphosyntactic labels like \textsc{3p} 
%and \textsc{sg}. Rather, it learns \emph{morphs}, 
%%the pre-morphosyntactic units of form. We will call these units \emph{morphs}, 
%which may or may not correspond to morphosyntactic categories (see the discussion in section~\ref{sec:targets}). 
%When there \emph{is} a 
%correspondence between morphs and morphosyntactic categories, it is often a 
%one-to-many mapping because the same morph can be
%requisitioned by more than one morphosyntactic category.
%---
 %can lay claim to the same morph. 
%In ``runs: run, -s," for example, the suffix \textit{-s} 
%represents the union of three ``atomic" morphosyntactic categories, namely \textsc{3p}, \textsc{pres}, and \textsc{sg}. 

%Thus, the output of an MCMM, after a little post-processing, can look like ``runs: run, -s." It is essentially a list of morphological segmentations. But more accurately, it is a list of word-to-cluster mappings; for each word, it will specify the cluster(s) to which it belongs. Note that the morphs \emph{run} and  \emph{-s} are essentially the labels of particular morphological clusters.
%The output of an MCMM is thus essentially a list of word-to-cluster mappings. Each item in this list is a word followed by a list of the clusters in which it has membership.
%%After little post-processing, it can look like ``runs: run, -s." 
%Morphs like \emph{run} and \emph{-s} are essentially cluster labels. 
%That is, \emph{-s} represents a cluster whose words predominantly end in  \emph{-s}.

\subsubsection{Orthographic data.}

%To obtain morphological analyses for the wordlist O, I will use the 
%MILA Morphological Analysis tool (\textsc{mila-ma}) \citep{hebrew-resources:2008}.
%Because \textsc{mila-ma} requires that input words be spelled 
%according to Modern Hebrew standard orthography, it can only be
%used to create a gold standard for orthographic wordlist O. The 
%gold-standard morphological analyses for the transcribed wordlists TS and TR 
%must come from a different source (see below).
%\textsc{mila-ma} is essentially a finite-state transducer. Because its morphological 
%knowledge has been manually coded by humans and its output is
%deterministic, it provides a good approximation to human
%annotation. 
%
%However, many of the original \textsc{mila-ma} categories are ill-suited to the purpose
%of evaluating an MCMM's clustering. The \textsc{mila-ma} categories are
%often atomic and abstract, e.g., \texttt{feminine} and \texttt{masculine}. 
%Such categories are purely morphosyntactic; they are meaningless at the 
%word-internal level because they can only be observed in agreement phenomena. 
%Moreover, there is no morphological unit in Hebrew that means strictly `feminine,' %(i.e., nothing more than `feminine' and nothing less). 
%nor is there one that means strictly ``masculine." Hebrew inflectional affixes 
%tend to be fusional, having meanings like ``feminine plural" and ``masculine plural."
%
%For this and similar reasons, \textsc{mila-ma}'s categories need to be mapped 
%to a modified set of gold-standard categories, i.e., categories that correspond 
%more closely to actual differences in form.
%The MCMM's clustering will then be quantitatively compared to the modified 
%\textsc{mila-ma}-based gold-standard categorization. 
%In particular, I used the measures \emph{average cluster-wise purity}, 
%\emph{BCubed precision} and \emph{BCubed recall}. The latter two are important 
%because they are specifically designed for cases of overlapping clusters 
%\citep{amigo-et-al:2009}.

\subsubsection{Transcribed data.} Gold-standard category mappings for the 
transcribed words were obtained by extracting morphological analyses from the Berman 
Longitudinal Corpus (BLC). Recall that for each utterance in the BLC, 
there is a transcription tier and a morphological-analysis tier. The latter provides a 
morphological analysis for each word in the utterance, including roots for the words 
that have roots. I extracted the morphological analyses and used them to create a list of 
word-to-category mappings. % resembling created from \textsc{mila-ma}'s analyses.
\subsubsection{Evaluation Metrics} 
\label{sec:metrics}
We evaluate the intrinsic results according to three metrics: \textbf{average purity}, \textbf{BCubed precision}, and
 \textbf{BCubed} recall. For the purpose of describing these metrics,
 let $U$ denote the set of $K$ clusters discovered by a system, $V$ the set of $J$ gold-standard categories, and $X$ is the set of data points to be clusters. The cardinality of $X$ is $N$; i.e., there are a total of $N$ individual data points that have been clustered.
\paragraph{Average purity}
The standard purity metric seeks to calculate the global correctness of a 
given clustering by determining the proportion of data points that have 
assigned to the correct cluster. Each individual data point has at some 
point been associated with a gold-standard \emph{category} label. Thus, 
in any given cluster, there are in effect as many gold-standard category 
labels as there are data points. Standard purity assumes that most frequent 
 gold-standard category in a given cluster $u_k$ is the gold-standard label for the entire cluster. The number of correctly clustered data points in $u_k$ is thus equal to the frequency of  $v_j$ in cluster $u_k$. Standard purity sums up the correctly clustered data points over all clusters, and divides this sum by $N$, the total number of distinct data points in the clustering
 %The reason $v_j$ is the most frequent category in $u_k$ is that it is associated with more of $u_k$'s members than any other data point .in the cluster is the same as the frequency of data points that bear the label. 
% Thus, the frequency of the most common gold-standard category in a cluster $U_k$  is deemed the number of correctly clustered items in $U_k$ x$ instances of this category label in the cluster, there are also $x$ data points associated the course equal to the number of data points associated with this category. data points he purity of this particular is thi
 The standard version of purity is computed as 
\begin{equation} \label{eq:pur1}
\text{purity}(U, V) = \frac{1}{N} \sum_{k \in K} \text{max}_{j \in J} |u_k \cap v_j|
\end{equation}
In other words, 
The problem for our purposes is that this version of purity assumes that each datapoint belongs to exactly only one gold-standard category. In fact, it requires that this be so, for if any data point should belong to more than one cluster belong to more than one cluster, the numerator in \eqref{eq:pur1} would be greater than than 1, and thus the resulting purity would exceed 1.
In the present study, each data point is a word, and the categories are morphological categories. In natural languages, words frequently belong to multiple morphological categories at once.
%however, which contains multi-category examples, this assumption can yield purities greater than 1. 

To avoid purities that exceed 1, we modify equation \eqref{eq:pur1} as follows: 
\begin{equation} \label{eq:pur2}
\text{purity}_{\text{avg}}(U, V) =  \frac{1}{K} \sum_{k \in K} max_{j \in J} |u_k \cap v_j|
\end{equation}
This new equation represents an \emph{average cluster-wise purity}; i.e., it computes each cluster's internal purity and then averages over these purities. %the mean of the $K$ clusters internal purities. 
While this equation yields purities within $[0, 1]$, even 
when clusters overlap, it retains the well-known bias of the purity metric toward small clusters. We thus incorporate other metrics. 

\paragraph{BCubed precision and recall}
The metrics \emph{BCubed precision} (BP) and \emph{BCubed recall} (BR) \citep{bagga-and-baldwin:1998} evaluate a clustering by checking one 
pair of data points at a time, comparing the relationships that the algorithm has posited for each pair against their 
gold-standard relationships. These metrics are well-suited to cases of overlapping 
clusters \citep{amigo-et-al:2009}. 
In such cases, it is possible for two data points $x$ and $y$ to overlap both in their algorithm-assigned clusters and their 
gold-standard categories. Suppose that $x$ and $y$ overlap in $m$ clusters and $n$ categories. Ideally, $m$ would 
equal $n$, in which case there would be a one-to-one correspondence between clusters and gold-standard categories. 
In imperfect cases, however, either $m$ will be less than $n$ or vice versa. BCubed precision essentially 
measures the extent to which $m \leq n$; for if $m>n$---i.e., if $x$ and $y$ co-occur in more clusters than gold-standard categories, then the algorithm has posited at least one 
spurious relationship between $x$ and $y$. BCubed precision thus penalizes false co-memberships between pairs of data points. 
By contrast, BCubed recall measures the extent to which $m \geq n$; for if $n<m$, then the algorithm 
has missed at least one gold-standard relationship between $x$ and $y$. BCubed recall thus 
penalizes missing co-memberships. % (i.e., missed gold-standard relationships). 
At the core of BCubed precision is the measure multiplicity precision ($MultiP$):
%\begin{equation}
%MultiP(x,y) = \frac{min|U(x) \cap U(y)|, |V(x) \cap V (y)|}{|U(x) \cap U(y)|}
%\end{equation}
\begin{equation}
\text{MultiP}(x,y) = \frac{\text{min}(|U(x)\cap U(y)|, |V(x) \cap V(y)|)}{|U(x) \cap U(y)|}
\end{equation}
where $x$ and $y$ are data points in $X$; $U(x)$ is the set the clusters that contain $x$, 
and $U(y)$ is the set of clusters containing $y$.
Finally, 
$V(x)$ and $V(y)$ are the sets of gold-standard categories for $x$ and $y$. 
Notice that $MultiP(x,y)$ is a comparison of two items $x$ and $y$, neither of which is the 
particular focus of the measure. $MultiP(x,y)$ is thus a joint description $x$ and $y$. 
To obtain a precision value that describes $x$ alone, one must compute $Avg_{y}(MultiP (x,y))$, the 
average $MultiP(x,y)$ over $y \in X$, by computing $MultiP(x,y)$ for every $y$ that shares a cluster with $x$ 
and averaging the resulting values. To obtain a precision value that describes the whole dataset (not just a single $x$), 
one must compute $Avg_y(\text{MultiP}(x,y))$ for every $x$ in $X$ and then take the average of these averages. 
This average of averages is BCubed Precision. 
%\begin{equation}
%BP=Avg_x [ Avg_{y.U(x) \cap U(y)\neq \emptyset}(MultiP(x,y))]
%\end{equation}
\begin{equation}
BP=Avg_x [Avg_{y.U(x) \cap U(y) \neq \emptyset}(\text{MultiP}(x,y))]
\end{equation}
The measures multiplicity recall(MultiR) and BCubed recall (BR) are analogous to multiplicity precision and
 BCubed precision. The computation of MultiR is identical to that of MultiP 
 except that gold-standard categories replace clusters in the denominator: 
\begin{equation}
MultiR(x,y) = \frac{min(|U(x) \cap U(y)|, |V(x) \cap V (y)|)}{|V(x) \cap V(y)|}
\end{equation}
Likewise, BCubed recall is is nearly identical to BCubed precision except that MultiR replaces MultiP, and the set V replaces U in the $\text{Avg}$ subscript expression:
\begin{equation} 
BR = Avg_x [Avg_{y.V(x) \cap V(y) \neq \emptyset}(\text{MultiR}(x,y))]
\end{equation}

\subsection{Extrinsic Evaluation} \label{sec:eval-extrinsic} An \emph{extrinsic evaluation} 
views a system as a component of a larger, or outer, system. 
Its purpose is to evaluate the embedded system, but it does so by evaluating the outer 
system. If the outer system scores highly,
the embedded system, i.e., the system under evaluation, scores highly.
An extrinsic evaluation makes sense for my system for two reasons:

tic\.{t}ar\.{k}\'{i} %c:\[2\] 8:\[4,5\] 43:\[5,7\] \UTF{1E6D}:\[3\] \UTF{1E33}:\[6\] 22:\[0,1\]

\begin{enumerate}
\item Multimorph is essentially an embedded system by nature: It learns morphs, which are intermediate units, intended to facilitate the learning of morphemes. %My system is thus meant to be an embedded component of a larger process. 
\item While it would be very difficult to come up with gold-standard morphs, gold-standard morphemes are relatively easy to produce, since morphemes, in contrast to morphs, are already well-defined. 
%As intermediate units, the value of morphs lies in their utility, i.e., in their capacity for yielding correct morphemes. What they look like is not important as long as they are effective. is evaluate \emph{morphemes} that have been induced from morphs. 
%On the other hand, it is relatively easy to come up with gold-standard morphemes, since morphemes, in contrast to morphs, are well-defined. 
\end{enumerate}

The extrinsic evaluation will consist of the four stages described below. 
Stages 1 to 3 prepare the output of an MCMM to be fed to Morfessor in Stage 4. 
To obtain the gold-standard datasets for Morfessor, I manually segmented 
$\frac{1}{10}$ of the original, unprocessed wordlists, i.e., both the transcribed 
and orthographic wordlists. Note that this four-stage extrinsic evaluation only considers 
stem-external, concatenative morphology. This is because Stage 3 effectively removes 
interdigitation. Morfessor is not even capable of handling interdigitation, since it is 
a sequential algorithm.

But how is one to assess the output's quality? How do we tell how \emph{good} it is? I need a way to evaluate the clusters produced by the MCMM.
But a cluster is just a group of words that Multimorph has seen fit to put together according to criteria of its own devising.
These criteria could be virtually anything, as Multimorph is directed solely by an algorithm, not by any previously attained knowledge concerning
the workings of morphology or human language. 
Thus, the \emph{meaning} of one of Multimorph's clusters may not be immediately obvious.
 
 
%\subsection{} \label{sec:paradigms}
\subsection{Four-stage process.} \label{sec:extrinsic}
Multimorph's output 
will have to processed before it can be evaluated. This subsection 
will describe each of the four stages. The input to Stage 1 is a cluster centroid vector, 
i.e., one the $K$ columns in the $J \times K$ matrix $\mathbf{C}$. The $k$th column 
corresponds to the $k$th cluster. Each $j \in J$ corresponds to a particular feature; the 
$J$ rows correspond to the same $J$ features present in each original data point 
$\mathbf{x}_{i}$ as well as each reconstructed data points $\mathbf{r}_{i}$, where 
$0 \ge i < I$, and $I$ is the total number of data points (original and reconstructed).

\subsubsection{\textsc{Stage 1:} Interpret clusters as morphs.} Each cluster ultimately represents a unit of morphological organization, i.e., a morph. But as a cluster, it is too abstract to be directly relatable to actual words, i.e., to strings consisting of characters. The first stage is therefore
%in the four-stage extrinsic evaluation process is 
to make the clusters relatable to strings. In particular, each cluster is a regular expression that is derived from the active features of its centroid. A morph's regular expression is built up from ``atomic" regular expressions derived from its individual active features. 

The form of such an expression depends on the type of the feature 
upon which it is based. For example, a positional feature such as 
\texttt{a@1} manifests simply as \texttt{(a)}. (The parentheses 
are important for \textsc{stage 2}, in particular, for retrieving 
matching characters if morph's regular expression can be matched 
to a word.) The regular expressions of precedence features take the 
form \texttt{(x)(.?)}$\times (\delta-1)$\texttt{(y)}; i.e., the number 
of \texttt{.?}s depends on the value of the parameter $\delta$; in 
particular the number of \texttt{?} is equal to $\delta- 1$. Thus, 
$\delta$ values of 1, 2, and 3 correspond to 0, 1, and 2 \texttt{.?}s,
 respectively. 

If a cluster's centroid has more than one active feature, an 
atomic regular expression is derived from each such feature, 
and if the features are compatible, the atomic regular expressions 
are combined to create a composite expression. For example, the 
positional features are compatible if the indices are consecutive, 
as in \texttt{h@[0]} are \texttt{a@[1]}. would be combined 
to form the composite expression \texttt{(h)(a)}. Precedence features 
can also create composite regular expressions. For example, the 
features \texttt{d<b} and \texttt{b<r} would together yield the 
composite expression \texttt{(d).?(b).?(r)}, that is, if $\delta = 2$. 
In general, two precedence features \texttt{a<b} and \texttt{c<d} 
merge if $b=c$.

The output of Stage 1 is ultimately an array of \emph{morph objects}.
 Each morph object specifies values for certain attributes, the main ones 
 being a morph ID (i.e., a unique integer) and a regular expression derived 
 from the features of a particular cluster centroid. Stage 1 is thus
  concerned the clusters in and of themselves and their interpretation. 
  Stage 2, to which we turn next, is concerned with mapping clusters, i.e., 
  morphs, to the characters of actual words. 

%\begin{figure}[t]
%\centering
%\begin{subfigure}[a]{0.3\textwidth}
%\begin{tabular}{cc}
%Morph ID & Regular Expression \\ \hline
%3 & \texttt{(u)} \\
%31 &  \texttt{(f)} \\
%64 &  \texttt{(h).?.?(i)}  \\
%84 & morph ptn: \texttt{(\UTF{017E})}  \\
%95 &  \texttt{(l).?.?(v)}  \\
%148 &   \texttt{(c).?.?(\'{a})} \\
%151  &  \texttt{(i).?.?(l)} \\
%202 &  \texttt{(i).?.?(\'{a})}  \\
%264 &  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%284 &  \texttt{(l).?.?(t)}  \\
%360 &  \texttt{(r)} \\
%\begin{figure}[t]
%\label{fig:stage2}
%	%\subfigure[List of morphs initially associated with \textit{hicl\'{a}xti}]{
%	\begin{tabbing}
%	\hspace{0.6in} \= \hspace{5.5in} \kill
%	Morph ID \> Regex \\ 
%                3 \> \texttt{(u)} \\
%                31 \>  \texttt{(f)} \\
%                64 \>  \texttt{(h).?.?(i)}  \\
%                84 \>  \texttt{(\v{z})}  \\
%                95 \>  \texttt{(l).?.?(v)}  \\
%                148 \>   \texttt{(c).?.?(\a'{a})} \\
%                151  \>  \texttt{(i).?.?(l)} \\
%                202 \>  \texttt{(i).?.?(\a'{a})}  \\
%                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%                284 \>  \texttt{(l).?.?(t)}  \\
%                360 \>  \texttt{(r)} \\
%	\end{tabbing}
%	%}
%	%\subfigure[Mapping from Morph IDs to character indices]{
%%hicl\'{a}xti \quad 64:[0,1], 264:[2,3,5,6], 202:[1,4], i:[7], 148:[2,4], 151:[1,3], 284,[3,6], 95:[3,4]
%%}
%\end{figure}

%\end{tabular}
%\caption{Morphs associated with \textit{hicl\'{a}xti} and their regular expressions.}
%\label{fig:noise-clusters}
%\end{subfigure}
%\begin{subfigure}[ hicl\'{a}xti ]{0.3\textwidth}
%hicl\'{a}xti \quad 64:\[0,1\], 264:\[2,3,5,6\], 202:\[1,4\], i:\[7\], 148:\[2,4\], 151:\[1,3\], 284:\[3,6\], 95:\[3,4\]
%\label{fig:mapping}
%\end{subfigure}
%\end{figure}

\subsubsection{Stage 2. Match morphs to words.} 
Each of the\emph{covered} words in an MCMM clustering is a member of at least one and possibly many clusters. Some words belong many clusters, and sometimes the relationships between clusters and their member words, i.e., the reason why a would should be included in a given clusters, is not clear. Indeed, in any given clusters, most clusters are likely to be noisy to some extent. Sometimes the active features of a cluster do not seem match the characters of a member word. Example \ref{ex:noise-clusters} lists the morphs, i.e., their morph IDs and regular expressions, which one experiment ($s = 4,\delta = 3$) associated with the word \textit{hicl\'{a}xti} had some, where ``associated" means that \textit{hicl\'{a}xti} was member of the clusters from which these morphs were derived.

%\begin{exe}
%\label{ex:noise-clusters}
%	\begin{tabbing}
%	\hspace{1in} \= \hspace{5.5in} \kill
%	Morph ID \> Regex \\ 
%                3 \> \texttt{(u)} \\
%                31 \>  \texttt{(f)} \\
%                64 \>  \texttt{(h).?.?(i)}  \\
%                84 \>  \texttt{(\v{z})}  \\
%                95 \>  \texttt{(l).?.?(v)}  \\
%                148 \>   \texttt{(c).?.?(\a'{a})} \\
%                151  \>  \texttt{(i).?.?(l)} \\
%                202 \>  \texttt{(i).?.?(\a'{a})}  \\
%                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%                284 \>  \texttt{(l).?.?(t)}  \\
%                360 \>  \texttt{(r)} \\
%	\end{tabbing}
%\end{exe}	
Stage 2 must take such a list and establish from it a mapping from word characters to morphs. To this end, it considers each candidate morph's regular expression, attempting to match it with the word in question. If there is no match, the morph is discarded. For example, the expressions \texttt{(\v{z})} and \texttt{\(l\).?.?\(t\)} fail to match \textit{hicl\'{a}xti}. 

The end result of this process is a mapping from morphs (i.e., morph IDs) to characters (i.e. character indices, as follows:
\begin{exe}
\ex \label{ex:noise-clusters}
	\begin{tabbing}
	\hspace{1in} \= \hspace{5.5in} \kill
	Morph ID \> Regex \\ 
                3 \> \texttt{(u)} \\
                31 \>  \texttt{(f)} \\
                64 \>  \texttt{(h).?.?(i)}  \\
                84 \>  \texttt{(\v{z})}  \\
                95 \>  \texttt{(l).?.?(v)}  \\
                148 \>   \texttt{(c).?.?(\a'{a})} \\
                151  \>  \texttt{(i).?.?(l)} \\
                202 \>  \texttt{(i).?.?(\a'{a})}  \\
                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
                284 \>  \texttt{(l).?.?(t)}  \\
                360 \>  \texttt{(r)} \\
	\end{tabbing}
\end{exe}
\begin{exe} \ex \label{ex:mapping} hicl\'{a}xti \quad \texttt{64:[0,1]; 264:[2,3,5,6]; 202:[1,4]; i:[7]; 148:[2,4]; 151:[1,3]; 284:[3,6]; 95:[3,4]}
\end{exe}

%Map the Morph objects regular expression onto the characters of the word itself. This is done primarily through the matching of a regex to an input string. This will produce a lattice of potential morphID sequences, etc. 

\subsubsection{\textsc{stage 3:} Compress and Encode.}
A single, optimal path through the word was computed from Stage 2's mapping. In the case of (\ref{ex:mapping}), for example, some morphs can be eliminated some because some share character indices.  After eliminating morphs \texttt{151}, \texttt{202}, \texttt{284}, we arrive at the following compressed sequence of morph IDs computed at the end of Stage 2 is converted is optimized and compressed to yield a single best path through the word. 
 the best sequence or path and then compress it. 
 %That is, remove repeated instances of the same morphID. 
\begin{exe} \ex \label{ex:comp-with-indices} 
hicl\'{a}xti \quad \texttt{64:[0,1], 148:[2,4], 264:[2,3,5,6], i}
\end{exe}
Note the \texttt{i} at the end of the sequence; this is an example of``orphan" alphabetic character that was never associated with a morph. Such stranded characters were not uncommon. They were retained so that every character would be accounted for.
%The idea is to abstract away from the individual characters as much as possible and allow morph to be a single, atomic entity, with no internal structure.] 
The purpose of this stage, however, is to abstract away from the individual characters as much as possible, so that each morph could be treated as a single, atomic entity, with no internal structure.
Thus, the compressed sequence in (\ref{ex:comp-with-indices}) would be look more like the following:
\begin{exe} \ex \label{ex:comp-no-indices} 
hicl\'{a}xti \quad \texttt{64, 148, 264, i}
\end{exe}
The last step in Stage 3 is to \emph{encode} the compressed morph ID sequence by replacing each  ID number with a unique, atomic unicode character and joining the characters together to form a string a string, e.g., a four-character string in the case of (\ref{ex:comp-no-indices}).
I used the characters of the vast CJK unicode block for this purpose , but in principle any characters could be used, as long as they are atomic and no character is used twice. For the sake of the present discussion, we shall call this compressed and encoded dataset the \emph{experimental} set, as it is the experimental dataset \emph{within} the extrinsic evaluation. That is, it is the experimental input to Morfessor in Stage 4 (see below). 

We must note here that there was \emph{control} dataset for each experimental dataset (and an experimental dataset for each combination of the $s$ and $\delta$ parameters. Each control set consisted of the same words as its corresponding experimental set, except that the control words were the original, untouched words. Ten percent of the original words had been previously selected at random. These were segmented manually in order to serve as a gold-standard file. 
%  ten percent sample of the data was previous %, having undergone none of the stage 

%< Run morfessor on the control wordlist. This list contains the same words as the compressed dataset, except they are not compressed.>
 
\subsubsection{\textsc{stage 4:} Segment, Decompress, Evaluate}
 Morfessor is now run on both the experimental and control files and thus produces segmentation files (or models), one control and one experimental, the latter still consisting of encoded words, except that the encoded words are now segmented.
 
% original (or ``normal") words, while the other comprised segmentation of the encoded versions of the original words. 
 
 %Note, however, that even the encoded words of the experimental file had started out as original words, they so altered in their encoded forms that they generally bore no resemblance to their original forms.
 
% described  computes a segmentation for each compressed/encoded word in the experimental set. We also run Morfessor on the control dataset, thus obtaining segmentations of the original words. We at this point have two segmented files, but the words in the experimental file are still encoded. 
%A ten percent sample of the data was previous  10 percent sample of these segmented compressed words and \emph{decompress} them, i.e.,

To evaluate both files against the same gold-standard, the experimental file must now be decoded. To avoid altering  Morfessors segmentation decisions, each morfessor-computed segment (which may consist of multiple morphs) was decoded separately. The decoded segments were then reassembled. Morfessor's evaluation utility was used to evaluate each model against the gold standard. 

In the case of our example word \textit{hicl\'{a}xti} which we have been using throughout this section, Morfessor was given the compressed input in (\ref{ex:comp-no-indices}), which was four symbols long, and output a segmentation consisting of two segments, with the delimiter placed just before the \textit{i}. The first segment thus comprised the three \emph{symbols} corresponding to to the morphs 64, 148, and 264, and the second segment was the \emph{i}. In the final decoding process, the morphs 64, 148, and 264 together map to the substring hicl\'axt, and thus Morfessor's segmentation becomes hicl\'axt + i.
%the the preserving Morfessor's segmentations/segments). 
%[example]
%The reason we that now have to decompress these words (segmentations) is that we need to evaluate them and the control segmentations against the same gold standard segmentations. The experimental segmentations thus need to be comparable to the control segmentations; i.e., they at least need to be composed of roughly the same characters. 

%\subsubsection{Stage 1: Identify the morph characters} The first stage is to map each cluster's set of active features to a particular sequence of alphabetic characters. This sequence is regarded as the \emph{morph} corresponding to the cluster. Also in this stage, each morph is labeled as a prefix, stem-component, or suffix.
%
%\begin{enumerate}
%  \item Mapping from features to root or pattern characters:
%    \begin{enumerate}
%   	\item Only precedence [and bigram features] can map to root and pattern characters. Positional features never can.
%   	\item There must be at least one precedence feature [or one bigram feature] for each root-character bigram. (Note that precedence features are themselves basically bigrams.) These features must also overlap; e.g., the features \texttt{a<b} and \texttt{b<c}, which overlap at \textit{b}, indicate the root \textit{a.b.c}. %Note that roots can also be indicated by bigram features as well as combinations of bigram and precedence features (e.g., \texttt{a+b} and \texttt{b<c}).
%     \end{enumerate}

%   \item Mapping from features to prefix characters:
%   \begin{enumerate}
       %\ex \label{ex-1a} 
%       \item If there are at least two features of the form \texttt{a<b} and \texttt{a<c}, such that \textit{b} $\ne$ \textit{c}, then \textit{a} is at least part of a prefix. But what if the precedence features have an abstract component, as in the following?
%       \begin{itemize}
%       \item \texttt{x<C}
%       \item \texttt{x<V}
%       \end{itemize}
%       How does one determine inequality between abstract characters? Answer: C $\ne$ V. So there can still be inequality, just less of it. 
       %If there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, such that \textit{t} $\ne$ \textit{b}, \textit{t} $\ne$ \textit{c} (see above), then the prefix is either \textit{at-} or \textit{ta-}, respectively. 
%       \ex When there is a character \textit{a} satisfying rule \ref{ex-1a} has more than one character can only be determined by \emph{bigram} features. In particular, if there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, such that \textit{t} $\ne$ \textit{b}, \textit{t} $\ne$ \textit{c} (see above), then the prefix is either \textit{at-} or \textit{ta-}, respectively.
       %\ex 
%       \item If there is at least one positional feature of the form \texttt{a@[}$x$\texttt{]}, where $x$ is a positive integer, then \textit{a} is at least part of a prefix. If there are two or more consecutive positional features, i.e., features like \texttt{a@[}$x$\texttt{]}, \texttt{b@[}$x+1$\texttt{]}, \texttt{c@[}$x+2$\texttt{]}, and so on, then the prefix is the entire string of characters indicated by these features.
%    \end{enumerate}
%
%  \item Mapping from features to suffix characters:
%   \begin{enumerate}
%   \item If there are at least two features of the form \texttt{b<a} and \texttt{c<a}, such that \textit{b} $\ne$ \textit{c}, then \textit{a} is at least part of a suffix. If there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, where \textit{t} is a different character than either \textit{b} or \textit{c} (see above), then the prefix is either \textit{-at} or \textit{-ta}, respectively.
%
%   \item If there is at least one positional feature of the form \texttt{a@[}$x$\texttt{]}, where $x$ is a negative integer, then \textit{a} is at least part of a suffix. If there are two or more consecutive positional features, i.e., features like \texttt{a@[}$x${]}, \texttt{b@[}$x-1${]},\texttt{ c@[}$x-2${]}, and so on, then the suffix is the entire string of characters indicated by these features.
%   \end{enumerate}
%  \item If a cluster centroid has conflicting active features, then the cluster is void; it does not correspond to any morph.
%\end{enumerate}
%%EXAMPLES
%
%\begin{exe}
%%Cluster #0 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active features: e+w (1.0), e+i (1.0), l@[0] (1.0), e@[0] (1.0), e$<$w (0.9713), l+w (0.9466), e$<$i (0.8878), l<w (0.7655), e@[1] (0.7321), e+t (0.7049) \\
%Morph: None \\
%Pertinent rule: 4
%%
%%Cluster #2 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active feature: w+i (1.0), l+i (1.0), i+m (1.0), i+i (1.0), w$<$i (1.0), i@[-2] (1.0), m@[-1] (1.0), l<i (0.9941), i<i (0.8206), l+m (0.7164) \\
%Morph: -liim (suffix) \\
%Pertinent rule: 3b
%%
%%Cluster #5 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%%Active features: m+i (1.0), i+m (1.0), m<i (1.0), i@[-2] (1.0), m@[-1] (1.0), m+m (0.9825), m@[0] (0.8077), w+m (0.7786), m+w (0.7006), m<m (0.6840)
%%Morph: None
%%Pertinent rule: 4
%%
%%Cluster #6 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active features: b@[0] (1.0), b$<$w (0.9285), b$<$i (0.9232), b+t (0.6000), b+r (0.5867), b$<$t (0.4608), b+m (0.4576), b@[1] (0.4512) \\
%Morph: b- (prefix) \\
%Pertinent rules: 2a and 2b 
%%
%%Cluster #13 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: p$<$q (1.0), p$<$d (0.9999), q$<$d (0.9999) \\
%Morph: p.q.d. (root)  (The feature p*d is not relevant, as it turns out.) \\
%Pertinent rule: 1b
%%
%%Cluster #1 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: w$<$t (1.0), w@[-2] (1.0), t@[-1] (1.0) \\
%Morph: -wt (suffix) \\
%Pertinent rule: 3b
%%
%%Cluster #26 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: r$<$i (0.99), r@[-3] (0.98), r+i (0.96), i@[-2] (0.9), i+r (0.84), r+m (0.8), h+r (0.8), r$<$m (0.79), r+t (0.74), w+r (0.68) \\
%Morph: None (There are active features for both a prefix and a suffix.) \\
%Pertinent rule: 4
%%
%%Cluster #184 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active feature: p$<$g (1.0) \\
%Morph: None \\
%Pertinent rule: 1b 
%%
%%Cluster #201 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: x$<$z (1.0), z$<$q (0.98) \\
%Morph: x.z.q. (root) \\
%Pertinent rule: 1b
%%
%%Cluster #219 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: d$<$h (1.0), h$<$z (1.0), z$<$d (0.99), z$<$t (0.99) \\
%Morph: None \\
%Pertinent rule: 4
%\end{exe}
%
%Via Chinese: Every symbol in an encoded morfessor segment is itself a distinct morph.
%
%\subsubsection{Stage 2: Match morph characters to word characters} Once a morph has been gleaned from a cluster's centroid vector, the characters of the morph must be matched to the corresponding characters in the cluster's member words. 
%
%The $\mathbf{M}$ matrix contains the cluster activities for each word. By consulting $\mathbf{M}$, we can ascertain the set of clusters to which each cluster belongs. Each cluster can be thought of as essentially equivalent to a particular morph, since the identities of morphs come from clusters. Indeed, Stage 1 ``extracts" from each cluster a $K$-length vector that  contains the key alphabetic characters associated with the cluster. Thus, via the cluster activities, each word is mapped to its set of morphs.
%
%At this point we will know which morphs are associated with each word, but we do not yet know the order of the morphs in words with more than one. Additionally, we will know, for each morph, whether it is a prefix ($P$), suffix ($SU$), or stem-component ($ST$). For example, suppose that the word \textit{whxlwm} (`and the dream') belongs to four clusters, namely those corresponding to the morphs \texttt{w}_{P}, \texttt{h}_{P}, \texttt{xlm}_{ST}, \texttt{w}_{ST}, where the subscripts $P$ and $ST$ stand for \textit{prefix} and \textit{stem-component}, respectively.
%
%First, the prefixes are matched to word characters. Then stem-components are matched, and finally any suffixes are matched (there are no suffixes in the present example). Whenever a morph character matches a word character, the word character is popped from the word, and the morph character is popped from the morph. The word character is then linked to the morph in question. 
%
%Thus, in our example, the matching algorithm proceeds as follows: First, each of the prefixes \texttt{h}_{P} and \texttt{w}_{P} (the order should not matter) are compared to the first character of \textbf{whxlwm}. The \texttt{w}_{P} matches, so we get \{ \texttt{w}_{P}:\texttt{w} \}. The \texttt{w} is popped 
%
%from \textbf{whxlwm} as well as from \texttt{w}_{P}, thus completing the prefix morph \texttt{w}_{P} and removing it from consideration. Next, \texttt{h}_{P} (the only remaining prefix) is matched to the \texttt{h} of \textbf{hxlwm}, leaving us with \{  \texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h} \}, \textbf{xlwm}, and no more prefixes.
%
%Now only the stem-components remain. \texttt{w}_{ST} fails to match the first character 
%of \textbf{xlwm}. However, the \texttt{m} of \texttt{xlm}_{ST} does, yielding
% \{ \texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:m\}, 
% \textbf{lwm}, an stem-components {\texttt{lm}_{ST} and \texttt{w}_{ST}. 
% The \texttt{q} of {\texttt{lm}_{ST} matches the \texttt{q} in \textbf{qm}, giving us 
% \{\texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:\texttt{mq}\}, 
% \textbf{wm}, and stem-components \texttt{m}_{ST} and 
% \texttt{w}_{ST}. Next, \texttt{w}_{ST} matches the \texttt{w} in \textbf{wm}, 
%completing the stem-component \texttt{w}_{ST}. We now have \{\texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:\texttt{mq}, \texttt{w}_{ST}:\texttt{w}\}, \textbf{m}, and  
% \texttt{m}_{ST}.
% Finally, \texttt{m}_{ST} matches \textbf{m}, completing \{\texttt{xlm}_{ST}\} and consuming the word's last remaining character.
%%	\begin{verbatim}
%%		.*(x).*(\u00F3).?.?(t).*
%%	\end{verbatim}
%%	\texttt{.*(x).*(\'{o}).?.?(t).*}
%\subsubsection{Stage 3: Compress}. Here, each morph to an atomic symbol, 
%so that, e.g., a three-character morph becomes in effect single-character item, 
%as do all morphs consisting of more than one character. In this way, most of 
%the words is shortened (in terms of character-count) and thus compressed.
%Each atomic morph symbol is a unique unicode character. For example, 
%consider the 
%Hebrew words \textit{magdil} and \textit{gadol}, which share the root 
%\textit{g.d.l}. 
%Suppose that the Stage 2 outputs for these words are as in \eqref{ex:magdil} and 
%\eqref{ex:gadol}, respectively. 
%\begin{exe}  \ex \label{ex:unicode} \begin{xlist}
%	\ex magdil \quad ma-, \,\, g.d.l, \,\, i 
%	\label{ex:magdil}
%	\ex gadol \, \quad  g.d.l, \,\, a.o
%	\label{ex:gadol}
%	\end{xlist}
%\end{exe}
%Altogether, there are four \emph{unique} morphs in \eqref{ex:unicode}, namely \textit{ma-}, \textit{g.d.l}, 
%\textit{i}, and \textit{a.o}.
%Stage 3 will map each of these to a unique symbol, as in \eqref{ex:map}, for instance.
%\begin{exe}
%	\ex  \textit{ma-} $\mapsto$ \$ \quad \textit{g.d.l} $\mapsto$ \% \quad
%\textit{i} $\mapsto$ \& \quad \textit{a.o} $\mapsto$ \#
%\label{ex:map}
%\end{exe}
%Finally, each word is reassembled with atomic symbols being substituted for the morphs. 
%They are put together in the original order of their corresponding morphs.
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex magdil \quad \$\%\&
%	\label{ex:re-magdil}
%	\ex gadol \, \quad \%\#
%	\label{ex:re-gadol}
%	\end{xlist}
%\end{exe}
%% The atomic symbols are put together in the order of their corresponding character sequences, as illustrated in \eqref{ex:reassembled}. 
%Note, however, that in the case of interdigitation, the relative order of atomic symbols corresponding to interleaved sequences must be decided arbitrarily.
%
%\subsubsection{Stage 4: Test} 
%Two input files, a test and a 
%control file, are now fed to Morfessor. 
%The test file is the output of Stage 3.  %will contain the test data, i.e., the compressed words from Stage 3. 
%The control file consists of the original, unaltered words; its purpose is to serve as a baseline for measuring the effect 
%of the compression carried out in Stage 3. 
%% But what gives
%The idea here is to see if the MCMM's morphs, 
%now represented as atomic symbols, aide the 
%process of morphological segmentation.  % But how can we tell if the atomic symbols help?
%% What are they supposed to help?  Are they supposed to aide in the process of discovering the actual morphemes (i.e., the non-compressed, non-reduced morphemes)? If so, we need to somehow translate atomic symbols back into character sequences, but retaining the segmentation divisions computed on the strings of atomic symbols.
%Morfessor induces morphological segmentations for each file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, the atomic symbols were converted back into morphs (i.e., Stage 3 was undone).
%\begin{exe}
%	\ex   \$ $\mapsto$  \textit{ma-}
%	\quad \% $\mapsto$ \textit{g.d.l} 
%	\quad  \& $\mapsto$ \textit{i}
%	\quad  \# $\mapsto$ \textit{a.o}
%\label{ex:map}
%\end{exe}
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex  \$ \, + \, \% \, + \,  \& \quad $\mapsto$ \quad  ma \,+ \, gdl  \, + \, i
%	\label{ex:reconverted-magdil}
%	\ex  \% \, + \, \# \quad $\mapsto$  \quad gdl \, + \, ao
%	\label{ex:reconverted-gadol}
%	\end{xlist}
%\end{exe}
% The result is a test segmentation file whose words have the same characters as those of the control file, but possibly quite different segmentations. The two segmentations were evaluated against a common gold standard.
%%That is, do they make the task easier? 
%%Do they improve segmentation accuracy? 
%%Morfessor %then
%% induces morphological segmentations for each file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation will at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, change the atomic symbols back to morphs (i.e., undo Stage 3). Finally, evaluate the two segmentations against a common gold standard.
%
%\subsection{Gold-standard data}
%Recall that Morfessor is a nonlinear \emph{sequential} algorithm; that is, it possesses nonlinearity, but not nonsequentiality (see section~\ref{sec:nls})
%and is thus incapable of detecting non-concatenative roots and patterns. 
%Moreover, interdigitation is lost when atomic symbols are substitute for character sequences. 
%Two basic types gold-standard data were therefore required: one to assess Morfessor's output and one to evaluate the 
%stem-internal root-and-pattern morphology.
% 
%\paragraph{Morfessor gold standard}
%%I will need gold-standard segmentations against which to assess Morfessor's output.  
%Morfessor produces two output (or analysis) files, one for the test file (processed words) and one for the control file (original or uprocessed words)
%To obtain gold-standard datasets for Morfessor, I manually segmented $\frac{1}{10}$ of the original, unprocessed wordlist.
%This amounts to a total of three unprocessed wordlists: a list of transcribed words, a list of transcribed words with stress marked, and a list of words spelled according to orthographic conventions. 
%
%\paragraph{Non-concatenative gold standard}
%To evaluate my system's performance on non-concatenative morphology, I need gold-standard roots for both the transcribed wordlists and the orthographic data. For the transcribed data, I extracted root annotations from the CHILDES morphological analyses. 

%For the orthographic wordlist, I will take advantage of the root annotations provided in the original dataset of \cite{daya-et-al:2008} (see section~\ref{sec:data}).
% 
%\paragraph{Gold-standard annotation}
%How are we going to figure this out? The control will serve as the baseline. 
%But we also need a gold standard segmentation to provide a frame of reference for comparing the test and control segmentations. 
%That is, the respective accuracies of the test and control segmentations 
%is measured with respect to the gold standard.

%How large does the gold-standard set need to be? 1000 words? 1/10 of the total data set?

%\subsection{Ancillary (Gold-Standard-Based) Evaluation}
%This evaluation will involve mapping \textsc{mila-ma}'s categories onto different sets of gold-standard categories. How will these sets differ? The idea, I guess, is to use sets with varying amounts of granularity, or different amounts of modification, or perhaps different types of modification.
%For example, do we want the categories to create a strict partition? Maybe the gold-standard categories could/should overlap.
%
%An MCMM clusters its input vectors (= words) according to shared hidden-unit activations. 
%Its clusters overlap one another; i.e.,
%a single word can belong to several clusters at once. % because a word can contain multiple morphemes. 
%%Evaluating overlapping clusters is more complicated than evaluating disjoint clusters. 
%To evaluate these clusters,
%I will use the measures \emph{BCubed Precision} and \emph{BCubed Recall}, 
%which are specially designed for overlapping clusters \citep{amigo-et-al:2009}.
%%Precision and recall evaluate a system's output against an external gold standard. 
%
%I will use a
%finite-state morphological analyzer, namely the MILA Morphological Analysis tool (\textsc{mila-ma}) 
%\citep{hebrew-resources:2008} to generate gold-standard categories. It is, however, non-trivial to apply \textsc{mila-ma} to this purpose.
%is not entirely straightforward. First, there is the general problem of evaluating an unsupervised clustering algorithm
%against external criteria. One usually has an idea about what the output should be, 
%but without a training set,
%it is difficult to be specific about this. 
%Second, one must consider that
%\textsc{mila-ma}'s particular categories may not be appropriate for cluster evaluation in every case.
%
%For example, \textsc{mila-ma} tends to use atomic categories such as \textsc{masc} and \textsc{pl} as opposed to \textsc{masc.pl}. In the case of Hebrew, \textsc{masc} and \textsc{pl} are abstract because neither corresponds to an actual morpheme. That is, the Hebrew \textsc{masc.pl} suffix \textit{-im} is fusional; it cannot be split into separate \textsc{masc} and \textsc{pl} substrings (cf. \textsc{masc.sg} forms, 
%which have no ending, and the \textsc{fem.pl} suffix \textit{-wt}).
%A clustering algorithm would be 
%disinclined to treat \textsc{masc} and \textsc{pl} as \textsc{mila-ma} does, since this would mean creating separate \textsc{masc} and \textsc{pl} clusters  despite the lack of a \emph{particular} \textsc{masc} suffix and a \emph{particular} \textsc{pl} suffix.
%%lack of shared formal elements. clusters because such clusters would not be based on shared formal elements; \textsc{masc} 
%%would contain words ending in -$\emptyset$ and \textit{-im}, and \textsc{pl} would contain words ending in \textit{-wt} and \textit{-im}.
%For this and similar reasons, \textsc{mila-ma}'s categories will need to be mapped to a somewhat adapted set 
%of gold-standard categories.

%****************
%\begin{description}
%\item[Stage 1: Extract.] Derive morphs from cluster centroids. That is, for each cluster centroid vector,  map the \emph{active} features to a particular sequence of alphabetic characters. The mappings is governed by a set of mapping rules. The resulting sequence of alphabetic characters is the morph. Repeat this process for each cluster (i.e., cluster centroid).
%
%\textbf{Example:} Suppose that \texttt{z<k} and \texttt{k<r} are the active features in a given cluster's centroid. These features would map to the (potentially discontinuous) character sequence \textit{zkr}. The morph would thus be the root \textit{z.k.r}.
%
%\item[Stage 2: Match.] Map morph characters to word characters. That is, given a cluster and its morph (obtained in Stage 1), go through the cluster's words, and in each word, determine which characters are the morph's characters. Label these characters as components of the morph in question. Repeat this process for each cluster/morph.
%Each morph is a (possibly discontinuous) sequence of alphabetic characters. 
%Given a cluster and its newly extracted morph, identity the morph's characters in each of the clusters words. 
%That is, for each word, match the morph's characters to the \emph{correct} word characters. Note that there is potential for ambiguity here. Suppose, for example, that the morph in question is the \textit{-wt}. It's easy enough to find a single \texttt{t} in a string of letters, but \texttt{t} is a frequently occurring letter, and it could easily occur elsewhere in the word. I have to make sure my matching algorithm selects the right character in cases like this. Repeat this process for each cluster. 
%For each word $w$ in a given cluster, identify the characters in $w$ that correspond to the morph's characters. counterparts of each the morph characters to their counterpart character in the word in question. with their matching characters in the word in 
%the characters of the morph to the corresponding characters in the cluster's member words.
%
%\paragraph{Example:}  
%Consider a cluster whose member words are \textit{mazkir}, \textit{hizkir}, \textit{zoker}, \textit{zokrim}, and \textit{zikron}. The morph in this case is the root \textit{z.k.r}. Stage 2 identifies the root consonants in each word and labels them as components of the morph \textit{z.k.r}. Here, the morph's characters are ``labeled" via boldface type:
%%\footnote{In reality, of course, a larger and more sophisticated labeling/indexing system is necessary, as every morph will require a distinct label/index.}:  
%\textit{ma\textbf{zk}i\textbf{r}},
%\textit{hi\textbf{zk}i\textbf{r}}, \textit{{z}o\textbf{k}e\textbf{r}}, \textit{\textbf{z}o\textbf{kr}im},
%and \textit{\textbf{z}i\textbf{kr}on}.
%The process is repeated for each cluster/morph.
%
%\item[Stage 3: Compress.] Map each morph to a single unique unicode character.
%\textbf{Example:} Consider the 
%Hebrew words \textit{magdil} and \textit{gadol}, which share the root \textit{g.d.l}. 
%Suppose that the Stage-2 outputs for these words are as in \eqref{ex:magdil} and \eqref{ex:gadol}, respectively. 
%\begin{exe}  \ex \label{ex:unicode} \begin{xlist}
%	\ex magdil \quad ma-, \,\, g.d.l, \,\, i 
%	\label{ex:magdil}
%	\ex gadol \, \quad  g.d.l, \,\, a.o
%	\label{ex:gadol}
%	\end{xlist}
%\end{exe}
%Altogether, there are four \emph{unique} morphs in \eqref{ex:unicode}, namely \textit{ma-}, \textit{g.d.l}, 
%\textit{i}, and \textit{a.o}.
%Each of these is mapped to a unique atomic symbol, as in \eqref{ex:map}.
%\begin{exe}
%	\ex  \textit{ma-} $\mapsto$ \$ \quad \textit{g.d.l} $\mapsto$ \% \quad
%\textit{i} $\mapsto$ \textit{i} \quad \textit{a.o} $\mapsto$ \#
%\label{ex:map}
%\end{exe}
%%Let the atomic symbols inherit the sequential order of their counterpart morphs. 
%In general, the atomic symbols inherit the ordering of the original morphs. 
%The exceptional cases are those of interdigitation. When two morphs are interleaved, 
%they are unordered with respect to each other.
%However, when they are mapped to atomic symbols, they necessarily 
%take on an arbitrary relative order because there is no way to interleave two 
%\emph{atomic} units: either $A$ precedes $B$ or $B$ precedes $A$; 
%there is no other option.
%%but with the atomic symbols now taking the places of of the original morphs. Put the symbols in the same order as their morph counterparts.  hat for morphs that are two or more characters long.. They are put together in the same order as the original morphs. This reducing or elsince it replaces whole character sequences, even discontinuous ones, with atomic symbols (see section~). 
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex magdil \quad \$\%\textit{i}
%	\label{ex:re-magdil}
%	\ex gadol \, \quad \%\#
%	\label{ex:re-gadol}
%	\end{xlist}
%\end{exe}
%%Now, when the characters of the two morphs are interleaved, i.e. in the case of interdigitation, the relative order of the morphs is indeterminate. However, when these two morphs are mapped to atomic symbols, they necessarily take on an arbitrary relative order. That is, either $A$ precedes $B$ or $B$ precedes $A$; there is no other option. 
%The mapping from morphs to atomic symbols thus abstracts interdigitation.  
%
%\item[Stage 4: Test.]
%%-- concatenative morphs.]
%Feed both the control file (i.e., the file containing the original wordlist) and the test file (i.e., the output of Stage 3) to
%Morfessor \citep{creutz-and-lagus:2005, creutz-and-lagus:2007}. Morfessor then induces morphological segmentations for each input file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation will at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, change the atomic symbols back to morphs (i.e., undo Stage 3). Finally, evaluate the two segmentations against a common gold standard.
%%control file, will 
%%%now be fed to Morfessor. 
%%The test file is the output of Stage 3.  The control file will contain the original, unaltered words. 
%%It will provide a baseline 
%%for measuring the effect of the compression carried out in Stage 3. 
%%The idea here is to see if the MCMM's morphs, 
%%now represented as atomic symbols, aide the process of morphological segmentation. 
%%That is, do they make the task easier? 
%%Do they improve segmentation accuracy? 
%\end{description}

