\chapter{DUAL-PARADIGM EVALUATION}
\label{ch:eval}

\section{Introduction}

\paragraph{Evaluation.} Unsupervised learning systems like Multimorph are inherently difficult to evaluate. This is because, unlike supervised learning, the correct answers--e.g., the correct
categories for the input examples are basically unknown. They are not supplied to the learner as  any rate. The learner thus has no externally-provided learning targets. 
Arguably, the main advantage of unsupervised learning is that it can discover novel and potentially useful categories and associations \citep{parsons:2004}. On the other hand, it is very difficult to evaluate previously unknown categories according to precision and recall and other such measures that rely ``answer keys." 
The previously unknown categories in the present study were to be morphological 
categories that were perhaps going to embody in some way Aronoff's notion of the morphome \citep{aronoff:1994}.  But we did not know the precise form these categories would take. 

%---recall from REF that we use the term morph to avoid suggesting equivalence with he were unknown because we had  units were not conventional morphemes 
%or morphosyntactic categories. 

%Instead, MCMM-generated clusters corresponded roughly to Aronoff's 
%\emph{morphomes} \citep{aronoff:1994}, 

which can be described as 
\emph{pre-morphosyntactic} units, i.e., units that have been assembled from 
phonemes, but have not yet been assigned 
a syntactic or semantic meaning. However,  I use the term \emph{morph} in what follows, since it is a less loaded term than either
either morpheme or morphome.
%\emph{morphome}, since MCMM-generated clusters may not correspond 
%precisely to morphomes in every case (see section REF). %~\ref{sec:targets}).

Thus, the evaluation itself presents an important research question, namely the question 
of how to evaluate the output of an unsupervised morphological clustering algorithm, 
particularly one that considers only features of \emph{word-internal form}, having no 
access to morphosyntactic features that exist outside of the word, e.g., person, gender, and 
number of surrounding words.

%Multimorph is an unsupervised machine learning system, which makes it 
%intrinsically difficult to evaluate, and thus no single 
%evaluation method is likely to be perfect. 

I thus devised a dual-paradigm method for evaluating Multimorph's output, i.e., an approach consisting of two complementary methods, %I would thus rather not rely on a single method.
%not only because is it an unsupervised learning system, which are notoriously challenging to evaluate,  it learns unconventional morphological units, namely morphs. Indeed, it would be very difficult to come up with a gold standard for the morphs because we do not know what the morphs are supposed to look like; i.e., the ``right answers" are not obvious at the outset. 
%In particular, I will use 
one \emph{intrinsic} and one \emph{extrinsic}. We shall discuss these two components in what follows.

Finally, the evaluation also included a \emph{qualitative} component, which served to complement the quantitative 
%utput of the MCMM was evaluated \emph{qualitatively}. The  which is intended to complement the quantitative
 intrinsic an extrinsic methods.


\section{Qualitative Evaluation}
The qualitative component of the evaluation consisted mainly in directly (or ``manually") examining the clusters generated by Multimorph's MCMM generated. These clusters are words grouped together according to some shared elements of form. These shared elements are often immediately obvious to the human eye, but even in such cases they can become diluted or lost in automated quantitative assessment.   %shared elements of form are immediately obvious to the human eye, but they can easily become lost in automated 
%A qualitative componentmponent is advantageous is that the automaticl,kl quantative procedures may not catch everything. 
%Humans also have linguistic intuition, and since we are dealing with linguistic data, which can be very useful in interpreting a cluster. Since we are dealing with unsupervised learning, there is basically no telling what the clusters will come to represent. The qualitative component is meant to address this essentially lack of predictability.

\section{Quantitative Evaluation}
\subsection{Intrinsic Evaluation}
An \emph{intrinsic} evaluation regards a system as a stand-alone system and its output as an end unto itself. That is, an intrinsic evaluation thus evaluates a system directly, on its on terms; any other system is irrelevant. 
%disregards 
% as independent and isolated from other systems, examining its 
%output directly. That is, the system is evaluated as a stand-alone application, on its
%on terms, as it were,
%not as one embedded in a larger system.

The intrinsic component of the evaluation compares an MCMM-generated clustering to a 
\emph{gold-standard categorization}.\footnote{By convention, 
gold-standard clusters are not actually called clusters, 
but (gold-standard) \emph{categories}.} 
An MCMM's output consists of 
%Given the final valuations in the MCMM matrices $\mathbf{M}$ and $\mathbf{C}$,
the final values in the matrices $\mathbf{M}$ and $\mathbf{C}$.
Given these values, one can derive a list of word-to-cluster mappings; 
each mapping, or item, in this list
consists of a word followed by a list of cluster labels, i.e., labels pointing to
the clusters in which the word has membership, 
as in ``runs: run, -s,"  where \textit{runs} is the word, and \textit{run} and \textit{-s} 
are the labels of the clusters to which \textit{runs} belongs. These labels also represent 
morphs. That is, e.g., \emph{-s} represents a cluster whose words end with the morph 
\emph{-s}.

To evaluate a list of mappings like ``runs: run, -s," we need an analogous 
gold-standard list, one that supplies, for each word, a list of gold-standard \emph{categories}.
%(i.e., the categories to which the word belongs). 
This is to say that we need a gold-standard morphological analysis for each word. 
We needed different gold standard lists for the orthographic and transcribed datasets, 
since these datasets come from different sources, and their respective gold standards must 
be obtained through separate means.


% (Notice that a lot of properties are packed into the suffix \textit{-s}).
% Let us consider  
% briefly the nature of an MCMM's output. One can represent a 
% clustering (or categorization) as a table in which the rows 
% correspond to the clustered (or categorized) items, and 
% the columns to the clusters (or categories) themselves. Now, 
% in the proposed dissertation, the items is feature-vector 
% representations of words, and the clusters is morphs. Each 
% $\text{cell}_{i,k}$ contains either 1 or 0: 1 if $\text{word}_i$ has 
% morph $k$, and 0 if it does not. 
%%What we have just described is essentially the MCMM's $\mathb{M}$ matrix. 
% Each 1 thus represents the presence of a particular morph. 
 
% If we wanted to get away from the matrix format, i.e., the grid 
% of 1's and 0's, we could replace each 1 with its morph and eliminate the 0's altogether. 
% Suppose, for example, 
% that $\text{word}_{1253}$ is \textit{runs}, which has two morphs, namely the stem 
% \textit{run} and the suffix \textit{-s} 3rd-person (\textsc{3p}), present-tense (\textsc{Pres}) singular (\textsc{Sg}) s.
% (Notice that a lot of properties are packed into the suffix \textit{-s}). 
% Suppose further that \textit{run} is cluster 87 and \textit{-s} is cluster 6, and the overall clustering 
% has a total of $K=500$ clusters, in which case $\mathbf{M}$ has 500 columns, and
% row 1253 has 498 zeros and only two 1's, one at column 6 and the other at column 87.
%We can replace the 
%1 in column 6 with \textit{-s} and the 1 in column 87 with \textit{run} 
%and discard the zeros, yielding ``runs: run, -s," which is tantamount a 
%morphological segmentation. 
%---
%Notice that the suffix \textit{-s} maps to three 
%``atomic" morphosyntactic categories, 
%namely 3rd-person (\textsc{3p}), present-tense (\textsc{pres}), 
%and singular (\textsc{sg}). 
%My system cannot learn abstract morphosyntactic labels like \textsc{3p} 
%and \textsc{sg}. Rather, it learns \emph{morphs}, 
%%the pre-morphosyntactic units of form. We will call these units \emph{morphs}, 
%which may or may not correspond to morphosyntactic categories (see the discussion in section~\ref{sec:targets}). 
%When there \emph{is} a 
%correspondence between morphs and morphosyntactic categories, it is often a 
%one-to-many mapping because the same morph can be
%requisitioned by more than one morphosyntactic category.
%---
 %can lay claim to the same morph. 
%In ``runs: run, -s," for example, the suffix \textit{-s} 
%represents the union of three ``atomic" morphosyntactic categories, namely \textsc{3p}, \textsc{pres}, and \textsc{sg}. 

%Thus, the output of an MCMM, after a little post-processing, can look like ``runs: run, -s." It is essentially a list of morphological segmentations. But more accurately, it is a list of word-to-cluster mappings; for each word, it will specify the cluster(s) to which it belongs. Note that the morphs \emph{run} and  \emph{-s} are essentially the labels of particular morphological clusters.
%The output of an MCMM is thus essentially a list of word-to-cluster mappings. Each item in this list is a word followed by a list of the clusters in which it has membership.
%%After little post-processing, it can look like ``runs: run, -s." 
%Morphs like \emph{run} and \emph{-s} are essentially cluster labels. 
%That is, \emph{-s} represents a cluster whose words predominantly end in  \emph{-s}.

\subsubsection{Orthographic data.}

%To obtain morphological analyses for the wordlist O, I will use the 
%MILA Morphological Analysis tool (\textsc{mila-ma}) \citep{hebrew-resources:2008}.
%Because \textsc{mila-ma} requires that input words be spelled 
%according to Modern Hebrew standard orthography, it can only be
%used to create a gold standard for orthographic wordlist O. The 
%gold-standard morphological analyses for the transcribed wordlists TS and TR 
%must come from a different source (see below).
%\textsc{mila-ma} is essentially a finite-state transducer. Because its morphological 
%knowledge has been manually coded by humans and its output is
%deterministic, it provides a good approximation to human
%annotation. 
%
%However, many of the original \textsc{mila-ma} categories are ill-suited to the purpose
%of evaluating an MCMM's clustering. The \textsc{mila-ma} categories are
%often atomic and abstract, e.g., \texttt{feminine} and \texttt{masculine}. 
%Such categories are purely morphosyntactic; they are meaningless at the 
%word-internal level because they can only be observed in agreement phenomena. 
%Moreover, there is no morphological unit in Hebrew that means strictly `feminine,' %(i.e., nothing more than `feminine' and nothing less). 
%nor is there one that means strictly ``masculine." Hebrew inflectional affixes 
%tend to be fusional, having meanings like ``feminine plural" and ``masculine plural."
%
%For this and similar reasons, \textsc{mila-ma}'s categories need to be mapped 
%to a modified set of gold-standard categories, i.e., categories that correspond 
%more closely to actual differences in form.
%The MCMM's clustering will then be quantitatively compared to the modified 
%\textsc{mila-ma}-based gold-standard categorization. 
%In particular, I used the measures \emph{average cluster-wise purity}, 
%\emph{BCubed precision} and \emph{BCubed recall}. The latter two are important 
%because they are specifically designed for cases of overlapping clusters 
%\citep{amigo-et-al:2009}.

\subsubsection{Transcribed data.} Gold-standard category mappings for the 
transcribed words were obtained by extracting morphological analyses from the Berman 
longitudinal corpus. Recall that for each utterance in the Berman longitudinal corpus, 
there is a transcription tier and a morphological-analysis tier. The latter provides a 
morphological analysis for each word in the utterance, including roots for the words 
that have roots. I extracted the morphological analyses and used them to create a list of 
word-to-category mappings. % resembling created from \textsc{mila-ma}'s analyses.
\subsubsection{Evaluation Metrics} 
\label{sec:metrics}
We evaluate the intrinsic results according to three metrics: \textbf{average purity}, \textbf{BCubed precision}, and
 \textbf{BCubed} recall. For the purpose of describing these metrics,
 let $U$ denote the set of $K$ clusters discovered by a system, $V$ the set of $J$ gold-standard categories, and $X$ is the set of data points to be clusters. The cardinality of $X$ is $N$; i.e., there are a total of $N$ individual data points that have been clustered.
\paragraph{Average purity}
The standard purity metric seeks to calculate the global correctness of a given clustering by determining the proportion of data points that have assigned to the correct cluster. Each individual data point has at some point been associated with a gold-standard \emph{category} label. Thus, in any given cluster, there are in effect as many gold-standard category labels as there are data points. Standard purity assumes that most frequent 
 gold-standard category in a given cluster $u_k$ is the gold-standard label for the entire cluster. The number of correctly clustered data points in $u_k$ is thus equal to the frequency of  $v_j$ in cluster $u_k$. Standard purity sums up the correctly clustered data points over all clusters, and divides this sum by $N$, the total number of distinct data points in the clustering
 %The reason $v_j$ is the most frequent category in $u_k$ is that it is associated with more of $u_k$'s members than any other data point .in the cluster is the same as the frequency of data points that bear the label. 
% Thus, the frequency of the most common gold-standard category in a cluster $U_k$  is deemed the number of correctly clustered items in $U_k$ x$ instances of this category label in the cluster, there are also $x$ data points associated the course equal to the number of data points associated with this category. data points he purity of this particular is thi
 The standard version of purity is computed as 
\begin{equation} \label{eq:pur1}
\text{purity}(U, V) = \frac{1}{N} \sum_{k \in K} \text{max}_{j \in J} |u_k \cap v_j|
\end{equation}
In other words, 
The problem for our purposes is that this version of purity assumes that each datapoint belongs to exactly only one gold-standard category. In fact, it requires that this be so, for if any data point should belong to more than one cluster belong to more than one cluster, the numerator in \eqref{eq:pur1} would be greater than than 1, and thus the resulting purity would exceed 1.
In the present study, each data point is a word, and the categories are morphological categories. In natural languages, words frequently belong to multiple morphological categories at once.
%however, which contains multi-category examples, this assumption can yield purities greater than 1. 

To avoid purities that exceed 1, we modify equation \eqref{eq:pur1} as follows: 
\begin{equation} \label{eq:pur2}
\text{purity}_{\text{avg}}(U, V) =  \frac{1}{K} \sum_{k \in K} max_{j \in J} |u_k \cap v_j|
\end{equation}
This new equation represents an \emph{average cluster-wise purity}; i.e., it computes each cluster's internal purity and then averages over these purities. %the mean of the $K$ clusters’ internal purities. 
While this equation yields purities within $[0, 1]$, even 
when clusters overlap, it retains the well-known bias of the purity metric toward small clusters. We thus incorporate other metrics. 

\paragraph{BCubed precision and recall}
The metrics \emph{BCubed precision} (BP) and \emph{BCubed recall} (BR) \citep{bagga-and-baldwin:1998} evaluate a clustering by checking one 
pair of data points at a time, comparing the relationships that the algorithm has posited for each pair against their 
gold-standard relationships. These metrics are well-suited to cases of overlapping clusters \citep{amigo-et-al:2009}. 
In such cases, it is possible for two data points $x$ and $y$ to overlap both in their algorithm-assigned clusters and their 
gold-standard categories. Suppose that $x$ and $y$ overlap in m clusters and n categories. Ideally, $m$ would 
equal $n$; 
i.e. there would be a one-to-one correspondence between clusters and gold-standard categories. 
In most cases, however, either $m$ will be less than $n$ or vice versa. BCubed precision essentially 
measures the extent to which $m \leq n$; for if $m>n$, then the algorithm has posited at least one 
spurious relationship be.tween $x$ and $y$. BCubed precision thus penalizes spurious cluster assignments. 
On the other hand, BCubed recall measures the extent to which $m \geq n$; for if $n<m$, then the algorithm 
has missed at least one gold-standard relationship between $x$ and $y$. BCubed recall thus 
penalizes absent cluster assignments (i.e., overlooked gold-standard relationships). 
At the core of BCubed precision is the measure multiplicity precision ($MultiP$):
%\begin{equation}
%MultiP(x,y) = \frac{min|U(x) \cap U(y)|, |V(x) \cap V (y)|}{|U(x) \cap U(y)|}
%\end{equation}
\begin{equation}
\text{MultiP}(x,y) = \frac{\text{min}(|U(x)\cap U(y)|, |V(x) \cap V(y)|)}{|U(x) \cap U(y)|}
\end{equation}
where $x$ and $y$ are data points in $X$; $U(x)$ is the set the clusters that contain $x$, 
and $U(y)$ is the set of clusters containing $y$.
Finally, 
$V(x)$ and $V(y)$ are the sets of gold-standard categories for $x$ and $y$. 
Notice that $MultiP(x,y)$ is a comparison of two items $x$ and $y$, neither of which is the 
particular focus of the measure. $MultiP(x,y)$ is thus a joint description $x$ and $y$. 
To obtain a precision value that describes $x$ alone, one must compute $Avg_{y}(MultiP (x,y))$, the 
average $MultiP(x,y)$ over $y \in X$, by computing $MultiP(x,y)$ for every $y$ that shares a cluster with $x$ 
and averaging the resulting values. To obtain a precision value that describes the whole dataset (not just a single $x$), 
one must compute $Avg_y(\text{MultiP}(x,y))$ for every $x$ in $X$ and then take the average of these averages. 
This average of averages is BCubed Precision. 
%\begin{equation}
%BP=Avg_x [ Avg_{y.U(x) \cap U(y)\neq \emptyset}(MultiP(x,y))]
%\end{equation}
\begin{equation}
BP=Avg_x [Avg_{y.U(x) \cap U(y) \neq \emptyset}(\text{MultiP}(x,y))]
\end{equation}
The measures multiplicity recall(MultiR) and BCubed recall (BR) are analogous to multiplicity precision and
 BCubed precision. The computation of MultiR is identical to that of MultiP 
 except that gold-standard categories replace clusters in the denominator: 
\begin{equation}
MultiR(x,y) = \frac{min(|U(x) \cap U(y)|, |V(x) \cap V (y)|)}{|V(x) \cap V(y)|}
\end{equation}
Likewise, BCubed recall is is nearly identical to BCubed precision except that MultiR replaces MultiP, and the set V replaces U in the $\text{Avg}$ subscript expression:
\begin{equation} 
BR = Avg_x [Avg_{y.V(x) \cap V(y) \neq \emptyset}(\text{MultiR}(x,y))]
\end{equation}

\subsection{Extrinsic Evaluation} \label{sec:eval-extrinsic} An \emph{extrinsic evaluation} 
views a system as a component of a larger, or outer, system. 
Its purpose is to evaluate the embedded system, but it does so by evaluating the outer 
system. If the outer system scores highly,
the embedded system, i.e., the system under evaluation, scores highly.
An extrinsic evaluation makes sense for my system for two reasons:

tic\.{t}ar\.{k}\'{i} %c:\[2\] 8:\[4,5\] 43:\[5,7\] \UTF{1E6D}:\[3\] \UTF{1E33}:\[6\] 22:\[0,1\]

\begin{enumerate}
\item Multimorph is essentially an embedded system by nature: It learns morphs, which are intermediate units, intended to facilitate the learning of morphemes. %My system is thus meant to be an embedded component of a larger process. 
\item While it would be very difficult to come up with gold-standard morphs, gold-standard morphemes are relatively easy to produce, since morphemes, in contrast to morphs, are already well-defined. 
%As intermediate units, the value of morphs lies in their utility, i.e., in their capacity for yielding correct morphemes. What they look like is not important as long as they are effective. is evaluate \emph{morphemes} that have been induced from morphs. 
%On the other hand, it is relatively easy to come up with gold-standard morphemes, since morphemes, in contrast to morphs, are well-defined. 
\end{enumerate}

The extrinsic evaluation will consist of the four stages described below. 
Stages 1 to 3 prepare the output of an MCMM to be fed to Morfessor in Stage 4. 
To obtain the gold-standard datasets for Morfessor, I manually segmented 
$\frac{1}{10}$ of the original, unprocessed wordlists, i.e., both the transcribed 
and orthographic wordlists. Note that this four-stage extrinsic evaluation only considers 
stem-external, concatenative morphology. This is because Stage 3 effectively removes 
interdigitation. Morfessor is not even capable of handling interdigitation, since it is 
a sequential algorithm.

But how is one to assess the output's quality? How do we tell how \emph{good} it is? I need a way to evaluate the clusters produced by the MCMM.
But a cluster is just a group of words that Multimorph has seen fit to put together according to criteria of its own devising.
These criteria could be virtually anything, as Multimorph is directed solely by an algorithm, not by any previously attained knowledge concerning
the workings of morphology or human language. 
Thus, the \emph{meaning} of one of Multimorph's clusters may not be immediately obvious.
 
 
%\subsection{} \label{sec:paradigms}
\subsection{Four-stage process.} \label{sec:extrinsic}
Multimorph's output 
will have to processed before it can be evaluated. This subsection 
will describe each of the four stages. The input to Stage 1 is a cluster centroid vector, 
i.e., one the $K$ columns in the $J \times K$ matrix $\mathbf{C}$. The $k$th column 
corresponds to the $k$th cluster. Each $j \in J$ corresponds to a particular feature; the 
$J$ rows correspond to the same $J$ features present in each original data point 
$\mathbf{x}_{i}$ as well as each reconstructed data points $\mathbf{r}_{i}$, where 
$0 \ge i < I$, and $I$ is the total number of data points (original and reconstructed).

\subsubsection{\textsc{Stage 1:} Interpret clusters as morphs.} Each cluster ultimately represents a unit of morphological organization, i.e., a morph. But as a cluster, it is too abstract to be directly relatable to actual words, i.e., to strings consisting of characters. The first stage is therefore
%in the four-stage extrinsic evaluation process is 
to make the clusters relatable to strings. In particular, each cluster is a regular expression that is derived from the active features of its centroid. A morph's regular expression is built up from ``atomic" regular expressions derived from its individual active features. 

The form of such an expression depends on the type of the feature 
upon which it is based. For example, a positional feature such as 
\texttt{a@1} manifests simply as \texttt{(a)}. (The parentheses 
are important for \textsc{stage 2}, in particular, for retrieving 
matching characters if morph's regular expression can be matched 
to a word.) The regular expressions of precedence features take the 
form \texttt{(x)(.?)}$\times (\delta-1)$\texttt{(y)}; i.e., the number 
of \texttt{.?}s depends on the value of the parameter $\delta$; in 
particular the number of \texttt{?} is equal to $\delta- 1$. Thus, 
$\delta$ values of 1, 2, and 3 correspond to 0, 1, and 2 \texttt{.?}s,
 respectively. 

If a cluster's centroid has more than one active feature, an 
atomic regular expression is derived from each such feature, 
and if the features are compatible, the atomic regular expressions 
are combined to create a composite expression. For example, the 
positional features are compatible if the indices are consecutive, 
as in \texttt{h@[0]} are \texttt{a@[1]}. would be combined 
to form the composite expression \texttt{(h)(a)}. Precedence features 
can also create composite regular expressions. For example, the 
features \texttt{d<b} and \texttt{b<r} would together yield the 
composite expression \texttt{(d).?(b).?(r)}, that is, if $\delta = 2$. 
In general, two precedence features \texttt{a<b} and \texttt{c<d} 
merge if $b=c$.

The output of Stage 1 is ultimately an array of \emph{morph objects}.
 Each morph object specifies values for certain attributes, the main ones 
 being a morph ID (i.e., a unique integer) and a regular expression derived 
 from the features of a particular cluster centroid. Stage 1 is thus
  concerned the clusters in and of themselves and their interpretation. 
  Stage 2, to which we turn next, is concerned with mapping clusters, i.e., 
  morphs, to the characters of actual words. 

%\begin{figure}[t]
%\centering
%\begin{subfigure}[a]{0.3\textwidth}
%\begin{tabular}{cc}
%Morph ID & Regular Expression \\ \hline
%3 & \texttt{(u)} \\
%31 &  \texttt{(f)} \\
%64 &  \texttt{(h).?.?(i)}  \\
%84 & morph ptn: \texttt{(\UTF{017E})}  \\
%95 &  \texttt{(l).?.?(v)}  \\
%148 &   \texttt{(c).?.?(\'{a})} \\
%151  &  \texttt{(i).?.?(l)} \\
%202 &  \texttt{(i).?.?(\'{a})}  \\
%264 &  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%284 &  \texttt{(l).?.?(t)}  \\
%360 &  \texttt{(r)} \\
%\begin{figure}[t]
%\label{fig:stage2}
%	%\subfigure[List of morphs initially associated with \textit{hicl\'{a}xti}]{
%	\begin{tabbing}
%	\hspace{0.6in} \= \hspace{5.5in} \kill
%	Morph ID \> Regex \\ 
%                3 \> \texttt{(u)} \\
%                31 \>  \texttt{(f)} \\
%                64 \>  \texttt{(h).?.?(i)}  \\
%                84 \>  \texttt{(\v{z})}  \\
%                95 \>  \texttt{(l).?.?(v)}  \\
%                148 \>   \texttt{(c).?.?(\a'{a})} \\
%                151  \>  \texttt{(i).?.?(l)} \\
%                202 \>  \texttt{(i).?.?(\a'{a})}  \\
%                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%                284 \>  \texttt{(l).?.?(t)}  \\
%                360 \>  \texttt{(r)} \\
%	\end{tabbing}
%	%}
%	%\subfigure[Mapping from Morph IDs to character indices]{
%%hicl\'{a}xti \quad 64:[0,1], 264:[2,3,5,6], 202:[1,4], i:[7], 148:[2,4], 151:[1,3], 284,[3,6], 95:[3,4]
%%}
%\end{figure}

%\end{tabular}
%\caption{Morphs associated with \textit{hicl\'{a}xti} and their regular expressions.}
%\label{fig:noise-clusters}
%\end{subfigure}
%\begin{subfigure}[ hicl\'{a}xti ]{0.3\textwidth}
%hicl\'{a}xti \quad 64:\[0,1\], 264:\[2,3,5,6\], 202:\[1,4\], i:\[7\], 148:\[2,4\], 151:\[1,3\], 284:\[3,6\], 95:\[3,4\]
%\label{fig:mapping}
%\end{subfigure}
%\end{figure}

\subsubsection{Stage 2. Match morphs to words.} 
Each of the\emph{covered} words in an MCMM clustering is a member of at least one and possibly many clusters. Some words belong many clusters, and sometimes the relationships between clusters and their member words, i.e., the reason why a would should be included in a given clusters, is not clear. Indeed, in any given clusters, most clusters are likely to be noisy to some extent. Sometimes the active features of a cluster do not seem match the characters of a member word. Example \ref{ex:noise-clusters} lists the morphs, i.e., their morph IDs and regular expressions, which one experiment ($s = 4,\delta = 3$) associated with the word \textit{hicl\'{a}xti} had some, where ``associated" means that \textit{hicl\'{a}xti} was member of the clusters from which these morphs were derived.

%\begin{exe}
%\label{ex:noise-clusters}
%	\begin{tabbing}
%	\hspace{1in} \= \hspace{5.5in} \kill
%	Morph ID \> Regex \\ 
%                3 \> \texttt{(u)} \\
%                31 \>  \texttt{(f)} \\
%                64 \>  \texttt{(h).?.?(i)}  \\
%                84 \>  \texttt{(\v{z})}  \\
%                95 \>  \texttt{(l).?.?(v)}  \\
%                148 \>   \texttt{(c).?.?(\a'{a})} \\
%                151  \>  \texttt{(i).?.?(l)} \\
%                202 \>  \texttt{(i).?.?(\a'{a})}  \\
%                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%                284 \>  \texttt{(l).?.?(t)}  \\
%                360 \>  \texttt{(r)} \\
%	\end{tabbing}
%\end{exe}	
Stage 2 must take such a list and establish from it a mapping from word characters to morphs. To this end, it considers each candidate morph's regular expression, attempting to match it with the word in question. If there is no match, the morph is discarded. For example, the expressions \texttt{(\v{z})} and \texttt{\(l\).?.?\(t\)} fail to match \textit{hicl\'{a}xti}. 

The end result of this process is a mapping from morphs (i.e., morph IDs) to characters (i.e. character indices, as follows:
\begin{exe}
\ex \label{ex:noise-clusters}
	\begin{tabbing}
	\hspace{1in} \= \hspace{5.5in} \kill
	Morph ID \> Regex \\ 
                3 \> \texttt{(u)} \\
                31 \>  \texttt{(f)} \\
                64 \>  \texttt{(h).?.?(i)}  \\
                84 \>  \texttt{(\v{z})}  \\
                95 \>  \texttt{(l).?.?(v)}  \\
                148 \>   \texttt{(c).?.?(\a'{a})} \\
                151  \>  \texttt{(i).?.?(l)} \\
                202 \>  \texttt{(i).?.?(\a'{a})}  \\
                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
                284 \>  \texttt{(l).?.?(t)}  \\
                360 \>  \texttt{(r)} \\
	\end{tabbing}
\end{exe}
\begin{exe} \ex \label{ex:mapping} hicl\'{a}xti \quad \texttt{64:[0,1]; 264:[2,3,5,6]; 202:[1,4]; i:[7]; 148:[2,4]; 151:[1,3]; 284:[3,6]; 95:[3,4]}
\end{exe}

%Map the Morph object’s regular expression onto the characters of the word itself. This is done primarily through the matching of a regex to an input string. This will produce a lattice of potential morphID sequences, etc. 

\subsubsection{\textsc{stage 3:} Compress and Encode.}
A single, optimal path through the word was computed from Stage 2's mapping. In the case of (\ref{ex:mapping}), for example, some morphs can be eliminated some because some share character indices.  After eliminating morphs \texttt{151}, \texttt{202}, \texttt{284}, we arrive at the following compressed sequence of morph IDs computed at the end of Stage 2 is converted is optimized and compressed to yield a single best path through the word. 
 the best sequence or “path” and then compress it. 
 %That is, remove repeated instances of the same morphID. 
\begin{exe} \ex \label{ex:comp-with-indices} 
hicl\'{a}xti \quad \texttt{64:[0,1], 148:[2,4], 264:[2,3,5,6], i}
\end{exe}
Note the \texttt{i} at the end of the sequence; this is an example of``orphan" alphabetic character that was never associated with a morph. Such stranded characters were not uncommon. They were retained so that every character would be accounted for.
%The idea is to abstract away from the individual characters as much as possible and allow morph to be a single, atomic entity, with no internal structure.] 
The purpose of this stage, however, is to abstract away from the individual characters as much as possible, so that each morph could be treated as a single, atomic entity, with no internal structure.
Thus, the compressed sequence in (\ref{ex:comp-with-indices}) would be look more like the following:
\begin{exe} \ex \label{ex:comp-no-indices} 
hicl\'{a}xti \quad \texttt{64, 148, 264, i}
\end{exe}
The last step in Stage 3 is to \emph{encode} the compressed morph ID sequence by replacing each  ID number with a unique, atomic unicode character and joining the characters together to form a string a string, e.g., a four-character string in the case of (\ref{ex:comp-no-indices}).
I used the characters of the vast CJK unicode block for this purpose , but in principle any characters could be used, as long as they are atomic and no character is used twice. For the sake of the present discussion, we shall call this compressed and encoded dataset the \emph{experimental} set, as it is the experimental dataset \emph{within} the extrinsic evaluation. That is, it is the experimental input to Morfessor in Stage 4 (see below). 

We must note here that there was \emph{control} dataset for each experimental dataset (and an experimental dataset for each combination of the $s$ and $\delta$ parameters. Each control set consisted of the same words as its corresponding experimental set, except that the control words were the original, untouched words. Ten percent of the original words had been previously selected at random. These were segmented manually in order to serve as a gold-standard file. 
%  ten percent sample of the data was previous %, having undergone none of the stage 

%< Run morfessor on the control wordlist. This list contains the same words as the compressed dataset, except they are not compressed.>
 
\subsubsection{\textsc{stage 4:} Segment, Decompress, Evaluate}
 Morfessor is now run on both the experimental and control files and thus produces segmentation files (or models), one control and one experimental, the latter still consisting of encoded words, except that the encoded words are now segmented.
 
% original (or ``normal") words, while the other comprised segmentation of the encoded versions of the original words. 
 
 %Note, however, that even the encoded words of the experimental file had started out as original words, they so altered in their encoded forms that they generally bore no resemblance to their original forms.
 
% described  computes a segmentation for each compressed/encoded word in the experimental set. We also run Morfessor on the control dataset, thus obtaining segmentations of the original words. We at this point have two segmented files, but the words in the experimental file are still encoded. 
%A ten percent sample of the data was previous  10 percent sample of these segmented compressed words and \emph{decompress} them, i.e.,

To evaluate both files against the same gold-standard, the experimental file must now be decoded. To avoid altering  Morfessor’s segmentation decisions, each morfessor-computed segment (which may consist of multiple morphs) was decoded separately. The decoded segments were then reassembled. Morfessor's evaluation utility was used to evaluate each model against the gold standard. 

In the case of our example word \textit{hicl\'{a}xti} which we have been using throughout this section, Morfessor was given the compressed input in (\ref{ex:comp-no-indices}), which was four symbols long, and output a segmentation consisting of two segments, with the delimiter placed just before the \textit{i}. The first segment thus comprised the three \emph{symbols} corresponding to to the morphs 64, 148, and 264, and the second segment was the \emph{i}. In the final decoding process, the morphs 64, 148, and 264 together map to the substring hicl\'axt, and thus Morfessor's segmentation becomes hicl\'axt + i.
%the the preserving Morfessor's segmentations/segments). 
%[example]
%The reason we that now have to decompress these words (segmentations) is that we need to evaluate them and the control segmentations against the same gold standard segmentations. The experimental segmentations thus need to be comparable to the control segmentations; i.e., they at least need to be composed of roughly the same characters. 

%\subsubsection{Stage 1: Identify the morph characters} The first stage is to map each cluster's set of active features to a particular sequence of alphabetic characters. This sequence is regarded as the \emph{morph} corresponding to the cluster. Also in this stage, each morph is labeled as a prefix, stem-component, or suffix.
%
%\begin{enumerate}
%  \item Mapping from features to root or pattern characters:
%    \begin{enumerate}
%   	\item Only precedence [and bigram features] can map to root and pattern characters. Positional features never can.
%   	\item There must be at least one precedence feature [or one bigram feature] for each root-character bigram. (Note that precedence features are themselves basically bigrams.) These features must also overlap; e.g., the features \texttt{a<b} and \texttt{b<c}, which overlap at \textit{b}, indicate the root \textit{a.b.c}. %Note that roots can also be indicated by bigram features as well as combinations of bigram and precedence features (e.g., \texttt{a+b} and \texttt{b<c}).
%     \end{enumerate}

%   \item Mapping from features to prefix characters:
%   \begin{enumerate}
       %\ex \label{ex-1a} 
%       \item If there are at least two features of the form \texttt{a<b} and \texttt{a<c}, such that \textit{b} $\ne$ \textit{c}, then \textit{a} is at least part of a prefix. But what if the precedence features have an abstract component, as in the following?
%       \begin{itemize}
%       \item \texttt{x<C}
%       \item \texttt{x<V}
%       \end{itemize}
%       How does one determine inequality between abstract characters? Answer: C $\ne$ V. So there can still be inequality, just less of it. 
       %If there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, such that \textit{t} $\ne$ \textit{b}, \textit{t} $\ne$ \textit{c} (see above), then the prefix is either \textit{at-} or \textit{ta-}, respectively. 
%       \ex When there is a character \textit{a} satisfying rule \ref{ex-1a} has more than one character can only be determined by \emph{bigram} features. In particular, if there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, such that \textit{t} $\ne$ \textit{b}, \textit{t} $\ne$ \textit{c} (see above), then the prefix is either \textit{at-} or \textit{ta-}, respectively.
       %\ex 
%       \item If there is at least one positional feature of the form \texttt{a@[}$x$\texttt{]}, where $x$ is a positive integer, then \textit{a} is at least part of a prefix. If there are two or more consecutive positional features, i.e., features like \texttt{a@[}$x$\texttt{]}, \texttt{b@[}$x+1$\texttt{]}, \texttt{c@[}$x+2$\texttt{]}, and so on, then the prefix is the entire string of characters indicated by these features.
%    \end{enumerate}
%
%  \item Mapping from features to suffix characters:
%   \begin{enumerate}
%   \item If there are at least two features of the form \texttt{b<a} and \texttt{c<a}, such that \textit{b} $\ne$ \textit{c}, then \textit{a} is at least part of a suffix. If there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, where \textit{t} is a different character than either \textit{b} or \textit{c} (see above), then the prefix is either \textit{-at} or \textit{-ta}, respectively.
%
%   \item If there is at least one positional feature of the form \texttt{a@[}$x$\texttt{]}, where $x$ is a negative integer, then \textit{a} is at least part of a suffix. If there are two or more consecutive positional features, i.e., features like \texttt{a@[}$x${]}, \texttt{b@[}$x-1${]},\texttt{ c@[}$x-2${]}, and so on, then the suffix is the entire string of characters indicated by these features.
%   \end{enumerate}
%  \item If a cluster centroid has conflicting active features, then the cluster is void; it does not correspond to any morph.
%\end{enumerate}
%%EXAMPLES
%
%\begin{exe}
%%Cluster #0 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active features: e+w (1.0), e+i (1.0), l@[0] (1.0), e@[0] (1.0), e$<$w (0.9713), l+w (0.9466), e$<$i (0.8878), l<w (0.7655), e@[1] (0.7321), e+t (0.7049) \\
%Morph: None \\
%Pertinent rule: 4
%%
%%Cluster #2 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active feature: w+i (1.0), l+i (1.0), i+m (1.0), i+i (1.0), w$<$i (1.0), i@[-2] (1.0), m@[-1] (1.0), l<i (0.9941), i<i (0.8206), l+m (0.7164) \\
%Morph: -liim (suffix) \\
%Pertinent rule: 3b
%%
%%Cluster #5 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%%Active features: m+i (1.0), i+m (1.0), m<i (1.0), i@[-2] (1.0), m@[-1] (1.0), m+m (0.9825), m@[0] (0.8077), w+m (0.7786), m+w (0.7006), m<m (0.6840)
%%Morph: None
%%Pertinent rule: 4
%%
%%Cluster #6 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active features: b@[0] (1.0), b$<$w (0.9285), b$<$i (0.9232), b+t (0.6000), b+r (0.5867), b$<$t (0.4608), b+m (0.4576), b@[1] (0.4512) \\
%Morph: b- (prefix) \\
%Pertinent rules: 2a and 2b 
%%
%%Cluster #13 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: p$<$q (1.0), p$<$d (0.9999), q$<$d (0.9999) \\
%Morph: p.q.d. (root)  (The feature p*d is not relevant, as it turns out.) \\
%Pertinent rule: 1b
%%
%%Cluster #1 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: w$<$t (1.0), w@[-2] (1.0), t@[-1] (1.0) \\
%Morph: -wt (suffix) \\
%Pertinent rule: 3b
%%
%%Cluster #26 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: r$<$i (0.99), r@[-3] (0.98), r+i (0.96), i@[-2] (0.9), i+r (0.84), r+m (0.8), h+r (0.8), r$<$m (0.79), r+t (0.74), w+r (0.68) \\
%Morph: None (There are active features for both a prefix and a suffix.) \\
%Pertinent rule: 4
%%
%%Cluster #184 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active feature: p$<$g (1.0) \\
%Morph: None \\
%Pertinent rule: 1b 
%%
%%Cluster #201 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: x$<$z (1.0), z$<$q (0.98) \\
%Morph: x.z.q. (root) \\
%Pertinent rule: 1b
%%
%%Cluster #219 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: d$<$h (1.0), h$<$z (1.0), z$<$d (0.99), z$<$t (0.99) \\
%Morph: None \\
%Pertinent rule: 4
%\end{exe}
%
%Via Chinese: Every symbol in an encoded morfessor segment is itself a distinct morph.
%
%\subsubsection{Stage 2: Match morph characters to word characters} Once a morph has been gleaned from a cluster's centroid vector, the characters of the morph must be matched to the corresponding characters in the cluster's member words. 
%
%The $\mathbf{M}$ matrix contains the cluster activities for each word. By consulting $\mathbf{M}$, we can ascertain the set of clusters to which each cluster belongs. Each cluster can be thought of as essentially equivalent to a particular morph, since the identities of morphs come from clusters. Indeed, Stage 1 ``extracts" from each cluster a $K$-length vector that  contains the key alphabetic characters associated with the cluster. Thus, via the cluster activities, each word is mapped to its set of morphs.
%
%At this point we will know which morphs are associated with each word, but we do not yet know the order of the morphs in words with more than one. Additionally, we will know, for each morph, whether it is a prefix ($P$), suffix ($SU$), or stem-component ($ST$). For example, suppose that the word \textit{whxlwm} (`and the dream') belongs to four clusters, namely those corresponding to the morphs \texttt{w}_{P}, \texttt{h}_{P}, \texttt{xlm}_{ST}, \texttt{w}_{ST}, where the subscripts $P$ and $ST$ stand for \textit{prefix} and \textit{stem-component}, respectively.
%
%First, the prefixes are matched to word characters. Then stem-components are matched, and finally any suffixes are matched (there are no suffixes in the present example). Whenever a morph character matches a word character, the word character is popped from the word, and the morph character is popped from the morph. The word character is then linked to the morph in question. 
%
%Thus, in our example, the matching algorithm proceeds as follows: First, each of the prefixes \texttt{h}_{P} and \texttt{w}_{P} (the order should not matter) are compared to the first character of \textbf{whxlwm}. The \texttt{w}_{P} matches, so we get \{ \texttt{w}_{P}:\texttt{w} \}. The \texttt{w} is popped 
%
%from \textbf{whxlwm} as well as from \texttt{w}_{P}, thus completing the prefix morph \texttt{w}_{P} and removing it from consideration. Next, \texttt{h}_{P} (the only remaining prefix) is matched to the \texttt{h} of \textbf{hxlwm}, leaving us with \{  \texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h} \}, \textbf{xlwm}, and no more prefixes.
%
%Now only the stem-components remain. \texttt{w}_{ST} fails to match the first character 
%of \textbf{xlwm}. However, the \texttt{m} of \texttt{xlm}_{ST} does, yielding
% \{ \texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:m\}, 
% \textbf{lwm}, an stem-components {\texttt{lm}_{ST} and \texttt{w}_{ST}. 
% The \texttt{q} of {\texttt{lm}_{ST} matches the \texttt{q} in \textbf{qm}, giving us 
% \{\texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:\texttt{mq}\}, 
% \textbf{wm}, and stem-components \texttt{m}_{ST} and 
% \texttt{w}_{ST}. Next, \texttt{w}_{ST} matches the \texttt{w} in \textbf{wm}, 
%completing the stem-component \texttt{w}_{ST}. We now have \{\texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:\texttt{mq}, \texttt{w}_{ST}:\texttt{w}\}, \textbf{m}, and  
% \texttt{m}_{ST}.
% Finally, \texttt{m}_{ST} matches \textbf{m}, completing \{\texttt{xlm}_{ST}\} and consuming the word's last remaining character.
%%	\begin{verbatim}
%%		.*(x).*(\u00F3).?.?(t).*
%%	\end{verbatim}
%%	\texttt{.*(x).*(\'{o}).?.?(t).*}
%\subsubsection{Stage 3: Compress}. Here, each morph to an atomic symbol, 
%so that, e.g., a three-character morph becomes in effect single-character item, 
%as do all morphs consisting of more than one character. In this way, most of 
%the words is shortened (in terms of character-count) and thus compressed.
%Each atomic morph symbol is a unique unicode character. For example, 
%consider the 
%Hebrew words \textit{magdil} and \textit{gadol}, which share the root 
%\textit{g.d.l}. 
%Suppose that the Stage 2 outputs for these words are as in \eqref{ex:magdil} and 
%\eqref{ex:gadol}, respectively. 
%\begin{exe}  \ex \label{ex:unicode} \begin{xlist}
%	\ex magdil \quad ma-, \,\, g.d.l, \,\, i 
%	\label{ex:magdil}
%	\ex gadol \, \quad  g.d.l, \,\, a.o
%	\label{ex:gadol}
%	\end{xlist}
%\end{exe}
%Altogether, there are four \emph{unique} morphs in \eqref{ex:unicode}, namely \textit{ma-}, \textit{g.d.l}, 
%\textit{i}, and \textit{a.o}.
%Stage 3 will map each of these to a unique symbol, as in \eqref{ex:map}, for instance.
%\begin{exe}
%	\ex  \textit{ma-} $\mapsto$ \$ \quad \textit{g.d.l} $\mapsto$ \% \quad
%\textit{i} $\mapsto$ \& \quad \textit{a.o} $\mapsto$ \#
%\label{ex:map}
%\end{exe}
%Finally, each word is reassembled with atomic symbols being substituted for the morphs. 
%They are put together in the original order of their corresponding morphs.
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex magdil \quad \$\%\&
%	\label{ex:re-magdil}
%	\ex gadol \, \quad \%\#
%	\label{ex:re-gadol}
%	\end{xlist}
%\end{exe}
%% The atomic symbols are put together in the order of their corresponding character sequences, as illustrated in \eqref{ex:reassembled}. 
%Note, however, that in the case of interdigitation, the relative order of atomic symbols corresponding to interleaved sequences must be decided arbitrarily.
%
%\subsubsection{Stage 4: Test} 
%Two input files, a test and a 
%control file, are now fed to Morfessor. 
%The test file is the output of Stage 3.  %will contain the test data, i.e., the compressed words from Stage 3. 
%The control file consists of the original, unaltered words; its purpose is to serve as a baseline for measuring the effect 
%of the compression carried out in Stage 3. 
%% But what gives
%The idea here is to see if the MCMM's morphs, 
%now represented as atomic symbols, aide the 
%process of morphological segmentation.  % But how can we tell if the atomic symbols help?
%% What are they supposed to help?  Are they supposed to aide in the process of discovering the actual morphemes (i.e., the non-compressed, non-reduced morphemes)? If so, we need to somehow translate atomic symbols back into character sequences, but retaining the segmentation divisions computed on the strings of atomic symbols.
%Morfessor induces morphological segmentations for each file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, the atomic symbols were converted back into morphs (i.e., Stage 3 was undone).
%\begin{exe}
%	\ex   \$ $\mapsto$  \textit{ma-}
%	\quad \% $\mapsto$ \textit{g.d.l} 
%	\quad  \& $\mapsto$ \textit{i}
%	\quad  \# $\mapsto$ \textit{a.o}
%\label{ex:map}
%\end{exe}
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex  \$ \, + \, \% \, + \,  \& \quad $\mapsto$ \quad  ma \,+ \, gdl  \, + \, i
%	\label{ex:reconverted-magdil}
%	\ex  \% \, + \, \# \quad $\mapsto$  \quad gdl \, + \, ao
%	\label{ex:reconverted-gadol}
%	\end{xlist}
%\end{exe}
% The result is a test segmentation file whose words have the same characters as those of the control file, but possibly quite different segmentations. The two segmentations were evaluated against a common gold standard.
%%That is, do they make the task easier? 
%%Do they improve segmentation accuracy? 
%%Morfessor %then
%% induces morphological segmentations for each file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation will at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, change the atomic symbols back to morphs (i.e., undo Stage 3). Finally, evaluate the two segmentations against a common gold standard.
%
%\subsection{Gold-standard data}
%Recall that Morfessor is a nonlinear \emph{sequential} algorithm; that is, it possesses nonlinearity, but not nonsequentiality (see section~\ref{sec:nls})
%and is thus incapable of detecting non-concatenative roots and patterns. 
%Moreover, interdigitation is lost when atomic symbols are substitute for character sequences. 
%Two basic types gold-standard data were therefore required: one to assess Morfessor's output and one to evaluate the 
%stem-internal root-and-pattern morphology.
% 
%\paragraph{Morfessor gold standard}
%%I will need gold-standard segmentations against which to assess Morfessor's output.  
%Morfessor produces two output (or analysis) files, one for the test file (processed words) and one for the control file (original or uprocessed words)
%To obtain gold-standard datasets for Morfessor, I manually segmented $\frac{1}{10}$ of the original, unprocessed wordlist.
%This amounts to a total of three unprocessed wordlists: a list of transcribed words, a list of transcribed words with stress marked, and a list of words spelled according to orthographic conventions. 
%
%\paragraph{Non-concatenative gold standard}
%To evaluate my system's performance on non-concatenative morphology, I need gold-standard roots for both the transcribed wordlists and the orthographic data. For the transcribed data, I extracted root annotations from the CHILDES morphological analyses. 

%For the orthographic wordlist, I will take advantage of the root annotations provided in the original dataset of \cite{daya-et-al:2008} (see section~\ref{sec:data}).
% 
%\paragraph{Gold-standard annotation}
%How are we going to figure this out? The control will serve as the baseline. 
%But we also need a gold standard segmentation to provide a frame of reference for comparing the test and control segmentations. 
%That is, the respective accuracies of the test and control segmentations 
%is measured with respect to the gold standard.

%How large does the gold-standard set need to be? 1000 words? 1/10 of the total data set?

%\subsection{Ancillary (Gold-Standard-Based) Evaluation}
%This evaluation will involve mapping \textsc{mila-ma}'s categories onto different sets of gold-standard categories. How will these sets differ? The idea, I guess, is to use sets with varying amounts of granularity, or different amounts of modification, or perhaps different types of modification.
%For example, do we want the categories to create a strict partition? Maybe the gold-standard categories could/should overlap.
%
%An MCMM clusters its input vectors (= words) according to shared hidden-unit activations. 
%Its clusters overlap one another; i.e.,
%a single word can belong to several clusters at once. % because a word can contain multiple morphemes. 
%%Evaluating overlapping clusters is more complicated than evaluating disjoint clusters. 
%To evaluate these clusters,
%I will use the measures \emph{BCubed Precision} and \emph{BCubed Recall}, 
%which are specially designed for overlapping clusters \citep{amigo-et-al:2009}.
%%Precision and recall evaluate a system's output against an external gold standard. 
%
%I will use a
%finite-state morphological analyzer, namely the MILA Morphological Analysis tool (\textsc{mila-ma}) 
%\citep{hebrew-resources:2008} to generate gold-standard categories. It is, however, non-trivial to apply \textsc{mila-ma} to this purpose.
%is not entirely straightforward. First, there is the general problem of evaluating an unsupervised clustering algorithm
%against external criteria. One usually has an idea about what the output should be, 
%but without a training set,
%it is difficult to be specific about this. 
%Second, one must consider that
%\textsc{mila-ma}'s particular categories may not be appropriate for cluster evaluation in every case.
%
%For example, \textsc{mila-ma} tends to use atomic categories such as \textsc{masc} and \textsc{pl} as opposed to \textsc{masc.pl}. In the case of Hebrew, \textsc{masc} and \textsc{pl} are abstract because neither corresponds to an actual morpheme. That is, the Hebrew \textsc{masc.pl} suffix \textit{-im} is fusional; it cannot be split into separate \textsc{masc} and \textsc{pl} substrings (cf. \textsc{masc.sg} forms, 
%which have no ending, and the \textsc{fem.pl} suffix \textit{-wt}).
%A clustering algorithm would be 
%disinclined to treat \textsc{masc} and \textsc{pl} as \textsc{mila-ma} does, since this would mean creating separate \textsc{masc} and \textsc{pl} clusters  despite the lack of a \emph{particular} \textsc{masc} suffix and a \emph{particular} \textsc{pl} suffix.
%%lack of shared formal elements. clusters because such clusters would not be based on shared formal elements; \textsc{masc} 
%%would contain words ending in -$\emptyset$ and \textit{-im}, and \textsc{pl} would contain words ending in \textit{-wt} and \textit{-im}.
%For this and similar reasons, \textsc{mila-ma}'s categories will need to be mapped to a somewhat adapted set 
%of gold-standard categories.

%****************
%\begin{description}
%\item[Stage 1: Extract.] Derive morphs from cluster centroids. That is, for each cluster centroid vector,  map the \emph{active} features to a particular sequence of alphabetic characters. The mappings is governed by a set of mapping rules. The resulting sequence of alphabetic characters is the morph. Repeat this process for each cluster (i.e., cluster centroid).
%
%\textbf{Example:} Suppose that \texttt{z<k} and \texttt{k<r} are the active features in a given cluster's centroid. These features would map to the (potentially discontinuous) character sequence \textit{zkr}. The morph would thus be the root \textit{z.k.r}.
%
%\item[Stage 2: Match.] Map morph characters to word characters. That is, given a cluster and its morph (obtained in Stage 1), go through the cluster's words, and in each word, determine which characters are the morph's characters. Label these characters as components of the morph in question. Repeat this process for each cluster/morph.
%Each morph is a (possibly discontinuous) sequence of alphabetic characters. 
%Given a cluster and its newly extracted morph, identity the morph's characters in each of the clusters words. 
%That is, for each word, match the morph's characters to the \emph{correct} word characters. Note that there is potential for ambiguity here. Suppose, for example, that the morph in question is the \textit{-wt}. It's easy enough to find a single \texttt{t} in a string of letters, but \texttt{t} is a frequently occurring letter, and it could easily occur elsewhere in the word. I have to make sure my matching algorithm selects the right character in cases like this. Repeat this process for each cluster. 
%For each word $w$ in a given cluster, identify the characters in $w$ that correspond to the morph's characters. counterparts of each the morph characters to their counterpart character in the word in question. with their matching characters in the word in 
%the characters of the morph to the corresponding characters in the cluster's member words.
%
%\paragraph{Example:}  
%Consider a cluster whose member words are \textit{mazkir}, \textit{hizkir}, \textit{zoker}, \textit{zokrim}, and \textit{zikron}. The morph in this case is the root \textit{z.k.r}. Stage 2 identifies the root consonants in each word and labels them as components of the morph \textit{z.k.r}. Here, the morph's characters are ``labeled" via boldface type:
%%\footnote{In reality, of course, a larger and more sophisticated labeling/indexing system is necessary, as every morph will require a distinct label/index.}:  
%\textit{ma\textbf{zk}i\textbf{r}},
%\textit{hi\textbf{zk}i\textbf{r}}, \textit{{z}o\textbf{k}e\textbf{r}}, \textit{\textbf{z}o\textbf{kr}im},
%and \textit{\textbf{z}i\textbf{kr}on}.
%The process is repeated for each cluster/morph.
%
%\item[Stage 3: Compress.] Map each morph to a single unique unicode character.
%\textbf{Example:} Consider the 
%Hebrew words \textit{magdil} and \textit{gadol}, which share the root \textit{g.d.l}. 
%Suppose that the Stage-2 outputs for these words are as in \eqref{ex:magdil} and \eqref{ex:gadol}, respectively. 
%\begin{exe}  \ex \label{ex:unicode} \begin{xlist}
%	\ex magdil \quad ma-, \,\, g.d.l, \,\, i 
%	\label{ex:magdil}
%	\ex gadol \, \quad  g.d.l, \,\, a.o
%	\label{ex:gadol}
%	\end{xlist}
%\end{exe}
%Altogether, there are four \emph{unique} morphs in \eqref{ex:unicode}, namely \textit{ma-}, \textit{g.d.l}, 
%\textit{i}, and \textit{a.o}.
%Each of these is mapped to a unique atomic symbol, as in \eqref{ex:map}.
%\begin{exe}
%	\ex  \textit{ma-} $\mapsto$ \$ \quad \textit{g.d.l} $\mapsto$ \% \quad
%\textit{i} $\mapsto$ \textit{i} \quad \textit{a.o} $\mapsto$ \#
%\label{ex:map}
%\end{exe}
%%Let the atomic symbols inherit the sequential order of their counterpart morphs. 
%In general, the atomic symbols inherit the ordering of the original morphs. 
%The exceptional cases are those of interdigitation. When two morphs are interleaved, 
%they are unordered with respect to each other.
%However, when they are mapped to atomic symbols, they necessarily 
%take on an arbitrary relative order because there is no way to interleave two 
%\emph{atomic} units: either $A$ precedes $B$ or $B$ precedes $A$; 
%there is no other option.
%%but with the atomic symbols now taking the places of of the original morphs. Put the symbols in the same order as their morph counterparts.  hat for morphs that are two or more characters long.. They are put together in the same order as the original morphs. This reducing or elsince it replaces whole character sequences, even discontinuous ones, with atomic symbols (see section~). 
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex magdil \quad \$\%\textit{i}
%	\label{ex:re-magdil}
%	\ex gadol \, \quad \%\#
%	\label{ex:re-gadol}
%	\end{xlist}
%\end{exe}
%%Now, when the characters of the two morphs are interleaved, i.e. in the case of interdigitation, the relative order of the morphs is indeterminate. However, when these two morphs are mapped to atomic symbols, they necessarily take on an arbitrary relative order. That is, either $A$ precedes $B$ or $B$ precedes $A$; there is no other option. 
%The mapping from morphs to atomic symbols thus abstracts interdigitation.  
%
%\item[Stage 4: Test.]
%%-- concatenative morphs.]
%Feed both the control file (i.e., the file containing the original wordlist) and the test file (i.e., the output of Stage 3) to
%Morfessor \citep{creutz-and-lagus:2005, creutz-and-lagus:2007}. Morfessor then induces morphological segmentations for each input file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation will at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, change the atomic symbols back to morphs (i.e., undo Stage 3). Finally, evaluate the two segmentations against a common gold standard.
%%control file, will 
%%%now be fed to Morfessor. 
%%The test file is the output of Stage 3.  The control file will contain the original, unaltered words. 
%%It will provide a baseline 
%%for measuring the effect of the compression carried out in Stage 3. 
%%The idea here is to see if the MCMM's morphs, 
%%now represented as atomic symbols, aide the process of morphological segmentation. 
%%That is, do they make the task easier? 
%%Do they improve segmentation accuracy? 
%\end{description}

