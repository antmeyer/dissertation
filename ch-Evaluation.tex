\chapter{DUAL-PARADIGM EVALUATION}
\label{ch:eval}

\section{Introduction}
%\lipsum[1]
%\ttfamily\lipsum[1]

%\paragraph{Evaluation.} 
Unsupervised learning systems like Multimorph are inherently difficult to evaluate. 
%This is because, supervised learning, the correct answers--e.g., the correct
%categories for the input examples are basically unknown. They are not supplied to the learner, at any rate. 
%This is because an unsupervised learning system is not provided with explicit learning targets.
Arguably, the main advantage of unsupervised learning is that it can discover novel and potentially useful categories and associations \citep{parsons:2004}. 
On the other hand, it is very difficult to evaluate previously unknown 
categories according to precision and recall. % and other measures that rely ``answer keys." 
The previously unknown categories in this study were morphological 
categories that would perhaps embody Aronoff's notion of the morphome \citep{aronoff:1994}.  
However, we did not know the precise form these categories would take. 

%---recall from REF that we use the term morph to avoid suggesting equivalence with he were unknown because we had  units were not conventional morphemes 
%or morphosyntactic categories. 

%Instead, MCMM-generated clusters corresponded roughly to Aronoff's 
%\emph{morphomes} \citep{aronoff:1994}, 
%which can be described as 
%\emph{pre-morphosyntactic} units, i.e., units that have been assembled from 
%phonemes, but have not yet been assigned 
%a syntactic or semantic meaning. I use the term \emph{morph} to refer to such units in what follows, since it is a less loaded term than either
%either \emph{morpheme} or \emph{morphome}.
%\emph{morphome}, since MCMM-generated clusters may not correspond 
%precisely to morphomes in every case (see section REF). %~\ref{sec:targets}).

Thus, the evaluation itself presented an important research question, namely the question 
of how to evaluate the output of an unsupervised morphological clustering algorithm, 
particularly one that considers only features of \emph{word-internal form}, having no 
access to morphosyntactic features that exist outside of the word, e.g., person, gender, and 
number of surrounding words.

%Multimorph is an unsupervised machine learning system, which makes it 
%intrinsically difficult to evaluate, and thus no single 
%evaluation method is likely to be perfect. 

I thus devised a dual-paradigm method for evaluating Multimorph's output, i.e., an approach consisting of two complementary methods, %I would thus rather not rely on a single method.
%not only because is it an unsupervised learning system, which are notoriously challenging to evaluate,  it learns unconventional morphological units, namely morphs. Indeed, it would be very difficult to come up with a gold standard for the morphs because we do not know what the morphs are supposed to look like; i.e., the ``right answers" are not obvious at the outset. 
%In particular, I will use 
one \emph{intrinsic} and one \emph{extrinsic}. 
The evaluation also included a \emph{qualitative} component, which served to complement the quantitative 
%utput of the MCMM was evaluated \emph{qualitatively}. The  which is intended to complement the quantitative
 intrinsic an extrinsic methods. We will discuss each of these components in what follows.


\section{Qualitative Evaluation} \label{sec:qualitative}
The qualitative component of the evaluation consisted mainly in directly (or ``manually") examining the clusters generated by Multimorph's MCMM generated. These clusters are words grouped together according to some shared elements of form. These shared elements are often immediately obvious to the human eye, but even in such cases they can become diluted or lost in automated quantitative assessment.   %shared elements of form are immediately obvious to the human eye, but they can easily become lost in automated 
%A qualitative componentmponent is advantageous is that the automaticl,kl quantative procedures may not catch everything. 
%Humans also have linguistic intuition, and since we are dealing with linguistic data, which can be very useful in interpreting a cluster. Since we are dealing with unsupervised learning, there is basically no telling what the clusters will come to represent. The qualitative component is meant to address this essentially lack of predictability.

\section{Quantitative Evaluation}
\subsection{Intrinsic Evaluation}
\label{sec:intrinsic}
An \emph{intrinsic} evaluation regards a system as a stand-alone system and its output as an end unto itself. An intrinsic evaluation thus judges a system's output directly as opposed to indirectly via the performance of a downstream system. However, intrinsic evaluations still require some \emph{externally-generated} standard against which to measure a system's performance. Such a standard is called a \emph{gold standard} and generally consists of a set of ``correct answers'' that humans have had some part in creating.
%That is, an intrinsic evaluation thus evaluates a system directly, on its on terms; any other system is irrelevant. 
%disregards 
% as independent and isolated from other systems, examining its 
%output directly. That is, the system is evaluated as a stand-alone application, on its
%on terms, as it were,
%not as one embedded in a larger system.

The evaluation's intrinsic component thus required a 
\emph{gold-standard categorization} against which to evaluate 
Multimorph's clusterings,\footnote{Note the use of the word {categorization} 
instead of \emph{clustering} in the term \emph{gold-standard categorization}. 
By convention, gold-standard clusters are not actually called clusters, but rather 
but (gold-standard) \emph{categories}}, and thus measure the precision 
and recall of Multimorph's clusterings. 

In supervised learning, the training process requires that each learning example 
(or data point) have a target label, i.e., the ``correct answer.'' The target labels 
later serve as the gold-standard labels during the evaluation the trained model. 
In supervised learning, therefore, it is trivial to obtain a gold-standard 
for evaluation; if a model was trained at all, then the gold standard 
already exists. By contrast, in unsupervised learning, obtaining a gold-standard 
dataset can be much less straightforward. One problem, as mentioned above, 
is that unsupervised learning is by nature open-ended; it is 
supposed result in the discovery of categories and relationships.  

Another difficulty (where obtaining a gold standard is concerned) is the 
form-centric nature of Multimorph's learning domain. Multimorph learns morphs, 
which are autonomous morphological categories, as discussed in chapter~\ref{autonomous}. 
They mediate between phonology and morphosyntax, but are at the same time independent 
of both of these levels of structure. It is thus not always clear what the morphs are in a 
given data set. As intermediate units that are unconstrained by meaning, they tend 
to be whatever they need to be in order to bridge the gap between phonological units 
and meaning (syntax and semantics).  There are thus not many morphomic lexicons 
or corpora wherein autonomous morphological units are annotated. There are no 
such resources for Hebrew.

There is in fact no ideal choice for a gold-standard dataset for Multimorph, 
not at the present time, at any rate. One of motivating factors behind this 
thesis is that not much is currently know about the autonomous morphologies 
of particular languages, and that unsupervised learning can contribute to the 
process of discovering autonomous morphological categories. At the same time, 
however, this means that gold-standard datasets are not going to be forthcoming. 
I chose to include this intrinsic evaluation component largely as means 
to supplement and support the highly experimental extrinsic evaluation 
component.  It was an exercise in maximizing the utility of less-than-ideal data, data that was not really
suitable to the present study.

As described in chapter~\ref{ch:experi}, the source of Multimorph's input words was the 
Berman Longitudinal Corpus. Each word in this corpus is both (quasi-)phonetically 
transcribed as well as morphologically annotated, as described in chapter~\ref{ch:experi} 
(see in particular section~\ref{sec:morph-annotation}).
%i.e., labeled with morphological categories. 
%Each word thus comes with a 
%morphological analysis consisting of one or more morphological categories (see \ref{sec:morph-annotation}).
The first step in 
building the gold-standard data was to extract each word's morphological analysis 
or analyses (section~\ref{sec:extr}). 

\begin{figure}[t]
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
\vdots \> \vdots \\
hes\a'{i}gu \> pos:v\&root:nsg\&ptn:hifil\&tense:past\&pers:3\&gen:unsp\&num:pl \\
hev\a'{a}nt \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:2\&gen:fm\&num:sg \\
hev\a'{a}nti \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:1\&gen:unsp\&num:sg \\
hev\a'{e}\textipa{P}nu \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:1\&gen:unsp\&num:pl \\
hev\a'{e}\textipa{P}t \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:2\&gen:fm\&num:sg \\
hev\a'{e}\textipa{P}ta \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:2\&gen:ms\&num:sg \\
hev\a'{e}\textipa{P}tem \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:2\&gen:ms\&num:pl \\
hev\a'{e}\textipa{P}ti \> pos:v\&root:bw\textipa{P}\&ptn:hifil\&tense:past\&pers:1\&gen:unsp\&num:sg \\
hev\a'{i}n \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:3\&gen:ms\&num:sg \\
hev\a'{i}na \> pos:v\&root:byn\&ptn:hifil\&tense:past\&pers:3\&gen:fm\&num:sg \\
\vdots \> \vdots \\
\end{tabbing}
\label{fig:analyses}
\caption{Morphological analyses extracted from the Berman Longitudinal Corpus.}
\end{mdframed}
\end{figure}

%I stated in chapter~\ref{autonomous}
%The intrinsic component of this study's evaluation compares an MCMM-generated clustering to a 
%\emph{gold-standard categorization},  computing its precision and recall according to the extent to which it matches the gold-standard categorization. 




%        self.clusters_justWords = []
%        self.Rv = Rv
%        self.Mv = Mv
%        self.Cv = Cv
%        cdef INT i, k, j, n
%        cdef INT I = self.Rv.shape[0]
%        cdef INT K = self.Mv.shape[1]
%        cdef INT J = self.Cv.shape[0]
%        #cdef object clusters_m, clusters_mr, clusters_mc, clusters_mcr
%        #clusters_m = [[] for k in range(K)]
%        #clusters_mr = [[] for k in range(K)]
%        clusters_mc = [[] for k in range(K)]
%        #clusters_mcr = [[] for k in range(K)]
%        self.clusters_justWords = [[] for k in range(K)]
%        cdef bint membership = 0
%        cdef object word_and_val
%        cdef FLOAT mc = 0.0
%        # "wordList" is a list of the text-formatted words corresponding to the datapoints (feature vectors).
%        # the activations matrix M contains I rows (# of data points) and K columns (# of clusters).
%        # the k-th column in M corresponds to the k-th column in C.
%        for i in range(I):
%            # Iterate over the row M[i], which is a row of k elements
%            # Each k represents a cluster
%            for k in range(K):
%                # for each index k, find the kth column in the
%                # J x K matrix C.
%                # Then proceed down the j row indices in this column
%                # until a [j,k] cell is found that meets the membership
%                # criteria. Only one such cell is needed.
%
%                # if self.Mv[i,k] >= thresh:
%                #     word_and_val = (wordList[i], "{:.4f}".format(self.Mv[i,k]))
%                #     if word_and_val in clusters_m[k]:
%                #         pass
%                #     else:
%                #         clusters_mcr[k].append(word_and_val)
%                #     if wordList[i] in self.clusters_justWords[k]:
%                #         pass
%                #     else:
%                #         self.clusters_justWords[k].append(wordList[i])
%
%                # if self.Mv[i,k] >= thresh:
%                #     for j in range(J):
%                #         if (self.Rv[i,j] >= 0.5):
%                #             membership = 1
%                #             break
%                # if membership == 1:
%                #     membership = 0
%                #     word_and_val = (wordList[i], "{:.4f}".format(self.Mv[i,k]))
%                #     if word_and_val in clusters_mr[k]:
%                #         pass
%                #     else:
%                #         clusters_mr[k].append(word_and_val)
%                #     if wordList[i] in self.clusters_justWords[k]:
%                #         pass
%                #     else:
%                #         self.clusters_justWords[k].append(wordList[i])
%
%                mc = 0.0
%                for $ \gets j in range(J):
%                    mc = self.Mv[i,k]*self.Cv[j,k]
%                    if (mc >= thresh):
%                        membership = 1
%                        break
%                if membership == 1:
%                    membership = 0
%                    word_and_val = (wordList[i], "{:.4f}".format(mc))
%                    if word_and_val in clusters_mc[k]:
%                        pass
%                    else:
%                        clusters_mc[k].append(word_and_val)
%                    if wordList[i] in self.clusters_justWords[k]:
%                        pass
%                    else:
%                        self.clusters_justWords[k].append(wordList[i])
\begin{algorithm}
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead 
\KwIn{$\mathbf{M}$,  $\mathbf{C}$, input words $\mathbf{w}$, \text{threshold} $\theta$, $I$ $J$,$K$} 
\KwOut{$\mathbf{U}$, a list of \emph{clusters}, in which each cluster is a list of words. Let $\{\mathbf{u}_k$ denote the $k$th cluster, i.e., the list of words composing the $k$th cluster.}
 %$k \in K\}$, where each $\mathbf{u}_k$ is the list of the $k$th cluster's (active) member words}
%
%belong to that cluster.
%$\text{clusters} \gets \big[\,[\,\,]_0, [\,\,]_1, [\,\,]_2, \,\dots, \, [\,\,]_K \,\big]$\;
Initialize cluster member lists: $\mathbf{U} = \{\mathbf{u}_0, \mathbf{u}_1, \mathbf{u}_2,\ldots, \mathbf{u}_K\}$ where each $\mathbf{u}_k \gets \emptyset$\;
\For {$ i \gets 0$ \textbf{to} $I$}{
	%$w \gets \text{words}[i]$\;
	\For {$ k  \gets 0$ \textbf{to} $K$}{
	%$\text{Clusters}[k] \gets \emptyset_k$\;
	%$\text{vote}_{k,j} \gets 0$\;
	$j \gets 0$\;
	%$\text{threshold-met} \gets \textbf{False}$\;
		%\# Make sure there is at least one $c_{j,k}$ such that $m_{i,k}c_{j,k}\geq \theta$,\;
		%\# where $\text{vote}_{k,j} = m_{i,k}c_{j,k}$\;
		%\While {$m_{i,k}c_{j,k} < \theta$ \textbf{and} $j < \textsc{Number-of-Rows}(\textbf{C})$}{
		\While {$m_{i,k}c_{j,k} < \theta$ \textbf{and} $j < J$}{  \label{line:vote-thresh}
		%\While {$m_{i,k}c_{j,k} < \theta$ \textbf{and} $j < \textsc{length}({\textbf{C}_{(\bullet,k)})$}{
		%{$\text{vote}_{k,j} < \theta$ \textbf{and} $j < \textsc{Number-of-Rows}(\textbf{C})$}{  
			%$\text{vote}_{k,j} \gets m_{i,k}c_{j,k}$\;
			$j \gets j + 1$\;
			%$\text{threshold-met} \gets \textbf{True}$\;
		}
	%\If{$\text{threshold-met}$}{
	\If{$m_{i,k}c_{j,k} \geq \theta$}{
		%$\text{threshold-met} \gets \textbf{False}$\;
		%$\text{Clusters}[k] \gets \text{Clusters}[k] \cup w_i$\;
		$\mathbf{u}_k \gets \mathbf{u}_k \cup w_i$\; \label{line:append}
	}
	}
}
\Return $\mathbf{U}$\;
\caption{\textsc{Clusters-and-Members}}
\label{alg:members}
\end{algorithm}

The purpose of the while loop beginning on line~\ref{line:vote-thresh} is to impose the following constraint on cluster membership: In order for a word to be a member of a given cluster, the cluster must be responsible for at least one active surface unit in the word's reconstruction vector.
 Recall from chapter~\ref{ch:MCMM} that \emph{votes} in an MCMM are products of the form $m_{i,k}c_{j,k}$, and that the Noisy-OR mixing function requires at least one `\textsc{yes}' vote in order to activate a given reconstruction unit. For Multimorph's purposes, a vote is treated as a `\textsc{yes}' vote as long as it satisfies the criterion 
 \begin{equation}\label{eq:vote-criterion}
 m_{i,k}c_{j,k} \geq \theta_{\text{mc}}
 \end{equation}
 where $\theta_{\text{mc}}$ is a threshold. 
 The while loop on \ref{line:vote-thresh} always begins with a particular $k$ index supplied 
 by the loop within which it is nested. We will call this $k$ the index of the current cluster. 
 The while loop proceeds to increment the index $j$ and thus descend the cells of the ($k$th) 
 column in $\mathbf{C}$ (i.e., the current cluster's centroid vector). Each new $j$ corresponds 
 to a different cell, i.e., a different  $c_{j,k}$, and thus a different vote $m_{i,k}c_{j,k}$. 
 The procedure visits each cell in turn until it finds a `\textsc{yes}' vote, i.e., a cell $c_{j,k}$ 
 such that the product $m_{i,k}c_{j,k}$---i.e., the weighted vote of the $k$th cluster---exceeds the membership threshold $\theta^{\text{mc}}$. 
 %the vote $m_{i,k}c_{j,k}$ exceeds the threshold $\theta^{\text{mc}}$.
The procedure only needs to find one `\textsc{yes}' vote to verify that the data point (or word) in question is an \emph{active} member of the $k$th cluster, a consequence of the Noisy-OR mixing function (see section~{sec:mixing-function}). 
%When a vote $m_{i,k}c_{j,k}$ is greater than $0$ but less than $\theta_{\text{mc}}$ for some cluster $k \in K$ and some feature $j \in J$, we say that this vote indicates \emph{inactive} membership in cluster $k$, but not active membership. 
%%If $0 \leq m_{i,k}c_{j,k} < \theta_{\text{mc}}$ for all features $j \in J$, then the data point is not an active member of cluster $k$,
%If a data point has no threshold-exceeding vote for any $k \in K$ or $j \in J$---i.e., if \emph{none} of its features can be attributed
%to any cluster, then the data point is not an active member of any cluster, in which case, we say the data point (or word) is not \emph{covered} by the clustering. A clustering's \emph{coverage}, a concept that will prove relevant to our discussion of the results in chapter~\ref{ch:results}.

%``\emph{inactive} membership," i.e., $j$ indices such that $0 < m_{i,k}c_{j,k} < \theta^{\text{mc}}$. 
%
%at least one of the $i$th datapoint's (i.e., the $i$th word's) features can be attributed to the $k$th cluster, for if at least one of its features (or surface units) can be so attributed to cluster $k$, then we can be sure that the $i$th datapoint is a member of cluster $k$.   member  cluster.  there is at least one \emph{active}
% surface unit associated with the current cluster. 
 
 Thus, the while loop terminates upon finding 
 such a vote, whereupon
the current ($i$th) word is appended to the active members of the current cluster (line~\ref{line:append}). 
The procedure iterates through the entire input wordlist and appends words to clusters member lists 
whenever criterion \eqref{eq:vote-criterion} is satisfied. It outputs each cluster's list of active member words.
 
 When a vote $m_{i,k}c_{j,k}$ is greater than $0$ but less than $\theta_{\text{mc}}$ for some cluster $k \in K$ and some feature $j \in J$, we say that this vote indicates \emph{inactive} membership in cluster $k$, but not active membership. 
%If $0 \leq m_{i,k}c_{j,k} < \theta_{\text{mc}}$ for all features $j \in J$, then the data point is not an active member of cluster $k$,
If a data point has no threshold-exceeding vote for any $k \in K$ or $j \in J$---i.e., if \emph{none} of its features can be attributed
to any cluster, then the data point is not an active member of any cluster, in which case, we say the data point (or word) is not \emph{covered} by the clustering. A clustering's \emph{coverage}, a concept that will prove relevant to our discussion of the results in chapter~\ref{ch:results}.
 
% \theta$ $1$ (or close to $1$) for at least one $k \in K$ in order for $r_{i,j}$ to be $1$ (or close to $1$). The constraint imposed on line~\ref{line:vote-thresh} thus ensures that 
%\begin{algorithm}
%\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead 
%\KwIn{A set $C = \{c_1, c_2, \ldots, c_r\}$ of denominations of coins, where $c_i > c_2 > \ldots > c_r$ and a positive number $n$}
%\KwOut{A list of coins $d_1,d_2,\ldots,d_k$, such that $\sum_{i=1}^k d_i = n$ and $k$ is minimized}
%$C \gets \emptyset$\;
%\For{$i \gets 1$ \textbf{to} $r$}{
%  \While{$n \geq c_i$} {
%    $C \gets C \cup \{c_i\}$\;
%    $n \gets n - c_i$\;
%  }
%}
%\Return{$C$}\;
%\caption{{\sc Change} Makes change using the smallest number of coins}
%\label{algo:change}
%\end{algorithm}

In this way, the $\mathbf{M}$ and $\mathbf{C}$ matrices together provide all the information necessary 
to derive a list of word-to-cluster mappings: The $\mathbf{M}$ 
matrix tells us the indices of the clusters to which a given data point (or word) 
belongs, and the $\mathbf{C}$ matrix provides a meaningful cluster label for each 
cluster index. Each word-to-cluster mapping thus 
consists of a word followed by a list of cluster labels.  
as in ``runs: run, -s,"  where \textit{runs} is the word, and \textit{run} and \textit{-s} 
are the labels of the clusters to which \textit{runs} belongs. These labels also represent 
morphs. That is, e.g., \emph{-s} represents a cluster whose words end with the morph 
\emph{-s}. 

%To evaluate a list of mappings like ``runs: run, -s," we need an analogous 
%gold-standard list, one that supplies, for each word, a list of gold-standard \emph{categories}.
%This is to say that we need a gold-standard morphological analysis for each word. 
%We needed different gold standard lists for the orthographic and transcribed datasets, 
%since these datasets come from different sources, and their respective gold standards must 
%be obtained through separate means.


\subsection{Gold Standard Categories}

\subsubsection{Verbs} 

\begin{table}[tb]
\centering
 %\footnotesize
 \subtable[Tensed verb forms\label{subtab:hifil-tensed}]{
  \centering
 \setlength{\extrarowheight}{4pt}
 \begin{tabular}{lcccc}
 \toprule
    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
    & \textit{Sg} & \textit{Pl} & \textsc{sg} & \textsc{pl} \\
\cmidrule{2-3}  \cmidrule{4-5}
   \textsc{1.m/f} & hi-tx\'{i}l-ti & hi-tx\'{i}l-nu & \textipa{P}e-tx\'{i}l & na-tx\'{i}l \\
   \hline
   \textsc{2.m} &  hi-tx\'{i}l-ta & hi-tx\'{i}l-tem  &  ta-tx\'{i}l & \\
  \textsc{2.f} & hi-tx\'{i}l-t & hi-tx\'{i}l-ten &  ta-txil-\'{i} & \raisebox{8pt}[0pt]{ta-txil-\'{u}}\\
   \hline
 \textsc{3.m} & hi-tx\'{i}l &   &  ya-tx\'{i}l & \\
 \textsc{3.f} & hi-txil-\'{a} & \raisebox{8pt}[0pt]{hi-txil-\'{u}} &  ta-tx\'{i}l & \raisebox{8pt}[0pt]{ya-txil-\'{u}} \\
   \bottomrule
 \end{tabular}
 } \\
  \subtable[Participle forms\label{subtab:hifil-part}]{
 \centering
 \setlength{\extrarowheight}{4pt}
      \begin{tabular}{ccc}
      %\multicolumn{3}{c}{\textit{mzkir}  `he reminds'} \\
      \toprule
       & \textsc{sg} & \textsc{pl} \\
      \midrule
      \textsc{m} & ma-tx\'{i}l & ma-txil-\'{i}m \\
      \textsc{f} & ma-txil-\'{a} & ma-txil-\'{o}t \\
      \bottomrule
      \end{tabular}
 }
  \label{fig:paradigms}
 \caption{Past and future-tense paradigms for the root \textit{t.x.l} in the \textit{Hif`il} binyan.}
 \end{table}
 
 \begin{table}[tb!]
  \centering
 %\begin{center}
 %\footnotesize
 \subtable[Tensed verb forms\label{subtab:qal-tensed}]{
 \centering
 \setlength{\extrarowheight}{4pt}
 \begin{tabular}{lcccc}
 \toprule
    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
    & \textit{Sg} & \textit{Pl} & \textsc{sg} & \textsc{pl} \\
\cmidrule{2-3}  \cmidrule{4-5}
   \textsc{1.m/f} & gam\'{a}r-ti & gam\'{a}r-nu & \textipa{P}e-gm\'{o}r & ni-gm\'{o}r \\
   \hline
   \textsc{2.m} &  gam\'{a}r-ta & gam\'{a}r-tem  &  ti-gm\'{o}r & \\
  \textsc{2.f} & gam\'{a}r-t & gam\'{a}r-ten &  ti-gmer-\'{u} & \raisebox{8pt}[0pt]{ti-gmer-\'{u}}\\
   \hline
 \textsc{3.m} & gam\'{a}r &   &  yi-gm\'{o}r & \\
 \textsc{3.f} & gamr-\'{a} & \raisebox{8pt}[0pt]{gamr-\'{u}} &  ti-gm\'{o}r & \raisebox{8pt}[0pt]{yi-gmer-\'{u}} \\
   \bottomrule
 \end{tabular}
 }\\
 \subtable[Participle forms\label{subtab:qal-part}]{
 \centering
 \setlength{\extrarowheight}{4pt}
      \begin{tabular}{ccc}
      %\multicolumn{3}{c}{\textit{mzkir}  `he reminds'} \\
      \toprule
       & \textsc{sg} & \textsc{pl} \\
      \midrule
      \textsc{m} & gom\'{e}r & gomr-\'{i}m \\
      \textsc{f} & gom\'{e}r-et & gomr-\'{o}t \\
      \bottomrule
      \end{tabular}
 }
  \label{tab:paradigms}
 \caption{Past and future-tense paradigms for the root \textit{g.m.r} in the \textit{Qal} binyan.}
 \end{table}
 
% \begin{figure}[tb!]
% \begin{center}
% %\footnotesize
% \begin{tabular}{lcccc}
% \toprule
%    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
%    \cmidrule{2-3}  \cmidrule{4-5}
%    & \textit{Sg.} & \textit{Pl.} & \textit{Sg.} & \textit{Pl.} \\
%\cmidrule{2-3}  \cmidrule{4-5}
%   \textit{1.m/f} & gam\'ar-ti & garm\'ar-nu & \textipa{P}etx\'{i}l & na-tx\'{i}l\\
%   \hline
%   \textit{2.m} & gam\'ar-ta & gam\'ar-tem  &  tigm\'{o}r & \\
%   \textit{2.f} & \raisebox{1.5ex}[0pt]{htxil-t} & htxil-tn &  ta-txil\'{i} & \raisebox{1.5ex}[0pt]{ta-txil-\'{u}\\
%   \hline
%   \textit{3.m} & gam\'ar &   &  yi-gm\'{o}r & yi-gmer\'{u} \\
%   %\cline{1-2}
%   \textit{3.f} & gamr-\'{a} & \raisebox{1.5ex}[0pt]{gamr-\'{u}} &  ta-tx\'{i}l & \raisebox{1.5ex}[0pt]{ya-txil-\'{u}} \\
%   \bottomrule
% \end{tabular}
% \caption{Past and future-tense paradigms for the root \textit{t.x.l} in the \textit{Hif'il} binyan.}
% \label{fig:paradigms}
% \end{center}
% \end{figure}
 The BLC uses the conventional conjugational categories for verbs, i.e., categories corresponding to the cells in a conjugation paradigm.
 % %For every cell in figure~\ref{fig:paradigms}, there is a 
% \textsc{mila-ma} represents verbal inflections for person, number, and gender in %composite 
% \texttt{person/gender/number} (or \texttt{PGN}) features taking values like \texttt{3p/F/Sg} and \texttt{2p/M/Sg}. However, there is not always a one-to-one correspondence between such category labels and the actual distinctions in form. Consider, for example, the verb \textit{ttxil} `she/you will begin'. In the absence of context, this form is entirely ambiguous; it can be either \textsc{2.m.sg} or \textsc{3.f.sg}. \textsc{mila-ma} produces a separate analysis for each possibility, giving rise to two distinct \texttt{PGN} categories, namely \texttt{PGN:2/M/Sg} and \texttt{PGN:3/F/Sg}, where only one form exists. This is similar to the problem of negative and unmarked categories: an MCCM has no means of making a distinction where none exists in the feature representations.


\begin{table}[tb]
\centering
\subtable[One-to-one mappings from paradigm cells to categories\label{subtab:cats-one-to-one}]{
 \setlength{\extrarowheight}{6pt}
\begin{tabular}{lcl}
\toprule
Paradigm Cell &  & Gold-Standard Category \\
\midrule
past-tense, 1st-person singular & $\to$ & \texttt{past\%1\%Sg}\\ \hline
past-tense, 1st-person plural & $\to$ & \texttt{past\%1\%Pl}\\ \hline
past-tense, 2nd-person masc. singular & $\to$ & \texttt{past\%2\%M\%Sg} \\ \hline
past-tense, 2nd-person fem. singular & $\to$  & \texttt{past\%2\%F\%Sg} \\ \hline
past-tense, 3rd-person fem. singular & $\to$  & \texttt{past\%3\%F\%Sg} \\ \hline
future-tense, 1st-person singular & $\to$  & \texttt{fut\%1\%Sg} \\ \hline
future-tense, 1st-person plural & $\to$  & \texttt{fut\%1\%Pl}\\ 
\bottomrule
\end{tabular}
} \\
\subtable[Many-to-one mappings from paradigm cells to categories\label{subtab:cats-many-to-one}]{
 \setlength{\extrarowheight}{6pt}
\begin{tabular}{lcl}
\toprule
Paradigm Cells & &  Gold-Standard Category \\
\midrule
past-tense, 3rd-person plural & \multirow{4}{*}{$\to$} & \multirow{4}{*}{\texttt{(past\%3\%Pl)|(fut\%23\%Pl)}} \\
future-tense, 2rd-person masc plural & & \\
future-tense, 2rd-person fem plural & & \\
future-tense, 3rd-person plural & &\\
\hline
future-tense, 2rd-person masc. singular & \multirow{6}{*}{$\to$} & \multirow{6}{*}{\texttt{fut\%(2\%M)|(23\%F)}} \\
future-tense, 2rd-person masc plural &   & \\
future-tense, 2rd-person fem singular & &  \\
future-tense, 2rd-person fem plural & & \\
future-tense, 3rd-person fem singular & & \\
future-tense, 3rd-person fem plural & &\\
\bottomrule
\end{tabular}
}
\label{tab:verb-cat-descriptions}
\caption{Mappings from conventional paradigm cells (see table~\ref{tab:paradigms}) to gold-standard categories for Multimorph's intrinsic evaluation}
\end{table}

%\texttt{fut\%(2\%M)|(23\%F)}
%\texttt{hifil\%prefix\_stem}
%\texttt{past\%2\%M\%Sg}
%\texttt{fut\%1\%Sg}
%\texttt{qal\%prefix\_stem}
%\texttt{nifal\%suffix\_stem}
%\texttt{fut\%3\%M}
%\texttt{past\%3\%F\%Sg}
%\texttt{fut\%3\%M}
%\texttt{past\%2\%M\%Sg}
%\texttt{past\%3\%Pl}
%\texttt{past\%2\%F\%Sg}
%\texttt{past\%1\%Sg}
%\texttt{past\%2\%F\%Sg}
%\texttt{(past\%3\%Pl)|(fut\%23\%Pl)}

%      \begin{table}[h!]
%      \small
%      \begin{center}
%      \begin{tabular}{ccc}
%      %\multicolumn{3}{c}{\textit{mzkir}  `he reminds'} \\
%      \toprule
%       & \texttt{sg} & \texttt{pl} \\
%      \midrule
%      \texttt{m} & ma-zk\'{i}r & ma-zkir-\'{i}m \\
%      \hline
%      \texttt{f} & ma-zk\'{i}r-\'{a} & ma-zkir-\'{o}t \\
%      \bottomrule
%      \end{tabular}
%      \end{center}
%      \label{tab:hifil-part}
%      \caption{Inflections of {\textit{mzkir}  `he reminds'}}
%    \end{table}
    


%A_{m,n} = 
% \begin{pmatrix}
%  a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
%  a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
%  \vdots  & \vdots  & \ddots & \vdots  \\
%  a_{m,1} & a_{m,2} & \cdots & a_{m,n} 
% \end{pmatrix}
 
 
\subsubsection{Nominals}
\begin{description}
\item[Basic inflections.] Hebrew nominals, i.e., adjectives and nouns, 
inflect for gender and number. However, as discussed in chapter~\ref{autonomous}, 
particularly section~\ref{sec:heb-example}, the Hebrew inflectional endings 
for nominals are \emph{fusional}; it is impossible to separate the  masculine plural suffix \textit{-im} into 
distinct singular and plural components.

\begin{figure}[t]\label{fig:blc-nouns}
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
pas \> pos:n\&gen:ms\&num:sg\&stat:u\\
pas\a'{e}y \> pos:n\&gen:ms\&num:pl\&stat:cstr pos:n\&gen:ms\&num:pl\&stat:cstr\\
pas\a'{i}m \> pos:n\&\&gen:ms\&num:pl\&stat:abs\\
%qi\v{s}q\a'{u}\v{s} \> pos:n\&gen:ms\&num:sg\&stat:u\\
%qi\v{s}u\textipa{P}\a'{i}m \> pos:n\&root:q\v{s}	extipa{P}\&ptn:CiCuC\&gen:ms\&num:pl\&stat:abs\\
%qi\v{s}u\UTF{1E6D}\a'{i}m \> pos:n\&root:q\v{s}\UTF{1E6D}\&ptn:CiCuC\&gen:ms\&num:pl\&stat:abs\\
%qi\v{s}\a'{u}\textipa{P} \> pos:n\&root:q\v{s}\textipa{P}\&ptn:CiCuC\&gen:ms\&num:sg\&stat:u\\
%qi\v{s}\a'{u}\UTF{1E6D} \> pos:n\&root:q\v{s}\UTF{1E6D}\&ptn:CiCuC\&gen:ms\&num:sg\&stat:u\\
qvuc\a'{a} \> pos:n\&root:qbc\&ptn:CCuCa\&gen:fm\&num:sg\&stat:u\\
qvuc\a'{o}t \> pos:n\&root:qbc\&ptn:CCuCa\&gen:fm\&num:pl\&stat:u \\
zriq\a'{a} \> pos:n\&root:zrq\&ptn:CCiCa\&gen:fm\&num:sg\&stat:u \\
zriq\a'{a}t	 \> pos:n\&root:zrq\&ptn:CCiCa\&gen:fm\&num:sg\&stat:cstr \\
%qlaf \> pos:n\&root:qlp\&ptn:CCaC\&gen:ms\&num:sg\&stat:u\\
%qlaf\a'{i}m \> pos:n\&root:qlp\&ptn:CCaC\&gen:ms\&num:pl\&stat:abs\\
%siman\a'{i}m \> pos:n\&root:sym\&ptn:CiCan\&gen:ms\&num:pl\&stat:abs\\
%sim\a'{a}n \> pos:n\&root:sym\&ptn:CiCan\&gen:ms\&num:sg\&stat:u\\
%sin\a'{a}r \> pos:n\&root:snr\&ptn:CiCaC\&gen:ms\&num:sg\&stat:u\\
%sin\a'{o}r \> pos:n\&root:snr\&ptn:CiCoC\&gen:ms\&num:sg\&stat:u\\
%sipr\a'{a} \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:3\&gen:fm\&num:sg\\
%sipr\a'{u} \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:3\&gen:unsp\&num:pl\\
%sipur\a'{e}y \> pos:n\&root:spr\&ptn:CiCuC\&gen:ms\&num:pl\&stat:cstr\\
%sipur\a'{i}m \> pos:n\&root:spr\&ptn:CiCuC\&gen:ms\&num:pl\&stat:abs\\
%sip\a'{a}rnu \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:1\&gen:unsp\&num:pl\\
%sip\a'{a}rt \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:2\&gen:fm\&num:sg\\
%sip\a'{a}rta \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:2\&gen:ms\&num:sg\\
%sip\a'{a}rti \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:1\&gen:unsp\&num:sg\\
%sip\a'{e}r \> pos:v\&root:spr\&ptn:piel\&tense:past\&pers:3\&gen:ms\&num:sg\\
%wilon\a'{o}t \> pos:n\&root:wln\&ptn:CiCoCa\&gen:fem\&num:pl\&stat:u\\
%wil\a'{o}n \> pos:n\&root:wyl\&ptn:CiCon\&gen:ms\&num:sg\&stat:u\\
%tmun\a'{a} \> pos:n\&root:mnh\&ptn:CCuCa\&gen:fm\&num:sg\&stat:u\\
%tmun\a'{o}t \> pos:n\&root:mnh\&ptn:CCuCa\&gen:fm\&num:pl\&stat:u\\
\end{tabbing}
\caption{BLC morphological analyses for nouns}
\end{mdframed}
\end{figure}
   
   \begin{table}[t]
      \small
      \centering
       \setlength{\extrarowheight}{6pt}
      \begin{tabular}{ccc} 
 \toprule
 %&  \multicolumn{2}{|c|}{masc. noun} & \multicolumn{2}{|c|}{fem. noun}
 &  \textsc{sg} & \textsc{pl}\\
\cmidrule{2-3} 
\textsc{m} & pas & pas\'{i}m \\
\textsc{f}  &	qvuc\'{a}	& qvuc\'ot \\	
    \bottomrule
    \end{tabular}
    \label{tab:abs-only}
    \caption{Fusional noun endings.}
    \end{table}
    
 The same is true 
of the feminine plural suffix \textit{-ot}. (See section~\ref{sec:heb-example} for the full argument.)
We thus modify these atomic categories to make them match 
Hebrew's fusional suffixes. That is, we merge \texttt{masculine} 
and \texttt{plural} into the single category \texttt{M\%Pl}. Likewise, 
\texttt{feminine} and \texttt{plural} become \texttt{F\%Pl}, and \texttt{feminine} 
and \texttt{singular} become \texttt{F\%Sg}.
We discard the part-of-speech category \emph{noun} because of vagueness; it corresponds to many different morphological exponents, but no \emph{particular} exponent.

\item[Construct State.]
The forms in table~\ref{tab:abs-only} are \emph{absolute} or \emph{free} forms. 
Nouns can also take \emph{construct} forms, i.e., the form used to create compound 
nouns and express genitive relationships. The construct state is characterized by 
certain changes in form with respect to the absolute forms. 
Examples~(\ref{ex:free-state}) and (\ref{ex:construct-phrase})
illustrate these changes.
In example~(\ref{ex:free-state}) , the masculine plural form \textit{pas-\'{i}m} is an independent noun phrase 
and thus takes the absolute state. In example~(\ref{ex:construct-phrase}), however, it is the head 
of a \emph{construct phrase} and thus takes the construct form \emph{pas-\'{e}y}. 
All of the four nominal inflectional categories are subject to primary-stress loss and consequent 
vowel deletion, but only two of the four, namely the masculine plural and feminine singular, 
take distinctly construct suffixes, as shown in table~\ref{tab:cstr-endings}. 

\begin{exe}
%\label{ex:abs-v-cstr}
\ex \textit{pas\'{i}m \quad weharaq\'{e}vet} \\ \label{ex:free-state}
pas-\'{i}m \quad we-ha-raq\'{e}vet \\
stripe-\textsc{pl} \quad and-the-train \\
`stripes and the train'
\ex \textit{pas\'{e}y \quad ha-raq\'{e}vet} \\ \label{ex:construct-phrase}
pas-\'{e}y \quad ha-raq\'{e}vet \\
stripe-\textsc{cstr.pl} \quad \textsc{def}-train \\
`the train tracks'
\end{exe}

   \begin{table}[t]
      \centering
      \setlength{\extrarowheight}{8pt}
         \small
      \begin{tabular}{ccccc} 
 \toprule
 &  \multicolumn{2}{c}{masculine} & \multicolumn{2}{c}{feminine} \\
 &  \textsc{sg} & \textsc{pl} & \textsc{sg} & \textsc{pl} \\
 \cmidrule{2-3} \cmidrule{4-5}
   \textsc{absolute} & pas & pas\'{i}m  & qvuc\'{a}	& qvuc\'{o}t \\
     \textsc{construct} & pas & pas\'{e}y & qvuc\'{a}t	& qvuc\'{o}t\\
    \bottomrule
    \end{tabular}
          \label{tab:cstr-endings}
     \caption{Masculine and feminine construct forms.}
    \end{table}
    
%The formal difference between construct and absolute states is not always apparent 
In the other two categories, the masculine singular and feminine plural, the construct and absolute states are not always morphologically distinguished. Because these categories lack distinct construct suffixes, any formal distinction must be provided by vowel reduction/deletion. However, in Modern Hebrew, vowels are not uniformly subject to reduction and deletion, a consequence of the persistent influence of Biblical Hebrew prosody \citep{morag:1959, bolozky:1997, ravid-and-shlesinger:2001}. 
For instance, some vowels are protected from deletion because their 
Biblical Hebrew antecedents were intrinsically (and thus unchangeably) long. 
Other vowels are protected if they occur in a closed syllable.\footnote{Sometimes 
a vowel in an open syllable is protected because the syllable \emph{would have been} 
closed in Biblical Hebrew, which recognized geminate consonants as a sequence of two 
moras. For instance, the word \textit{tabbax} `cook, chef', is pronounced [tabax] in Modern Hebrew---without the geminate /b/. Nevertheless, historical geminate \textit{b} protects the initial /a/ from deletion in the construct phrase [tabax  mi\v{s}k\'an] `resident chef' \citep[see][p. 43]{bolozky-and-schwarzwald:1990}.}

Thus, in the transcriptional data (both with and without stress marking), there 
is sometimes no formal construct marker (or exponent) in the masculine singular 
and feminine plural categories. 
Moreover, in the orthographic data, there is \emph{never} a distinction between 
the absolute and construct states in these 
two categories, since the vowels that are subject to deletion are not represented in the 
orthographic data in the first place.
%difference  since the orthographic data does not the represent vowels that are subject to deletion.
Therefore, when the BLC category \textit{stat:bound}  is associated with a masculine singular or feminine plural nominal, we discard it, as in examples~\ref{ex:cstr:m-sg} and \ref{ex:cstr:f-pl}. Otherwise, we fuse it to the \textsc{gender}\%\textsc{number} categories as \textit{cstr}, as in examples~\ref{ex:cstr:m-pl} and \ref{ex:cstr:f-sg}.
%\begin{exe}
%\ex \begin{xlist}
%		\ex gen:ms\&num:sg \quad $\to$ \quad m\%sg
%		\ex gen:ms\&num:sg\&stat:bound \quad $\to$ \quad \%m\%sg
%	\end{xlist}
%\ex \begin{xlist}
%		\ex gen:ms\&num:pl \quad $\to$ \quad m\%pl 
%		\ex gen:ms\&num:pl\&stat:bound \quad $\to$ \quad cstr\%m\%pl
%	\end{xlist}
%\ex \begin{xlist}
%		\ex gen:fm\&num:sg \quad $\to$ \quad f\%sg
%		\ex gen:fm\&num:sg\&stat:bound \quad $\to$ \quad cstr\%f\%sg
%	\end{xlist}
%\ex \begin{xlist}
%	\ex gen:fm\&num:pl \quad $\to$ \quad f\%pl
%	\ex gen:fm\&num:pl\&stat:bound \quad $\to$ \quad \%f\%pl
%\end{xlist}
%\end{exe}
\begin{exe}
\ex \label{ex:cstr-cats} 
\begin{xlist}
		\ex gen:ms\&num:sg \&stat:bound \quad $\to$ \quad \texttt{m\%sg} \label{ex:cstr:m-sg}
		\ex gen:ms\&num:pl\&stat:bound \quad $\to$ \quad \texttt{cstr\%m\%sg} \label{ex:cstr:m-pl}
		\ex gen:fm\&num:sg\&stat:bound  \quad $\to$ \quad \texttt{f\%pl} \label{ex:cstr:f-sg}
		\ex gen:fm\&num:pl\&stat:bound \quad $\to$ \quad \texttt{cstr\%f\%pl} \label{ex:cstr:f-pl}
	\end{xlist}
\ex \label{ex:adj-cats} 
\begin{xlist}
		\ex pos:adj\&gen:ms\&num:sg \quad $\to$ \quad \texttt{adj\&m\%sg} \label{ex:adj:m-sg}
		\ex pos:adj\&gen:ms\&num:pl \quad $\to$ \quad \texttt{adj\&m\%pl} \label{ex:adj:m-pl}
		\ex pos:adj\&gen:fm\&num:sg \quad $\to$ \quad \texttt{adj\&f\%sg} \label{ex:adj:f-sg}
		\ex pos:adj\&gen:fm\&num:pl \quad $\to$ \quad \texttt{adj\&f\%pl} \label{ex:adj:f-pl}
	\end{xlist}
\end{exe}

%\ex \begin{xlist}
%		\ex gen:ms\&num:pl \quad $\to$ \quad m\%pl 
%		\ex gen:ms\&num:pl\&stat:bound \quad $\to$ \quad cstr\%m\%pl
%	\end{xlist}
%\ex \begin{xlist}
%		\ex gen:fm\&num:sg \quad $\to$ \quad f\%sg
%		\ex gen:fm\&num:sg\&stat:bound \quad $\to$ \quad cstr\%f\%sg
%	\end{xlist}
%\ex \begin{xlist}
%	\ex gen:fm\&num:pl \quad $\to$ \quad f\%pl
%	\ex gen:fm\&num:pl\&stat:bound \quad $\to$ \quad \%f\%pl
%\end{xlist}
%\end{exe}
%distinguished between long, short, and reduced vowels. 
%Some vowels could undergo both lengthening and reduction; in open syllables, they were either lengthened or reduced depending on the stress pattern of the larger word, whereas in closed syllables, they were neither reduced nor lengthened. Other vowels, however, namely *\textipa{/i:/}, *\textipa{/o:/} and *\textipa{/u:/}, were intrinsically long, and this intrinsic length always protected them from reduction regardless of context. Modern Hebrew has inherited the stress patterns of Biblical Hebrew, even though Modern Hebrew has only one vowel length. Thus the Modern Hebrew \textipa{/o/} and \textipa{/u/} have inherited the unchangeability of the Biblical Hebrew  *\textipa{/o:/} and *\textipa{/u:/}, even though the Modern Hebrew vowels are not intrinsically long. Likes inherited the reduction-resistance 

% only two of the four absolute categories in table~\ref{}, 
%namely the masculine plural and feminine singular construct forms, actually take different suffixes than their absolute counterparts.

%in the construct state. This is illustrated in table taglekind of compound-noun construction.  the construct counterpart of contrasts A special form called the construct state is used to mark nouns as heads of construct phrases,  example~\ref{ex:construct-phrase}, the construct form \textit{pas-\'{e}y} `stripes (of)' is . It is head of the counterpart of the absolute \textit{pas-\'{i}m}. These are the canonical forms. When a nominal acts as the head of a construct phrase, it appears in the construn take when There is also a set of \emph{construct} forms; these serve to mark the head of a \emph{construct phrase}, as in example~\ref{ex:free-state} The absolute state is the canonical state for Hebrew nouns. s also a \emph{construct} state, which is characterized by destr the construct state canonical sHebrew nouns also appear in the \emph{construct} state.
%The forms of Hebrew nouns can change to indicate head status in a compound noun. The set of forms that mark headedness in compound nouns when they are placed   also depend on whether they are in the \emph{absolute state} or the \emph{construct state}, as illustrated in table~\ref{tab:cstr-endings}. The absolute state is the canonical state, the state of a noun in isolation, whereas the construct state is the state of dependent nouns in compound nouns. An example is \textit{pas\'{e}y in the compound noun raq\'{e}vet} `stripes-of train' or `train tracks'. The suffix \textit{-\'{e}y} is construct ending for masculine plural nouns.
%, namely \textit{-\'{e}y}. 

%\begin{exe}
%\ex \label{ex:cstr:pasey}
%	\textsf{pas\'{e}y+ha+rak\textipa{\'{e}}vet} \\
%	\textsf{n:det|+n|pas\&gen:ms\&num:pl\&stat:comp-\'{e}y+det|ha+n|rak\textipa{\'{e}}vet}
%\ex \label{ex:cstr:shaat} 
%	\textsf{\v{s}\textglotstop{at}+sip\'{u}r .} \\
%	\textsf{n|+n|\v{s}a\textipa{Q}\'a\&gen:fm\&num:sg\&stat:comp-\'at+n|sip\'ur .}
%\ex \label{ex:cstr:bdiqat} 
%	\textsf{bdiq\'{a}t+\textipa{P}ozn\'{a}yim} \\ 
%	\textsf{n|bdiq\'{a}\&root:tbd\&ptn:tbd\&gen:fm\&num:sg\&stat:bound-\'{a}t}
%\end{exe}
%As shown in table~\ref{tab:cstr-endings}, the suffixes \textit{-\'{e}y} and \textit{-\'{a}t} are the construct-state counterparts to the absolute suffixes \textit{-\'{i}m} and \textit{-\'{a}}, respectively. All of the four nominal inflectional categories are subject to destressing and vowel reduction/deletion processes in the construct state. The construct masculine singular has no suffix. It does undergo destressing and vowel reduction/deletion processes, as do the other construct categories, but this destressing is not always apparent in the BLC's transcriptions. Moreover, some vowels are resistant to reduction for historical reasons.
%%, for historical reasons, are intrinsically resistant to reduction. 
%For instance, the Modern Hebrew vowels \textit{u} and\textit{o} because of their historical\emph{unchangeably long} status. \footnote{Brief explanation} %because they were once diphthongs, and the vowel \textit{a} in 
%%However, the  Thewith another noun to form a compound noun. When isolated, nouns are in the \emph{absolute} state. For example, \dots
%%\begin{exe}
%%noun(construct)  noun(absolute)
%%\end{exe}
%%\dots Table~\ref{tab:cstr-endings} shows the absolute and construct forms for both masculine noun and a feminine %noun. 
%However, the other two inflectional categories, namely the masculine singular and the feminine plural, have identical absolute and construct forms. Or, to put it another way, the construct state has no identifiable exponent in these categories

%\textipa{Q}alul\a'{a} \> pos:adj\&root:\textipa{Q}ll\&ptn:CaCuC\&gen:fm\&num:sg\&src:deverb\\
%\textipa{Q}alul\a'{i}m \> pos:adj\&root:\textipa{Q}ll\&ptn:CaCuC\&gen:ms\&num:pl\&src:deverb\\
\item[Adjectives.]
As discussed in chapter~\ref{autonomous} (section~\ref{sec:heb-example} in particular), 
a large number of Modern Hebrew adjectives are derived by attaching the 
suffix -\textit{i}
to a noun. Thus, unlike nouns, many adjectives have a formal exponent, i.e., a particular aspect of from that distinctly corresponds to the category \emph{adjective}---at least the adjectives derived via -\textit{i} do, anyway. 
%---has a formal 
%exponent in the suffix \texit{-i}.
We therefore retain\emph{adjective} 
\texttt{pos:adj} (i.e., \textit{part-of-speech: adjective}) as a gold-standard category.
Recall that we discarded the \emph{noun} category because it lacked particular formal exponent. 
This universal treatment of adjectives, however, is imperfect, since many adjectives, such as
\textit{nexm\'{a}d} `pleasant' (see table~\ref{tab:non-y-adjs}),
lack a distinctive adjectival marker.  This is one of many ways in which the category mapping described in this section is an imperfect solution to the problem of obtaining suitable gold-standard categories.  
% he masculine-singular category is generally unmarked. However, masculine-singular adjectives derived via the \textit{-i} suffix, corresponds to masculine-singular infectional category; the other
%three categories bear the \textit{-i} suffix along with an additional ending. 
 
\begin{figure}[t]
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
nexmad\a'{a} \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:fm\&num:sg\&src:deverb\\
nexmad\a'{i}m \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:ms\&num:pl\&src:deverb\\
nexmad\a'{o}t \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:fm\&num:pl\&src:deverb\\
nexm\a'{a}d \> pos:adj\&root:xmd\&ptn:niCCaC\&gen:ms\&num:sg\&src:deverb\\
\end{tabbing}
\label{fig:analyses}
\caption{Morphological analyses extracted from the Berman Longitudinal Corpus. These entries are the inflections
of the adjective \textit{nexmad} `pleasant.'}
\end{mdframed}
\end{figure}

   \begin{table}[t]
      \small
      \centering
       \setlength{\extrarowheight}{8pt}
      \begin{tabular}{ccc} 
 \toprule
 & \textsc{sg} & \textsc{pl} \\
\cmidrule{2-3}
    \textsc{m} & nexmad & nexmad-\'{i}m \\
   \textsc{f} &  nexmad-\'{i} & nexmad-\'{o}t \\
    \bottomrule
    \end{tabular}
    \label{tab:non-y-adjs}
    \caption{Inflections of the adjective \textit{nexmad} `pleasant'}
    \end{table}

%\begin{table}[ht]
%\begin{center}
%\subtable[maqomi `local' \label{subtab:fusion:1}]{
%\setlength{\extrarowheight}{8pt}
%{\begin{tabular}{lcc}
%\  & masc & fem  \\
%\hline 
%sg & maqom-i & meqom-i-t  \\
%pl & meqom-iy-im & meqom-iy-ot  \\
%\end{tabular}}
%}
%\subtable[gadol `big' \label{subtab:fusion:2}]{
%\setlength{\extrarowheight}{8pt}
%{\begin{tabular}{lcc}
%\  & masc & fem  \\
%\hline 
%sg & gadol & gdol-a  \\
%pl & gdol-im & gdol-ot  \\
%\end{tabular}}
%}
%\label{tab:fusion}
%\caption{Fusional suffixes in Hebrew nominals}
%\end{center}
%\end{table}

\begin{table}[bt]
\centering
\small
\setlength{\extrarowheight}{8pt}
\begin{tabular}{lcc}
\toprule
& \textsc{sg} & \textsc{pl}  \\
\cmidrule(lr){2-3}
\textsc{m} & maqom-\'{i} &  meqom-iy-\'{i}m\\
\textsc{f} & meqom-\'{i}-t  & meqom-iy-\'{o}t  \\
\bottomrule
\end{tabular}
\label{tab:der-adjs}
\caption{Inflections of the adjective \textit{maqom\'{i}} `local,' derived from the noun for \textit{maqom} `place' via the suffix \textit{-i}/\textit{-y}}
\end{table}

%\texttt{cstr\%f\%mass}
%\texttt{cstr\%f\%sg}
%\texttt{cstr\%m\%mass}
%\texttt{cstr\%m\%pl}
%\texttt{cstr\%m\%sg}
%\textit{state} \% \textit{gender} \% \textit{number}
%\textit{adj}\%\textit{m}\%\textit{sg}

%\begin{exe}
%\ex \label{ex:cstr:pasey}
%	\textsf{pas\'{e}y+ha+rak\textipa{\'{e}}vet} \\
%	\textsf{n:det|+n|pas\&gen:ms\&num:pl\&stat:comp-\'{e}y+det|ha+n|rak\textipa{\'{e}}vet}
%\ex \label{ex:cstr:shaat} 
%	\textsf{\v{s}\textglotstop{at}+sip\'{u}r .} \\
%	\textsf{n|+n|\v{s}a\textipa{Q}\'a\&gen:fm\&num:sg\&stat:comp-\'at+n|sip\'ur .}
%\ex \label{ex:cstr:bdiqat} 
%	\textsf{bdiq\'{a}t+\textipa{P}ozn\'{a}yim} \\ 
%	\textsf{n|bdiq\'{a}\&root:tbd\&ptn:tbd\&gen:fm\&num:sg\&stat:bound-\'{a}t}
%\end{exe}

%Therefore, when the BLC category \textit{stat:bound} is associated with a masculine singular or feminine plural nominal, we discard it, as in examples~{ex:cstr:m-sg} and {ex:cstr:f-pl}. Otherwise, we fuse it to the \textsc{gender}\%\textsc{number} categories as \textit{cstr}, as in examples~{ex:cstr:m-pl} and {ex:cstr:f-sg}.

\item[Participles.]
As in English, participles in Hebrew can function as nouns and adjectives.
All binyanim except the \textit{Qal} and \textit{Nif`al} use 
the prefix stem to form the participle. The \textit{Nif`al} uses the prefix stem, 
and the \textit{Qal} uses a different set of stems altogether to form the 
participle.\footnote{The participle functions as the present-tense verb form in modern Hebrew, but it inflects as a nominal category.}
\textit{Qal} and \textit{Nif`al} participles, we add the categories \texttt{qal:participle} 
and \texttt{nifal:suffix\_stem}.  In general, the BLC category \texttt{pos:part} 
is combined with the fusional gender and number endings to categories like \texttt{part\%m\%sg}.

%def analysisFilter(analyses_str):
%	new_analyses = list()
%	analyses=analyses_str.split()
%	if "pos:part" in analyses_str:
%		####print "**",analyses_str
%		for analysis in analyses:
%			if "pos:v" in analysis:
%				continue
%			elif "pos:adj" in analysis:
%				continue
%			elif "pos:n" in analysis:
%				continue
%			else: new_analyses.append(analysis)
%		return new_analyses
%	else:
%		return analyses
%	elif pos == "part":
%		#pat = ur"(ptn:)([a-zC]+)(&tense:)(past)(&pers:3&gen:f&num:sg)(&|$)"
%		#pat = ur"(pos:)(part)(&[^\s]*)(&ptn:)([a-zC]*)(&[^\s]*)(&gen:)([mfu])(&num:)([sgplundoma]+)(&|$)"
%		#pat = ur"(pos:)(part)(&root:[^\s]+&ptn:)([a-zC]*)(&[^\s]*)(gen:)([mascfemunsp]+)(&num:)([sgplundoma]+)(&|$)"
%		pat = ur"(pos:)(part)([^\s]*&root:[^\s]+)(&ptn:)([a-zC]*)(&[^\s]*)(gen:)([mascfemunsp]+)(&num:)([sgplundoma]+)(&|$)"
%		re_part = re.compile(pat, re.UNICODE)
%		#analysis = re_part.sub(ur"\5%prefix_stem&\8%\10\11\6", analysis)
%		if ptn == "qal":
%			#1 (pos:)
%			#2 (part)
%			#3 ([^\s]*&ptn:)
%			#4 ([a-zC]*) the actual pattern
%			#5 (&[^\s]*)
%			#6 (gen:) 
%			#7 ([mascfemunsp]+)  # gender val
%			#8 (&num:)
%			#9 ([sgplundoma]+)    # number val
%			#10(&|$)
%			analysis = re_part.sub(ur"\3&\5%participle&\8%\10\6", analysis)
%			# stemFeature = binyan + "%participle"
%			# processedFeatures.append(stemFeature)
%		elif ptn == "nifal":
%			analysis = re_part.sub(ur"\3&\5%suffix_stem&\8%\10\6", analysis)
%			# stemFeature = binyan + "%suffix_stem"
%			# processedFeatures.append(stemFeature)
%		else:
%			analysis = re_part.sub(ur"\3&\5%prefix_stem&\8%\10\6", analysis)
			
			
%\item[Pronominals.] 
%Hebrew pronominals come in two broad types: free-standing pronouns on the one hand, i.e., pronouns that occur as independent words, and pronominal suffixes on the other. The former type includes the personal and demonstrative pronouns enumerated in 
%tables~\ref{tab:pers-pronouns} and \ref{tab:dem-pronouns}, respectively. 

%\begin{itemize}
%\item \textit{Free-standing pronouns} 
\item[Free-standing pronouns] include the personal and demonstrative pronouns presented in 
tables~\ref{tab:pers-pronouns} and \ref{tab:dem-pronouns}, respectively.
Like verbs, personal pronouns have person, number, and gender. 
Personal pronouns have person, gender, and number, each of which is treated as 
an atomic morphosyntactic feature in the BLC analyses, as illustrated in 
figure~\ref{fig:pers-pronouns}. For each personal-pronoun morphological analysis, 
we map the person, gender, and number categories onto
two gold-standard categories; one combining person and gender 
into a single class, e.g., \texttt{pro:pers\%2\%m} (2nd-person $\land$ masculine), 
and another combining person, gender, and number, 
e.g., \texttt{pro:pers\%2\%m\%sg} (2nd-person $\land$ masculine $\land$ singular). 
The former type of gold-standard class
thus corresponds to a particular row in table~\ref{tab:pers-pronouns}, 
while the latter corresponds to a particular \emph{cell} in this table.

%person and gender categories, e.g., $\textsc{person:2} \land \textsc{gender:m}$, 
%and the other to person-gender-and-number triples, i.e., to single cells in table~\ref{tab:pers:pronoun}. 
%Thus, a BLC analysis such as \texttt{pro:person\&pers:2\&gen:fm\&num:sg} would be translated 
%to the two gold-standard categories 
%\texttt{pro:pers\%2\%m} and \texttt{pro:pers\%2\%m\%sg}.

\begin{table}[tb]\label{tab:pers-pronouns}
\centering
%[Personal pronouns\label{tab:pers-pronouns}]{
\setlength{\extrarowheight}{6pt}
\begin{tabular}{lcc}
\toprule
      & \textsc{sg} & \textsc{pl} \\
     \midrule
    \textsc{1.m/f} & \textipa{P}an\'{i} & $\{$ \textipa{P}an\'{a}xnu, \textipa{P}\'{a}nu $\}$ \\
    \midrule
 \textsc{2.m} &  \textipa{P}at\'{a}  &  \textipa{P}at\'{e}m\\
   %\textit{2.m}  & fem & \textipa{P}at  &  \textipa{P}at\'{e}n\\
  \textsc{2.f} & \textipa{P}at  &  \textipa{P}at\'{e}n\\
   \midrule
 \textsc{3.m} & hu\textipa{P} & hem\\    
   %\raisebox{1.5ex}[0pt]{3} & fem  & hi\textipa{P} &  hen \\
   \textsc{3.f} & hi\textipa{P} &  hen \\
    \bottomrule
\end{tabular}
\caption{Personal pronouns}
\end{table}

\begin{table}[tb]  \label{tab:dem-pronouns}
\setlength{\extrarowheight}{6pt}
\centering
\begin{tabular}{lcc}
\toprule
 & \textsc{sg} & \textsc{pl} \\
  \cmidrule{2-3}   
 \textsc{m} & ze & {\textglotstop}\'{e}le \\ 
   %\raisebox{1.5ex}[0pt]{3} & fem  & hi\textipa{P} &  hen \\
\textsc{f} & \{ zo, zo\textipa{P}t \}  &  {\textglotstop}\'{e}lu \\
   \bottomrule
  \end{tabular}
\caption{Demonstrative pronouns}
  \end{table}
  
%  \begin{exe}
%\ex \label{ex:pro-pers-2f}\begin{xlist}
%		\ex \texttt{pro:person\&pers:2\&gen:fm\&num:sg} \quad $\to$ \quad \texttt{pro:person\%2\%f\&pro:person\%2\%f\%sg}
%		\ex \texttt{pro:person\&pers:2\&gen:fm\&num:pl} \quad $\to$ \quad \texttt{pro:person\%2\%f\&pro:person\%2\%f\%pl}
%	\end{xlist}
%\ex \label{ex:pro-pers-2m} \begin{xlist}
%		\ex \texttt{pro:person\&pers:2\&gen:ms\&num:sg} \quad $\to$ \\ $\qquad \qquad$ \texttt{pro:person\%2\%m\&pro:person\%2\%m\%sg}
%		\ex \texttt{pro:person\&pers:2\&gen:ms\&num:pl} \quad $\to$ \\ {} $\qquad \qquad$ \texttt{pro:person\%2\%m\&pro:person\%2\%m\%pl}
%	\end{xlist}
%\end{exe}

  \begin{figure}[t]\label{fig:pers-pronouns}
  \begin{mdframed}
 \begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
\v{s}ehem \> pre:sh\&pre:sh\&pos:pro:person\&pers:3\&gen:ms\&num:pl \\
\v{s}ehen \> pre:sh\&pre:sh\&pos:pro\_person\&pers:3\&gen:fm\&num:pl\\
\v{s}e\textipa{P}an\a'{i} \> pre:sh\&pos:pro:person\&pers:1\&num:sg \\
\v{s}e\textipa{P}at\a'{a} \> pre:sh\&pre:sh\&pos:pro\_person\&pers:2\&gen:ms\&num:sg\\
\v{s}e\textipa{P}at\a'{e}m \> pre:sh\&pre:sh\&pos:pro\_person\&pers:2\&gen:ms\&num:pl\\
\textipa{P}an\a'{a}xnu \> pos:pro:person\&pers:1\&num:pl\\
\textipa{P}an\a'{i} \> pos:pro:person\&pers:1\&num:sg\\
\textipa{P}at\a'{e}m \> pos:pro:person\&pers:2\&gen:ms\&num:pl\\
\textipa{P}at\a'{e}n \> pos:pro:person\&pers:2\&gen:fm\&num:pl\\
hu\textipa{P} \> pos:pro:person\&pers:3\&gen:ms\&num:sg \\
hi\textipa{P} \> pos:pro:person\&pers:3\&gen:fm\&num:sg \\
\end{tabbing}
\caption{Personal-pronoun analyses extracted from the BLC}
\end{mdframed}
\end{figure}


\begin{table}[ht]
\centering
\subtable[Pro. Suffixes for Sg. Nouns\label{subtab:pron-suf-sg}]{
 \setlength{\extrarowheight}{6pt}
\begin{tabular}{lcc}
\toprule
      & \textsc{sg} & \textsc{pl} \\
     \midrule
    \textsc{1.m/f} &  -\'{i} &  -\'{a}nu  \\
    \midrule
 \textsc{2.m} &  -\textsubdot{k}\'{a}  &  -\textsubdot{k}\'{e}m\\
   %\textit{2.m}  & fem & \textipa{P}at  &  \textipa{P}at\'{e}n\\
  \textsc{2.f} & -\'{e}y\textsubdot{k}  & -\textsubdot{k}\'{e}n\\
   \midrule
 \textsc{3.m} & -\'{o} & -\'{a}m\\    
   %\raisebox{1.5ex}[0pt]{3} & fem  & hi\textipa{P} &  hen \\
   \textsc{3.f} & -\'{a}h & -\'{a}n \\
    \bottomrule
\end{tabular}
}
\subtable[Pro. Suffixes for Pl. Nouns\label{subtab:pron-suf-pl}]{
\setlength{\extrarowheight}{6pt}
\begin{tabular}{lcc}
\toprule
      & \textsc{sg} & \textsc{pl} \\
     \midrule
    \textsc{1.m/f} &  -\'{a}y &  -\'{e}ynu  \\
    \midrule
 \textsc{2.m} &  -\'{e}y\textsubdot{k}a  &  -ey\textsubdot{k}\'{e}m\\
   %\textit{2.m}  & fem & \textipa{P}at  &  \textipa{P}at\'{e}n\\
  \textsc{2.f} & -\'{a}yi\textsubdot{k}   & -ey\textsubdot{k}\'{e}n\\
   \midrule
 \textsc{3.m} & -\'{a}yw & -eyh\'{e}m\\    
   %\raisebox{1.5ex}[0pt]{3} & fem  & hi\textipa{P} &  hen \\
   \textsc{3.f} & -\'{e}yha & -eyh\'{e}n \\
    \bottomrule
\end{tabular}
}
\label{tab:pers-pronouns}
\caption{Pronominal Suffixes}
\end{table}

%acc:pro|\textipa{P}et\&pers:3\&gen:fm\&num:sg-\'{a}h

%\fontfamily{cmtt}\selectfont mi\textipa{\.*{k}}nas\'{a}yw} n mi\textipa{\.*{k}}n\'{a}s\&gen:ms\&num:pl\&poss:3mascSG-\'{a}yw=trouser 
%\textsubdot
\begin{exe}
\ex \label{ex:prosuf-3ms}
{\fontfamily{cmtt}\selectfont \small
yad\'{o} \\
n|yad\&gen:fm\&num:sg\&poss:3mascSG-\'{o}=hand
}
\ex \label{ex:prosuf-3mpl}
{\fontfamily{cmvtt}\selectfont \small
yad\'{a}yw \\
n|yad\&gen:fm\&num:sg\,\&\,poss:3mascSG-\'{a}yw=hand
}
%\ex 
%{\fontfamily{cmtt}\selectfont 
%mi{k}nas\'{a}yw \\
%n|mi\textipa{\.*{k}}n\'{a}s\&gen:ms\&num:pl\&poss:3mascSG-\'{a}yw=trouser 
%}
\ex \label{ex:prep-pro-to}
{\fontfamily{cmtt}\selectfont \small
li \qquad \textrm{(i.e., \textit{le-} + \textit{-i})} \\
prep:pro|le\&pers:1\&gen:unsp\&num:sg-i=to
}
\ex \label{ex:prep-pro-on}
{\fontfamily{cmtt}\selectfont \small
\textipa{Q}al\'{a}yw  \\
%\quad prep:pro|\textipa{Q}al\&poss:3mascSG\\}
%prep:pro|\textipa{Q}al&pers:3&gen:ms&num:sg-\CID{325}yw=on
prep:pro|\textipa{Q}al\&pers:3\&gen:ms\&num:sg-\'{a}yw=on
}
\end{exe}

%\textipa{Q}al\a'{a}yw \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:3\&gen:ms\&num:sg pos:n\&gen:ms\&num:pl\&poss:3mascSG\\
%\textipa{Q}al\a'{e} \> pos:n\&gen:ms\&num:sg\&stat:u\\
%\textipa{Q}al\a'{e}yha \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:3\&gen:fm\&num:sg pos:n\&gen:ms\&num:pl\&poss:3femSG\\
%\textipa{Q}al\a'{e}ynu \> pos:n\&gen:ms\&num:pl\&poss:1PL\\
%\textipa{Q}al\a'{e}y\textsubdot{k}a \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:2\&gen:ms\&num:sg pos:n\&gen:ms\&num:pl\&poss:2mascSG\\
%\begin{exe}
%\ex {\fontfamily{qcr}\selectfont 
%\textglotstop mi\textsubdot{k}nas\'{a}yw \\
%n\|mi\textipa{\.*{k}}n\'{a}s\&gen:ms\&num:pl\&poss:3mascSG-\'{a}yw=trouser }
%\end{exe}
%ze
%pro:dem|ze\&pers:3\&gen:ms\&num:sg=it/this
%\textglotstop \'{e}le
%\textglotstop \'{e}lu
%pro:dem | ha\textglotstop \'{e}le&pers:3&gen:ms&num:pl=these ?
%pro:dem | \textglotstop \'{e}lu\&pers:3\&num:pl=these/those ?  
%	prep:pro\%1             
%	prep:pro\%1\%pl          
%	prep:pro\%1\%sg          
%	prep:pro\%2\%F\%Sg        
%	prep:pro\%2\%M\%Pl        
%	prep:pro\%2\%M\%Sg        
%	prep:pro\%2\%f           
%	prep:pro\%2\%m           
%	prep:pro\%3\%F\%Pl        
%	prep:pro\%3\%F\%Sg        
%	prep:pro\%3\%M\%Pl        
%	prep:pro\%3\%M\%Sg        
%	prep:pro\%3\%f           
%	prep:pro\%3\%m           
%	pro:indef              
%	pro\_dem                
%	pro\_dem\%3\%M\%Sg         
%	pro\_dem\%3\%m            
%	pro\_dist               
%	pro\_indef              
%	pro\_person             
%	pro\_person\%1           
%	pro\_person\%1\%M\%Pl      
%	pro\_person\%1\%m         
%	pro\_person\%1\%pl        
%	pro\_person\%2\%F\%Pl      
%	pro\_person\%2\%F\%Sg      
%	pro\_person\%2\%M\%Pl      
%	pro\_person\%2\%M\%Sg      
%	pro\_person\%2\%f         
%	pro\_person\%2\%m         
%	pro\_person\%3\%F\%Pl      
%	pro\_person\%3\%F\%Sg      
%	pro\_person\%3\%M\%Pl      
%	pro\_person\%3\%M\%Sg      
%	pro\_person\%3\%f         
%	pro\_person\%3\%m         
%	pro\_suf:1PL            
%	pro\_suf:1SG            
%	pro\_suf:2femSG         
%	pro\_suf:2mascPL        
%	pro\_suf:2mascSG        
%	pro\_suf:3femPL         
%	pro\_suf:3femSG         
%	pro\_suf:3mascPL        
%	pro\_suf:3mascSG        
%	pro\_suf\_state 
%prep:pro\%1\\
%prep:pro\%1\%pl\\
%prep:pro\%1\%sg\\
%prep:pro\%2\%F\%Sg\\
%prep:pro\%2\%M\%Pl\\
%prep:pro\%2\%M\%Sg\\
%prep:pro\%2\%f\\
%prep:pro\%2\%m\\
%prep:pro\%3\%F\%Pl\\
%prep:pro\%3\%F\%Sg\\
%prep:pro\%3\%M\%Pl\\
%prep:pro\%3\%M\%Sg\\
%prep:pro\%3\%f\\
%prep:pro\%3\%m\\
%pro:indef\\
%pro\_dem \\
%pro\_dem\%3\%M\%Sg\\
%pro\_dem\%3\%m\\
%pro\_dist\\
%pro\_indef\\
%pro\_person\\
%pro\_person\%1\\
%pro\_person\%1\%M\%Pl\\
%pro\_person\%1\%m\\
%pro\_person\%1\%pl\\
%pro\_person\%2\%F\%Pl\\
%pro\_person\%2\%F\%Sg\\
%pro\_person\%2\%M\%Pl\\
%pro\_person\%2\%M\%Sg\\
%pro\_person\%2\%f\\
%pro\_person\%2\%m\\
%pro\_person\%3\%F\%Pl\\
%pro\_person\%3\%F\%Sg\\
%pro\_person\%3\%M\%Pl\\
%pro\_person\%3\%M\%Sg\\
%pro\_person\%3\%f\\
%pro\_person\%3\%m\\
%pro\_suf:1PL\\
%pro\_suf:1SG\\
%pro\_suf:2femSG\\
%pro\_suf:2mascPL\\
%pro\_suf:2mascSG\\
%pro\_suf:3femPL\\
%pro\_suf:3femSG\\
%pro\_suf:3mascPL\\
%pro\_suf:3mascSG\\
%pro\_suf\_state\\
%
%\begin{figure}[t]
%\begin{mdframed}
%\begin{tabbing}
%\hspace{1in} \= \hspace{5.5in} \kill
%mi\textsubdot{k}nas\a'{a}yw \> pre:mi\&pos:n\&root:kns\&ptn:miCCaC\&gen:ms\&num:pl\&poss:3mascSG\\
%pan\a'{e}ynu \> pos:n\&root:pny\&ptn:CaCaC\&gen:ms\&num:pl\&poss:1PL\\
%ragl\a'{a}yi\textsubdot{k} \> pos:n\&root:rgl\&ptn:CeCeC\&gen:fm\&num:pl\&poss:2femSG\\
%sandal\a'{a}yi\textsubdot{k} \> pos:n\&gen:ms\&num:pl\&poss:2femSG\\
%sof\a'{o} \> pos:n\&root:syp\&ptn:CoC\&gen:ms\&num:sg\&poss:3mascSG\\
%sukat\a'{i} \> pos:n\&root:syk\&ptn:CuC\&gen:fm\&num:sg\&poss:1SG\\
%tor\a'{e}\textsubdot{k} \> pos:n\&root:tyr\&ptn:CoC\&gen:ms\&num:sg\&poss:2femSG\\
%wetaxton\a'{a}y \> pre:we\&pos:n\&gen:ms\&num:pl\&poss:1SG\\
%\textipa{P}a\v{s}mat\a'{e}nu \> pos:n\&root:\textipa{P}\v{s}m\&ptn:CaCC\&gen:fm\&num:sg\&poss:1PL\\
%\textipa{P}a\v{s}mat\a'{e}\textsubdot{k} \> pos:n\&root:\textipa{P}\v{s}m\&ptn:CaCC\&gen:fm\&num:sg\&poss:2femSG\\
%\end{tabbing}
%\caption{In the BLC analyses, pronominal suffixes are indicated via the tag \texttt{poss}. The person, gender, and number
%of the suffix follow this tag.}
%\label{fig:blc-poss}
%\end{mdframed}
%\end{figure}
%\begin{tabbing}
%\hspace{1in} \= \hspace{5.5in} \kill
%\v{s}ehem \> pre:sh\&pre:sh\&pos:pro\_person\&pers:3\&gen:ms\&num:pl pre:sh\&pos:pro\_person\&pers:3\&gen:ms\&num:pl\\
%\v{s}ehen \> pre:sh\&pre:sh\&pos:pro\_person\&pers:3\&gen:fm\&num:pl\\
%\v{s}e\textipa{P}an\a'{i} \> pre:sh\&pos:pro\_person\&pers:1\&num:sg pre:sh\&pos:pro\_person\&pers:1\&num:sg pre:sh\&pos:pro\_person\&pers:1\&num:sg pre:sh\&pre:sh\&pos:pro\_person\&pers:1\&num:sg\\
%\v{s}e\textipa{P}at\UTF{00E1} \> pre:sh\&root:\textipa{P}yt\&ptn:CaCa\&pre:sh\&pos:pro\_person\&pers:2\&gen:ms\&num:sg\\
%\v{s}e\textipa{P}at\UTF{00E9}m \> pre:sh\&root:\textipa{P}ty\&ptn:CaCy\&pre:sh\&pos:pro\_person\&pers:2\&gen:ms\&num:pl\\
%\textipa{P}an\UTF{00E1}xnu \> pos:pro\_person\&pers:1\&num:pl\\
%\textipa{P}an\a'{i} \> pos:pro\_person\&pers:1\&num:sg\\
%\textipa{P}at\UTF{00E9}m \> pos:pro\_person\&pers:2\&gen:ms\&num:pl\\
%\textipa{P}at\UTF{00E9}n \> pos:pro\_person\&pers:2\&gen:fm\&num:pl\\
%hu\textipa{P} \> pos:pro\_person\&pers:3\&gen:ms\&num:sg \\
%hi\textipa{P} \> pos:pro\_person\&pers:3\&gen:fm\&num:sg \\
%\end{tabbing}
%
%		pat = ur"(\t|&)(pos:)([^\s]*pro[^\s]*)(&[^\s]*pers:)([123]+)(&gen:)([fsemunsp]+)(&num:)([sgplduoun]+)(&|$)"
%		re_pro = re.compile(pat, re.UNICODE)
%		# 1. (\t|&)
%		# 2. (pos:)
%		# 3. ([^\s]*pro[^\s]*)
%		# 4. (&[^\s]*pers:)
%		# 5. ([123]+) ACTUAL PERSON
%		# 6. (&gen:)
%		# 7. ([fsemunsp]+) ACTUAL GENDER
%		# 8. (&num:)
%		# 9. ([sgplduoun]+) ACTUAL NUMBER
%		# 10. (&|$)
%		analysis = re_pro.sub(ur"\1\3\%\5\%\7\&\3\%\5\%\7\%\9\10", analysis)
%		*pro*\%[pers]\%[gen]  \&    *pro*\%[pers]\%[gen]\%[num]
	
%	pat = ur"(^.*)(pro:)([^&]+)(&|$)((?:&[^\s]*$)|$)"
%	re_protype = re.compile(pat, re.UNICODE)
%	if re_protype.search(analysis) != None and re_protype.search(analysis) != "":
%		pro\_type = re\_protype.sub(ur"\3", analysis)
%	analysis = analysis.replace("xxxxxxxxxx:", "pro\_suf\_state&pro\_suf:")
	
%\textipa{Q}al\a'{a}yw \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:3\&gen:ms\&num:sg pos:n\&gen:ms\&num:pl\&poss:3mascSG\\
%\textipa{Q}al\a'{e} \> pos:n\&gen:ms\&num:sg\&stat:u\\
%\textipa{Q}al\a'{e}yha \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:3\&gen:fm\&num:sg pos:n\&gen:ms\&num:pl\&poss:3femSG\\
%\textipa{Q}al\a'{e}ynu \> pos:n\&gen:ms\&num:pl\&poss:1PL\\
%\textipa{Q}al\a'{e}y\textsubdot{k}a \> root:\textipa{Q}yl\&ptn:CaC\&pos:prep:pro\&pers:2\&gen:ms\&num:sg pos:n\&gen:ms\&num:pl\&poss:2mascSG\\

%\begin{figure}
%\begin{tabbing}
%\hspace{1in} \= \hspace{5.5in} \kill
%\textipa{P}ot\a'{a}h \> pos:acc+pro\&pers:3\&gen:fm\&num:sg\\
%\textipa{P}ot\a'{a}m \> pos:acc+pro\&pers:3\&gen:ms\&num:pl\\
%\textipa{P}ot\a'{a}n \> pos:acc+pro\&pers:3\&gen:fm\&num:pl\\
%\textipa{P}ot\a'{a}nu \> pos:acc+pro\&pers:1\&gen:unsp\&num:pl\\
%\textipa{P}ot\a'{a}\textsubdot{k} \> pos:acc+pro\&pers:2\&gen:fm\&num:sg\\
%\textipa{P}ot\a'{i} \> pos:acc+pro\&pers:1\&gen:unsp\&num:sg\\
%\end{tabbing}
%\caption{The marker of definite-direct objects \textipa{P}et combines with pronominal suffixes
%to create (in effect) accusative pronouns}
%\label{fig:acc-examples}
%\end{figure}

%\begin{table}[tb]
%\centering
%\begin{tabular}{lll}
%	\toprule
%	BLC Category & & Gold-Std Category \\
%	\midrule
%	\texttt{poss:1SG} & $\to$ & \texttt{pos:prep:pro\%1\%sg}  \\
%	\bottomrule  
%\end {tabular}
%\end{table}
    
%\item \textit{Pronominal suffixes}
\item[Pronominal suffixes] can attach to both nouns and prepositions. When attached to nouns, they function as genitive---or possessive---pronouns, as in \textit{\textsubdot{t}ipot-\'{a}y} `my drops' (cf. \textsubdot{t}ip\'{o}t `drops').
In the latter case, they serve as the NP compliment (i.e., the object) of the preposition 
in question, as in, for instance \textit{\textipa{Q}al-\'{a}y} `on me' (cf. \textipa{Q}al `on').
There are two sets
of pronominal suffixes: one set for singular noun stems and another for plural noun stems. 
That is, the members of the first set, presented in table~\ref{subtab:pron-suf-sg}, 
attach to singular nouns, while the members of the second set, shown in table~\ref{subtab:pron-suf-pl} 
attach to plural nouns. 
Even though prepositions themselves have no number (nor person or gender). 
Nevertheless, some prepositions, e.g., the prefixal preposition \textit{le-}, 
take singular-stem pronominal suffixes, while others, e.g., \textit{\textipa{Q}al} `on,'
take plural-stem-attaching suffixes. Thus, in example~\ref{ex:prep-pro-to}, 
\textit{le-} takes 1st-person singular pronominal suffix for \emph{singular} stems, 
while \textit{\textipa{Q}al} in example~\ref{ex:prep-pro-on} takes the 1st-person 
for \emph{plural} nouns. In both of these examples, the person and number of 
the suffix is the same: 1st-person singular. (The gender common in the first person.)
The only functional difference between these two suffixes is that the former 
attaches to singular nouns (and certain prepositions), whereas the latter 
attaches to the plural nouns (and the other prepositions). 
%Note that the BLC does not indicate this difference in a distinct morphosyntactic feature. 

When a noun bears a pronominal suffix, both the noun and the suffix have their 
own distinct gender and number features. In addition, the suffix has a person feature. 
In such cases, the BLC treats the noun's gender and number as separate, atomic
categories in the usual way. To avoid additional and possibly conflicting gender 
and number features in the same
analysis, the BLC combines the suffix's person, gender, and number categories into 
a single composite category labeled \texttt{poss} for \textit{possessive}, as in 
examples~(\ref{ex:prosuf-3ms}) and (\ref{ex:prosuf-3mpl}).

On the other hand, when a preposition bears a pronominal suffix, there is no gender 
or number associated with the stem, and thus the BLC treats the gender and 
number of the suffix (as well as its person) 
as the primary morphosyntactic features of the word, as illustrated in 
examples~\ref{ex:prep-pro-to} 
and \ref{ex:prep-pro-on}. The BLC analyses for such suffix-bearing 
prepositions look much 
like its analyses for free-standing pronouns. 
In our category mappings, therefore, we treat the analyses of suffix-bearing 
prepositions as do those 
of personal pronouns. That is, we map person and gender onto a person-and-gender gold-standard
category, and then person, gender, and number onto another gold-standard category.
Thus, the BLC analysis for \textit{\textipa{Q}al-\'{a}yi\textsubdot{k}}, 
which consists of the categories \texttt{prep:pro}, \texttt{pers:2}, \texttt{gen:m}, and \texttt{num:3}, 
is mapped onto the two gold-standard classes \texttt{prep:pro\%2\%m} and 
\texttt{prep:pro\%2\%m\%sg}.

We retain the BLC's possessive person-number-gender feature, so that \texttt{poss:3mascSG},
for example, became \texttt{[pro\_suf:3mascSG}.\footnote{The replacement of \texttt{poss:} with 
\texttt{pro\_suf:3mascSG} was merely aesthetic and thus had no real effect.}
We additionally introduced the gold-standard class \texttt{pro\_suf\_state}, a class that applies to
all nouns bearing a pronominal suffix, regardless of the suffix's particular
%person, gender, and number 
morphosyntactic features. 
\begin{exe}
\ex \label{ex:mapping-pron-suf}
\texttt{poss:3mascSG}  $\,\,\to\,\,$ \texttt{pro\_suf\_state\&pro\_suf:3mascSG}, 
\end{exe}

%In the BLC analyses of nouns bearing a pronominal suffix, the noun stem has gender and number features
%attach to singular the noun it selects for. One set attaches to singular nouns;
%the other to plural nouns (table~\ref{tab:pron-suf-pl}).
%
%
%set consists of suffixes  members attach to singular nouns and one whose members attach to plurar nouns. 
% the other attaches to plural nouns. For example, -ay attaches to plurals, as in xxx ‘my cars’, while -i attaches to the singular form , as in `my car.’ 


%as the prepothey function as the Outside of the nominative function, pronouns occur as suffixes, e.g., in xxx, -ay is a pronominal suffix with a genitive function; it corresponds to the possessive pronoun ‘my’. There are two sets of pronominal suffixes, one set attaches to singular nouns, while the other attaches to plural nouns. For example, -ay attaches to plurals, as in xxx ‘my cars’, while -i attaches to the singular form , as in ’my car.’ 

%Pronominal suffixes also attach to prepositions. Prepositions by nature do not have number. They arbitrarily select either the suffixes in table~\ref{} or those in table~\ref{}. For example, \textipa{Q}al `on’ takes the suffixes for plural nouns, as in  \textit{\textipa{Q}al-ay} `on me’, while \textit{le-} takes the set for singular nouns, as in \textit{li} `to/for me.’
%The BLC annotates possessive pronominal suffixes differently from those that act as objects of prepositions. The BLC assigns the latter to the \texttt{prep:pro} category, treating them essentially as a class of pronouns. See table~\ref{} for a sample.

%\begin{tabbing}
%\hspace{1in} \= \hspace{5.5in} \kill
%hazo\textipa{P}t \> pre:ha\&pro_dem\&pers:3\&gen:fm\&num:sg\\
%hem \> pos:cop\&tense:present\&pers:3\&gen:ms\&num:pl pos:pro_person\&pers:1\&num:sg pos:pro_person\&pers:3\&gen:ms\&num:pl pos:part\&root:rcy\&ptn:qal\&gen:ms\&num:pl\\
%hem\v{s}\a'{e}\textsubdot{k} \> pos:n\&num:sg\&stat:u\\
%hen \> pos:cop\&tense:present\&pers:3\&gen:fm\&num:pl pos:pro_person\&pers:3\&gen:fm\&num:pl\\
%\textipa{P}\a'{a}nu \> pro_person\&pers:1\&num:pl 
%haze \> pre:ha\&pro\_dem\&pers:3\&gen:ms\&num:sg\\
%\v{s}em\a'{a}\v{s}ehu \> pre:sh\&root:m\v{s}h\&ptn:CaCeC\&pre:sh\&pro\_indef\&gen:ms\&num:sg\\
%\v{s}em\a'{i}\v{s}ehu \> pre:sh\&root:m\v{s}h\&ptn:CiCeC\&pre:sh\&pro\_indef\&pers:3\&gen:ms\&num:sg\\
%\end{tabbing}			
%\end{itemize}

\item[Particles.]
A distinct gold-standard category was created for each formally distinct prefixal particle, as well as three additional categories for the composite
prefixes \textit{la-} (`to the'), \textit{ba-} (`in the'), and \textit{ka-} (`as the'), 
which result from the blending of the prefixal prepositions \textit{le-} (`to'), 
\textit{be-} (`in'), and \textit{ke-} (`as') with the definite-article prefix \textit{ha-}.
and \text{be-} `in'. 
\end{description}

\begin{table}[t]
\centering
\setlength{\extrarowheight}{6pt}
\begin{tabular}{ccc}
\toprule
Prefix & Gloss & Gold-Std Category \\
\midrule
\textit{be-} & `in'	& \texttt{pre:be}   \\                    
\textit{ba-}	&`in the' &  \texttt{pre:be\raisebox{-1.2mm}{\textasciitilde}pre:ha}   \\                              
\textit{ke-}	&`as'  & \texttt{pre:ke} \\                
\textit{ka-}	& `as the' & \texttt{pre:ke\raisebox{-1.2mm}{\textasciitilde}pre:ha} \\          
\textit{le-}	& `to' & \texttt{pre:le}  \\               
\textit{la-}	& `to the' & \texttt{pre:le\raisebox{-1.2mm}{\textasciitilde}pre:ha} \\
\textit{ha-}	& `the' & \texttt{pre:ha}  \\      
\textit{mi(n)-} &`from' & \texttt{pre:mi}  \\               
\textit{\v{s}e-}	& `that, which' & \texttt{pre:sh} \\
\textit{we-} & `and' & \texttt{pre:we} \\
\bottomrule
\end{tabular}
\end{table}
%		pat = ur"(pos:)(adj)([^\s]*)(\&gen:)([a-z+])(\&num:)([a-z:]+)(\&stat:)(cstr)"
%		re_cstr = re.compile(pat, re.UNICODE)
%			
%		if gen == "f" and num == "pl":
%			analysis=re_cstr.sub(ur"\2\&\5\%\7\3", analysis)
%			analysis=re_cstr.sub(ur"\2\&\5\%\7\3", analysis)
%		else:
%			analysis=re_cstr.sub(ur"\2\&\9\%\5\%\7\3", analysis)
%			adj\&cstr%[gen]%[num]
%			analysis=re_cstr.sub(ur"\2\&\9\%\5\%\7\3", analysis)
%		1 (pos:)
%		2 (adj)
%		3 ([^\s]*)
%		4 (&gen:)
%		5 ([a-z+])
%		6 (&num:)
%		7 ([a-z:]+)
%		8 (&stat:)
%		9 (cstr)
%		
%		pat = ur"(pos:)(n)([^\s]*)(\&gen:)([a-z])(\&num:)([a-z:]+)(\&stat:)(cstr)"
%		re_cstr = re.compile(pat, re.UNICODE)
%		
%		if gen == "f" and num == "pl":
%			analysis=re_cstr.sub(ur"\5\%\7\3", analysis)
%			analysis=re_cstr.sub(ur"\5%\7\3", analysis)
%		gen([a-z+])
%		num([a-z:]+)
%		([^\s]*)
%		else:
%			analysis=re_cstr.sub(ur"\9\%\5\%\7\3", analysis)
%			analysis=re_cstr.sub(ur"\9\%\5\%\7\3", analysis)
%		(cstr)	
%		gen([a-z+])
%		num([a-z:]+)
%		([^\s]*)
%
%		pat = ur"(pos:)(n)([^\s]*)(&gen:)([a-z]+)(&num:)([a-z:]+)(&stat:u)?"
%		# 1: (pos:)
%		# 2: (n)
%		# 3: ([^\s]*)
%		# 4: (&gen:)
%		# 5: ([a-z]+)
%		# 6: (&num:)
%		# 7: ([a-z:]+)
%		# 8: (&stat:u)?
%		re_abs = re.compile(pat, re.UNICODE)
%		analysis=re_abs.sub(ur"\5%\7\3", analysis)
%		gen([a-z:]+)num([a-z:]+)([^\s]*)
%		
%		#print "analysis nominal", 2, ": ", analysis
%		pat = ur"(pos:)(adj)([^\s]*)(&gen:)([fmunsp]+)(&num:)([a-z:]+)(&stat:u)?"
%		# 1: (pos:)
%		# 2: (adj)
%		# 3: ([^\s]*)
%		# 4: (&gen:)
%		# 5: ([fmunsp]+)
%		# 6: (&num:)
%		# 7: ([a-z:]+)
%		# 8: (&stat:u)
%		re_abs = re.compile(pat, re.UNICODE)
%		#analysis=re_abs.sub(ur"\2&\5\%\7\3", analysis)
%		analysis=re_abs.sub(ur"\2&\5\%\7\3", analysis)
%		adj&[gen]\%[num]([^\s]*)
%\end{description}
	
%To measure precision and recall, one needs a gold standard. Supervised
%% learners train on a body of human-labeled examples, and thus come with
%% a ready made gold standard, namely some subset of the human-labeled
%% examples themselves. But unsupervised learning algorithms like the
%% MCMM do not train on pre-labeled data, and so they come with no
%% obvious gold standard against which their output can be extrinsically
%% evaluated.
%
%% \marginpar{MD: Should we mention transliterating Hebrew into Latin?
%%   It doesn't really affect the algorithm, though.}
%
%Our data is the same Hebrew word list used by \cite{daya-et-al:2008}
%in their study of automatic root identification.
%This list comprises 6888 unique words with an average length of 5.4 characters. About two-thirds are derived from consonantal roots.
%The dataset gives root annotations for the words that have roots, but specifies no other morphological information. 
%
%We thus use the output of an automatic
%finite-state morphological analyzer, namely the MILA Morphological
%Analysis tool (\textsc{mila-ma}) \citep{hebrew-resources:2008}.
%% \textsc{mila-ma} is in essence a finite-state transducer, a variation
%% of the finite-state automaton. 
%Because its morphological knowledge has been manually coded and its
%output is deterministic, \textsc{mila-ma} provides a fairly close
%approximation to human annotation. This is especially true in the case of the present study. That is, because we consider only the absolute, context-independent analyses of \emph{word
%types}, the problem of disambiguation, the usual source of uncertainty, does not arise.
%
%%group together features from different analyses.
%
%To obtain the the gold-standard dataset for a given experiment, 
%we run \textsc{mila-ma} on every word in the MCMM's output. 
%\textsc{mila-ma} gives
%at least one analysis for each word, and more than one for ambiguous words.
%For example, the form \textit{mniyim} can be
%a \textit{Hif`il} participle (`motivating'), a noun (`motivation'), or a noun bearing the prefixal preposition
%\textit{m-} (`from movement').
%% For each word, \textsc{mila-ma} produced at least one analysis, and
%% multiple analyses if the word in question is morphologically
%% ambiguous.
%\textsc{mila-ma} outputs a separate analysis for each of these possibilities; the analysis corresponding to the third possibility is displayed in 
%% possibilities;
%figure~\ref{fig:mila-output}, with brackets delimiting features and values.
%% .  As illustrated, a \textsc{mila-ma} analysis is string of categories
%% delimited by \texttt{[+}.  Each category in turn consists of a feature
%% and value delimited by \texttt{]}. 
%
%Then, in our evaluation of the MCMM's output, we examine each cluster that contains \textit{mniyim} (if any). In each case, we compare \textit{mniyim}'s \textsc{mila-ma} analyses with those of the other words in its cluster to determine the extent to which \textit{mniyim} has been grouped with similar words. Each cluster should correspond to a single predominant property. Therefore, the ideal case, at least according to our gold-standard, is that \textit{mniyim} should appear in three distinct clusters, one for each of the three \textsc{mila-ma} analyses for\textit{mniyim}.
%%other masculine plural nouns, for example, other words that begin
%%prepositional prefixes, other \textit{Hif'il} participles, or it appears in With some modifications (described next),
%%these feature-value pairs become our gold categories.
%%mniyim	[	+participle][+id]205[+undotted]hniy
%%  [+transliterated]hniy[+root]nwy[+binyan]+Hif'il
%%  [+register]+formal[+tense]+beinoni[+person]+any
%%  [+gender]+masculine[+number]+plural
%%  [+construct]+false[+definiteness]+false
%\begin{figure}[htb!]
%\footnotesize
%\begin{verbatim}
%mniyim	[+preposition]m[+noun][+id]7888
%  [+undotted]niy[+transliterated]niy
%  [+gender]+masculine[+number]+plural
%  [+definiteness]+false[+register]+formal
%  [+construct]+false
%\end{verbatim}
%%	mniyim	[	+noun][+id]6406[+undotted]mniy[+transliterated]mniy[+gender]+masculine[+number]+plural[+definiteness]+false[+register]+formal[+construct]+false
%%	mniyim	[+preposition]m[+noun][+id]7888[+undotted]niy[+transliterated]niy[+gender]+masculine[+number]+plural[+definiteness]+false[+register]+formal[+construct]+false
%\caption{One of the three analyses that \textsc{mila-ma} outputs for the word \textit{mniyim}}
%\label{fig:mila-output}
%\end{figure}
%
%\subsection{Category Mappings}
%\label{subsec:mappings}
%\textsc{mila-ma} outputs 22 possible feature labels. Four of these
%(\texttt{id}, \texttt{undotted}, \texttt{transliterated},
%\texttt{register}) are irrelevant to the present study and are thus discarded.  Each of the
%18 remaining features has at least two values,
%%(\textsc{true} and \textsc{false}), 
%and some have many more.  
%Each feature-value combination is a distinct gold-standard category.
%% Since \textsc{mila-ma} is run on the MCMM's output to obtain the gold
%% standard categories, the size of the gold standard varies with number
%% of words that the MCMM manages to cluster in a given experiment. 
%For a sense of the number of categories involved, consider that in one experiment, 
%our model assigned 4244 of the 6888 words to one or more clusters.
%%one or more clusters.
%Not counting the four irrelevant features mentioned above,
%\textsc{mila-ma}'s 
% output for these 4244 words contained 778 distinct feature-value
%combinations, i.e., categories.
%
%However, many of these original \textsc{mila-ma} categories are
%ill-suited to the purpose of evaluating an MCMM. 
%%We provide a sketch here
%%of our modifications to \textsc{mila-ma}'s category set.  
%%Space precludes a complete discussion, but it should be noted that 
%%while some mappings make the task easier, 
%%It should be noted that these linguistically-motivated mappings make the evaluation more rigorous:
%%By removing very broad categories, such as \texttt{verb} or
%%\texttt{construct:false}, we decrease the chances of receiving credit
%%for accidental clusterings.
% For example, \textsc{mila-ma}'s output contains negative categories
% like \texttt{definiteness:false} and \texttt{construct:false},
% %and \texttt{person/number/gender:-},
% but an MCMM is only capable of recognizing
% positive attributes, i.e., attributes that are actually present in
% forms. 
%% In other words, an MCMM can group together marked forms on the
%% grounds that they share a marker, but there would simply be no 
%% grounds for grouping forms that are not united by a common marker.
% %unmarked forms, since unmarked forms by definition have no formal
% %element in common. 
% Thus, while an MCMM might succeed in grouping
% \texttt{definiteness:true} forms on the basis of their shared
% definite prefix \textit{h-}, it would have no means of discovering 
% a category like
% \texttt{definiteness:false}, whose members would have no common marker.
% %for which there is no marker.
% %because the forms tha since there is no positive
% %attribute that unites them.
% 
% We thus modify \textsc{mila-ma}'s category set in certain ways. 
% The following is a summary of our modifications.
% It should be noted that in most cases these linguistically-motivated mappings make the evaluation more rigorous:
%By removing very broad categories, such as \texttt{verb} or
%\texttt{construct:false}, we decrease the chances of receiving credit
%for accidental clusterings.  
%% %For example, Hebrew makes words
%% %definite by attaching the prefix "h-". Thus, any word that has this prefix will belong to the 
%% %"definiteness:true" category, and likewise all words in the "definiteness:true" will begin with an "h-". Now, it is possible for the mcmm to cluster together all words that begin with "h-", for this is a positive formal property. But what about the "definiteness:false" words? These words all lack the definite marker, so there will be nothing that is the same across all "definiteness:false", and there will thus be no grounds for grouping them together. Note also: having negative categories inflates the results.
%% Other morphological categories may not seem at first glance seem to be negatively defined, but are nonetheless unmarked. For example, Hebrew \textsc{3.m.sg} verb forms are ``distinguished," as it were, by the absence of a marker (see the paradigm in figure \ref{fig:paradigms}).
%
%% \begin{itemize}
%% \itemsep0em
%%\item 
%%For POS labels, we retain \texttt{adverb}s, \texttt{adjective}s,
%%and \texttt{numeral}s, ignoring other categories because they are super-categories (e.g.,
%%\texttt{verb} covers seven \texttt{binyan} categories (verb classes))
%%or lack systematic marking (e.g., \texttt{preposition}).
%%%\item 
%%Atomic categories are combined where appropriate, to capture
%%  fusional morphology (e.g., \texttt{masculine} +
%%  \texttt{plural} $\mapsto$ \texttt{M\%Pl}).
%%%\item
%%Verb inflections are mapped to composite categories 
%%%based on overlapping prefixes and suffixes 
%%(cf. non-3rd-singular verbs in English).
%%  % (e.g., \texttt{{future\%(2\%M)\textbar(2\textbar3\%F)}}), akin to
%%  % non-3rd-singular verbs in English.
%%%\item 
%%For verb stems, the \texttt{binyan} and
%%  \texttt{tense} features are mapped from 21 combinations into 15
%%  features 
%%  %encoding \texttt{binyan} and 
%%  incorporating \texttt{stem\_type}.
%%%\item 
%%For participles, adjective and noun analyses are discarded
%%  (cf. \textit{-ing} forms in English).
%%%\item 
%%We use \texttt{rootless:Nominal} 
%%%  and \texttt{rootless}, 
%%  to capture the regularities of loan words.
%%%  (e.g., use of vowel letters)
%%%  , more frequent usage of \textit{v} (\textit{tet})).
%%%\item 
%%Negative categories (e.g., \texttt{construct:false}) and
%%  unmarked forms (e.g., \texttt{M/Sg} in nominals) are discarded.
%%%\end{itemize}
%
% \paragraph{POS labels.}
% Of \textsc{mila-ma}'s POS labels, we retain only \texttt{adverb}, \texttt{adjective}, and \texttt{numeral} (including cardinal and ordinal subtypes), which often bear distinctive markers. We discard each of the other POS categories for one of the following reasons: 
% \begin{itemize}
%   \item The category is a super-category of other categories and is therefore redundant. For example, \texttt{verb} is a super-category of the seven \textit{binyan} categories (verb classes). If a word is a member of any binyan, it is necessarily also a member of the \texttt{verb} category.
%   \item The category lacks a distinctive form (i.e., it is not systematically marked). Examples include \texttt{preposition} and \texttt{properName}.
%   \item The category is associated with many marginally systematic markers, but it has no predominant marker; e.g., there are many patterns by which nouns are derived, but no predominant pattern.
%%   \item The category does exhibit systematic marking, but its markers coincide with those of another category. For example, numerals bear the same markings as found on nouns and adjectives. 
% \end{itemize}
%
% \paragraph{Negative categories and unmarked forms.} 
% %As discussed above \dots
%
% As mentioned above, an MCMM can only recognize the presence of attributes, not their absence.
% We thus discard negatively defined categories such as \texttt{definiteness:false} and \texttt{construct:false}, as such categories tend to indicate the \emph{absence} of some specific marker. 
% Note again that we do not make the evaluation easier by discarding such categories because in most cases, given a positive and a negative category, the majority of words will trivially belong to the negative category.
% 
% A category does not have to negative to be unmarked, however. The \texttt{M/Sg} in nominals and \texttt{3/M/Sg} in verbs, for example, are unmarked forms. In general, we discard any category that is not associated with an overt marker. 
%
% % . In other words, an MCMM can group together marked forms on the basis
% % of some shared marker, but 
%% [it has no basis for grouping together unmarked forms, since unmarked
%% forms have no formal element in common.  Thus, while the MCMM might
%% succeed in grouping \texttt{definiteness:true} forms together on the
%% basis of the shared prefix \textit{h-}, it has no means to group
%% together \texttt{definiteness:false} words, since no positive
%% attribute unites them.]
% %
%% In general, we discard categories that are not associated with an overt
%% marker. These include \texttt{M/Sg} in nominals, the \texttt{M/Sg} and
%% \texttt{F/Pl} nominal construct states,
%% % (in that they are, in the absence of context, indistinguishable from
%% % their absolute-state counterparts),
%% and \texttt{3/M/Sg} in verbs.
%
%% % %We thus mapped the raw categories to a modified category set better suited to the present study.
% \begin{figure}[tb!]
% \begin{center}
% %\footnotesize
% \begin{tabular}{lcccc}
%    & \multicolumn{2}{c}{Past} & \multicolumn{2}{c}{Future} \\
%    & \textit{Sg} & \textit{Pl} & \textsc{sg} & \textsc{pl} \\
%   \hline
%   \textit{1.m/f} & htxil-ti & htxil-nw & a-txil & n-txil\\
%   \hline
%   \textit{2.m} & & htxil-tm  &  t-txil & \\
%   \textit{2.f} & \raisebox{1.5ex}[0pt]{htxil-t} & htxil-tn &  t-txil-i & \raisebox{1.5ex}[0pt]{t-txil-w}\\
%   \hline
%   \textit{3.m} & htxil &   &  itxil &\\
%   %\cline{1-2}
%   \textit{3.f} & htxil-h & \raisebox{1.5ex}[0pt]{htxil-w} &  t-txil & \raisebox{1.5ex}[0pt]{i-txil-w} \\
% \end{tabular}
% \caption{Past and future-tense paradigms for the root \textit{t.x.l} in the \textit{Hif'il} binyan.}
% \label{fig:paradigms}
% \end{center}
% \end{figure}
%
% \paragraph{Atomic categories.} 
% %For verbs, \textsc{mila-ma} combines person, number, and gender into a composite \texttt{person/gender/number} feature taking values such as \texttt{3/F/Sg}. However, 
% For nouns, adjectives, and participles, \textsc{mila-ma} expresses gender and number as separate atomic categories, e.g. \texttt{gender:masculine} and \texttt{number:plural}, even though Hebrew is fusional in its gender/number
% markings (see section~\ref{sec:intro}). Most masculine singular nominal forms are unmarked in Hebrew, while most masculine 
% nominals take the ending \textit{-im}. 
% On the other hand, most feminine plurals take the ending \textit{-wt}. 
% Since the masculine plural \textit{-im} has nothing in common with either the masculine singular or the feminine plural forms, it cannot be split into distinct masculine and plural components. We modify \textsc{mila-ma}'s atomic categories so that they better reflect Hebrew's fusional suffixes. That is, we merge \texttt{masculine} and \texttt{plural} into the single category \texttt{M\%Pl}. Likewise, \texttt{feminine} and \texttt{plural} become \texttt{F\%Pl}, and \texttt{feminine} and \texttt{singular} become \texttt{F\%Sg}.
%
%
% \paragraph{Verb Inflections.}
% %For every cell in figure~\ref{fig:paradigms}, there is a 
% \textsc{mila-ma} represents verbal inflections for person, number, and gender in %composite 
% \texttt{person/gender/number} (or \texttt{PGN}) features taking values like \texttt{3p/F/Sg} and \texttt{2p/M/Sg}. However, there is not always a one-to-one correspondence between such category labels and the actual distinctions in form. Consider, for example, the verb \textit{ttxil} `she/you will begin'. In the absence of context, this form is entirely ambiguous; it can be either \textsc{2.m.sg} or \textsc{3.f.sg}. \textsc{mila-ma} produces a separate analysis for each possibility, giving rise to two distinct \texttt{PGN} categories, namely \texttt{PGN:2/M/Sg} and \texttt{PGN:3/F/Sg}, where only one form exists. This is similar to the problem of negative and unmarked categories: an MCCM has no means of making a distinction where none exists in the feature representations.
% %The strictly word-internal feature sets that we use in our experiments provide the MCMM with no means of mapping a single form to more than one category.
% 
% We thus map \textsc{mila-ma}'s verb-inflection categories to composite categories based on overlapping prefixes and suffixes. For example, compare the future-tense forms \textit{t-txil-i} (\textsc{2.f.sg}) and \textit{t-txil} (\textsc{2.m.sg}/\textsc{3.f.sg}) (see figure~\ref{fig:paradigms}). Except for the addition of the \textit{-i} suffix, the \textsc{2.f.sg} form is identical to the \textsc{2.m.sg}/\textsc{3.f.sg} form. Noting both the overlap in prefixes and the difference in suffixes (i.e., \textit{-i} vs. $\emptyset$), we create the categories \texttt{future\%(2\%M)\textbar(2\textbar3\%F)} and \texttt{future\%2\%F\%Sg)}\footnote{
%The \texttt{\%} sign delimits tense, person, gender, and number, and the symbol \texttt{\textbar} means `or'.}
%The former corresponds to the prefix \textit{t-} and the latter to the suffix \textit{-i}. Both are thus necessary to represent the 
%\textsc{2.f.sg} future.
%% We define both past and future-tense inflectional features because past tense and future tense differ systematically in form.
%% In the past tense, \textsc{2.m.sg}, \textsc{3.f.sg}, and \textsc{2.f.sg} each map to a separate category, 
%% namely, \texttt{past\%2\%F\%Sg}, \texttt{past\%2\%M\%Sg}, and \texttt{past\%3\%F\%Sg}, respectively. 
%% \marginpar{Note that the 2ms and 2fs past-tense forms are orthographically identical, so
%% they should be combined in our category set. I overlooked this.}
%% We similarly derive the following categories:
%% %the \textsc{mila-ma} categories
%% \begin{itemize}
%% \item \texttt{past\%3\%F\%Sg} and \texttt{future\%3\%Sg} (We define no feature for the 3.m.sg past tense, since it is the unmarked form.)
%% \item \texttt{past\%1\%Sg}, \texttt{past\%1\%Pl}, \texttt{future\%1\%Sg}, and \texttt{future\%1\%Pl}
%% \item \texttt{past:2\%F\%Sg}, \texttt{past:2\%M\%Sg}
%% \marginpar{We still need past:2\%M\%Pl and past:2\%F\%Pl}
%% \item \texttt{(past\%3\%Pl)\textbar(future\%2\textbar3\%Pl)}
%% \end{itemize}
%% %\texttt{3p/MF/Pl}, \texttt{PGN:3p/MF/Pl}, \texttt{PGN:2p/MF/Pl}, \texttt{tense:past}, \texttt{tense:future} $\to$ 
%% %\texttt{(past\%3\%Pl)\textbar(future\%(2|3)\%Pl}
%% %\begin{itemize}
%% %\item 
%% %\item
%% %\item
%% %\end{itemize}
%% %\texttt{future:1p/MF/Sg}
%% %\texttt{future:1p/MF/Pl}
%% %\texttt{future:2p/F/Sg}
%% %\texttt{future:2p/MF/Pl}
%% %\texttt{future:} 
%% %\texttt{participle_prefix}
%% %	future:(2%M)|(2|3%F)
%% %	future:1%Pl
%% %	future:1%Sg
%% %	future:2%Sg
%% %	future:3%M
%% %	participle_prefix
%% %	past:1%Pl
%% %	past:1%Sg
%% %	past:2%F%Sg
%% %	past:2%M%Sg
%% %	past:3%F%Sg
%% %	past|future:2|3%Pl
%	
% \paragraph{Verb stems.}
% The forms of Hebrew verb stems vary along two dimensions:
% \begin{description}
% \item \textit{binyan:} There are seven \textit{binyanim}, or verb classes, each having a certain vowel pattern and taking a particular set of inflectional affixes (although person-gender-number markings are to some extent consistent across the binyanim).
% \item \textit{stem type:} Each binyan has a ``suffix stem" and a ``prefix stem." The former appears in paradigms dominated by suffixes (mainly the past tense), and the latter in paradigms dominated by prefixes (e.g., the future tense and (in most binyanim) the participle).
% \end{description} 
%% (1) There are the seven binyanim, and 
%% (2) each binyan generally has both a ``suffix" stem and a ``prefix" stem. 
%% The former is used
%% in the past tense, which has suffixes but no prefixes.
%% The latter is used in the future tenses, which takes prefixes in addition to suffixes.
% The suffix and prefix stems of the \textit{Hif'il} binyan are evident in figure \ref{fig:paradigms}; respectively, these are 
% %\verb*|h  i | 
% $h\Box \Box i \Box$
% (in the past tense) 
% and 
% %\verb*|  i | 
% $\Box \Box i \Box$
% (in the future tense).
%% \textit{h\underline{\hspace{1.5ex}} \underline{\hspace{1.5ex}} i \underline{\hspace{1.5ex}}} and \textit{\_\_i\_}, respectively.
% In every binyan except the \textit{Pa'al} and \textit{Nif'al}, the prefix stem is also used in the participle (i.e., present tense). The \textit{Pa'al} participle has its own unique stem, 
% and the \textit{Nif'al} participle uses the \textit{Nif'al} suffix stem.
% 
% The main idea is that tenses are in part identified by their stems; each tense uses a particular type of stem. 
% However, the shapes of these stem types depend on the binyan.
% \textsc{mila-ma} has separate \texttt{binyan} and \texttt{tense} feature types. 
% The former takes seven values, and the latter three.
% Since stem shape depends on \texttt{binyan} and \texttt{tense} jointly, we first combined these feature types, yielding 21 composite 
% textit{binyan}\texttt{:}\textit{tense} categories. However, noting that the future tense and participle often share the same stem type,
% we mapped these 21 categories to 15 \textit{binyan}\texttt{:}\textit{stem\_type} categories. (There are 15 instead of 14 because an additional category is necessary for the unique \textit{Pa'al} participle stem.) 
%
%%\footnotesize
%%\begin{tabular}{lccc}
%%	\textit{Pa'al} & \texttt{Pa'al:prefix\_stem} &
%%	\texttt{Pa'al:suffix\_stem} &
%%	\texttt{Pa'al:participle} \\
%%	\textit{Nif'al} & \texttt{Nif'al:prefix\_stem} &
%%	\texttt{Nif'al:suffix\_stem} & \\
%%	\textit{Pi'el} & \texttt{Pi'el:prefix\_stem} &
%%	\texttt{Pi'el:suffix\_stem} & \\
%%	\textit{Pu'al} & \texttt{Pu'al:prefix\_stem} &
%%	\texttt{Pu'al:suffix\_stem} & \\
%%	\textit{Hif'al} & \texttt{Hif'il:prefix\_stem} &
%%	\texttt{Hif'il:suffix\_stem} & \\
%%	\textit{Huf'al} & \texttt{Huf'al:prefix\_stem} &
%%	\texttt{Huf'al:suffix\_stem} & \\
%%	\textit{Hitpa'el} & \texttt{Hitpa'el:prefix\_stem} &
%%	\texttt{Hitpa'el:suffix\_stem} & \\
%%\end{tabular}	
%
% \paragraph{Participles.}
% In Hebrew, a participle, can function as a noun, an adjective, or as a participle/present-tense verb. Thus, whenever a word is a participle, \textsc{mila-ma} outputs not just a participle analysis, but also a noun analysis and an adjective analysis.
% %at least three separate analyses---one for each of these possibilities.
%%  whenever a word is a participle: one for the noun reading, one for the adjective reading, and one for participle/present-tense verb reading. However, we cannot expect the MCMM to find three distinct categories in a single form in the absence of the necessary contextual evidence for these categories.
% Moreover, we note that if word \textit{w} is a participle, then it is also (at least potentially) an adjective and a noun. But the converse is not true; that is, \textit{w} may be an adjective or a noun without also being a participle.
% Thus, whenever \textsc{mila-ma} gives participle, adjective, and noun analyses for a word, we discard the adjective and noun analyses---i.e., the \emph{entire} analyses, not just the \texttt{adjective} and \texttt{noun} categories, but all categories composing these analyses---because the participle analysis implies the other two.   
% %If a word w is a participle, then it is also an adjective. But the converse is not true. Consider the words bwgd, awhl, lwmd. These are all participles, but they are also all adjectives. But if there are two categories here, there should also be two clusters. And yet we cannot expect the algorithm to produce two identical clusters, one denoting 
% %"adjective" and the other "participle". This would be redundant, whereas the mcmm is seeking to compress the original data.
%
%\paragraph{Roots.}
%
%  \textsc{mila-ma} provides roots only for verbs, even though
%  nouns, adjectives, and adverbs often
%  contain roots. 
%  We retain \textsc{mila-ma}'s root categories, but union them with
%  the annotations
%%accompanying our data, i.e., the annotations of
%of \cite{daya-et-al:2008}. 
%
%% Root categories are thus the union of these two sets of word-to-root
%% mappings.
%
%\paragraph{Rootless words.} 
% There are two classes of words in Hebrew that generally lack roots: function words and loan words. The latter are usually nominals, e.g. \textit{svwdnt} `student' and \textit{ainvlqvwali} `intellectual'.
% Loan words are to some extent marked orthographically:
% \begin{itemize} 
% \item Whenever a pair of letters is homophonous, loan words systematically prefer one letter over the other. 
% \item For instance, loan words always use \textit{v} (\textit{tet}) 
% instead of \textit{t} (\textit{tav}) for the sound /t/, \textit{s} 
% (\textit{samex}) instead of \textit{e} (\textit{sin}) for the sound /s/, 
% and \textit{q} instead of \textit{k} for the sound /k/. 
%\item Most loan words are of English origin, and thus, due to the phonotactic patterns of English, certain character sequences such as \textit{sv} (i.e., /st/) occur over and over again. 
%\item %Moreover, 
%Loan words use the so-called \textit{vowel letters} to represent vowels more frequently and regularly than do native words, and unlike native words, they regularly use the letter \textit{a} (\textit{alef}, historically a glottal stop) to demarcate vowel-initial syllables, which were unknown in past forms of Hebrew. 
% \item %Finally, 
% Loan words tend to be longer than native words.
% \end{itemize}
%Function words, in contrast, such as particles and prepositions, are native to Hebrew and thus do not exhibit these distinctive orthographic patterns.
% To see if the MCMM is able to recognize any of the regularities observed in borrowed nominals, we introduce the category \texttt{rootless:Nominal}. 
% %We introduce the category \texttt{rootless} for all other rootless words.
%
%\paragraph{Definite prefix.}
%When the definite article prefix \textit{h-} follows the prefixal prepositions \textit{b-} (`in'), \textit{l-} (`to'), or \textit{k-} (`as, like'), it is deleted. Consequently, definite and indefinite forms are indistinguishable when they begin with any of these three prefixal prepositions. For example, 
%\texttt{bbit} can be `in the house' or `in a house' (because underlying \textit{b-h-bit} $\to$ \textit{bbit}). Because of this ambiguity,
%\textsc{mila-ma} outputs both the \texttt{definiteness:true} and \texttt{definiteness:false} analyses in such cases.
%We discard \texttt{definiteness:true} in these cases, since the deletion of the definite marker makes \texttt{definiteness:true} an unmarked category.
%The category \texttt{definiteness:false} is also discarded, since we discard all negative categories (as discussed above). 
%%The remaining categories in these two analyses may be discarded, modified, or left intact.
%% although the word in question remains definite. 
%
%\paragraph{Compound prefixes.}
%Modern Hebrew grammars and language-eduation textbooks often treat the prefix \textit{ke-} `when, while' as a single unanalyzable unit (cf. English words like \textit{into} and \textit{whenever}, which, while not prefixed clitics, are nonetheless compound function morphemes). 
%This is indeed the treatment adopted by \textsc{mila-ma}. However, the more traditional treatment is that \textit{ke-} is composed of the ``atomic" prefixal clitics \textit{k-} `as' and \text{e-} `that'. Without features pertaining to syntactic context, an MCMM would lack the evidence for merging \textit{k-} and \text{e-} into a single morpheme. We thus map the feature \texttt{temporalSubConj:ke} to the features 
%\texttt{prefix:k} and \texttt{prefix:e} wherever \texttt{temporalSubConj:ke} occurs in an analysis. We handle other compound prefixes similarly.


%\texttt{prefix:ke} $\mapsto$ \texttt{prefix:k} and \texttt{prefix:e}.
% (Notice that a lot of properties are packed into the suffix \textit{-s}).
% Let us consider  
% briefly the nature of an MCMM's output. One can represent a 
% clustering (or categorization) as a table in which the rows 
% correspond to the clustered (or categorized) items, and 
% the columns to the clusters (or categories) themselves. Now, 
% in the proposed dissertation, the items is feature-vector 
% representations of words, and the clusters is morphs. Each 
% $\text{cell}_{i,k}$ contains either 1 or 0: 1 if $\text{word}_i$ has 
% morph $k$, and 0 if it does not. 
%%What we have just described is essentially the MCMM's $\mathb{M}$ matrix. 
% Each 1 thus represents the presence of a particular morph. 
 
% If we wanted to get away from the matrix format, i.e., the grid 
% of 1's and 0's, we could replace each 1 with its morph and eliminate the 0's altogether. 
% Suppose, for example, 
% that $\text{word}_{1253}$ is \textit{runs}, which has two morphs, namely the stem 
% \textit{run} and the suffix \textit{-s} 3rd-person (\textsc{3p}), present-tense (\textsc{Pres}) singular (\textsc{Sg}) s.
% (Notice that a lot of properties are packed into the suffix \textit{-s}). 
% Suppose further that \textit{run} is cluster 87 and \textit{-s} is cluster 6, and the overall clustering 
% has a total of $K=500$ clusters, in which case $\mathbf{M}$ has 500 columns, and
% row 1253 has 498 zeros and only two 1's, one at column 6 and the other at column 87.
%We can replace the 
%1 in column 6 with \textit{-s} and the 1 in column 87 with \textit{run} 
%and discard the zeros, yielding ``runs: run, -s," which is tantamount a 
%morphological segmentation. 
%---
%Notice that the suffix \textit{-s} maps to three 
%``atomic" morphosyntactic categories, 
%namely 3rd-person (\textsc{3p}), present-tense (\textsc{pres}), 
%and singular (\textsc{sg}). 
%My system cannot learn abstract morphosyntactic labels like \textsc{3p} 
%and \textsc{sg}. Rather, it learns \emph{morphs}, 
%%the pre-morphosyntactic units of form. We will call these units \emph{morphs}, 
%which may or may not correspond to morphosyntactic categories (see the discussion in section~\ref{sec:targets}). 
%When there \emph{is} a 
%correspondence between morphs and morphosyntactic categories, it is often a 
%one-to-many mapping because the same morph can be
%requisitioned by more than one morphosyntactic category.
%---
 %can lay claim to the same morph. 
%In ``runs: run, -s," for example, the suffix \textit{-s} 
%represents the union of three ``atomic" morphosyntactic categories, namely \textsc{3p}, \textsc{pres}, and \textsc{sg}. 

%Thus, the output of an MCMM, after a little post-processing, can look like ``runs: run, -s." It is essentially a list of morphological segmentations. But more accurately, it is a list of word-to-cluster mappings; for each word, it will specify the cluster(s) to which it belongs. Note that the morphs \emph{run} and  \emph{-s} are essentially the labels of particular morphological clusters.
%The output of an MCMM is thus essentially a list of word-to-cluster mappings. Each item in this list is a word followed by a list of the clusters in which it has membership.
%%After little post-processing, it can look like ``runs: run, -s." 
%Morphs like \emph{run} and \emph{-s} are essentially cluster labels. 
%That is, \emph{-s} represents a cluster whose words predominantly end in  \emph{-s}.

%\subsubsection{Orthographic data.}

%To obtain morphological analyses for the wordlist O, I will use the 
%MILA Morphological Analysis tool (\textsc{mila-ma}) \citep{hebrew-resources:2008}.
%Because \textsc{mila-ma} requires that input words be spelled 
%according to Modern Hebrew standard orthography, it can only be
%used to create a gold standard for orthographic wordlist O. The 
%gold-standard morphological analyses for the transcribed wordlists TS and TR 
%must come from a different source (see below).
%\textsc{mila-ma} is essentially a finite-state transducer. Because its morphological 
%knowledge has been manually coded by humans and its output is
%deterministic, it provides a good approximation to human
%annotation. 
%
%However, many of the original \textsc{mila-ma} categories are ill-suited to the purpose
%of evaluating an MCMM's clustering. The \textsc{mila-ma} categories are
%often atomic and abstract, e.g., \texttt{feminine} and \texttt{masculine}. 
%Such categories are purely morphosyntactic; they are meaningless at the 
%word-internal level because they can only be observed in agreement phenomena. 
%Moreover, there is no morphological unit in Hebrew that means strictly `feminine,' %(i.e., nothing more than `feminine' and nothing less). 
%nor is there one that means strictly ``masculine." Hebrew inflectional affixes 
%tend to be fusional, having meanings like ``feminine plural" and ``masculine plural."
%
%For this and similar reasons, \textsc{mila-ma}'s categories need to be mapped 
%to a modified set of gold-standard categories, i.e., categories that correspond 
%more closely to actual differences in form.
%The MCMM's clustering will then be quantitatively compared to the modified 
%\textsc{mila-ma}-based gold-standard categorization. 
%In particular, I used the measures \emph{average cluster-wise purity}, 
%\emph{BCubed precision} and \emph{BCubed recall}. The latter two are important 
%because they are specifically designed for cases of overlapping clusters 
%\citep{amigo-et-al:2009}.

%\subsubsection{Transcribed data.} Gold-standard category mappings for the 
%transcribed words were obtained by extracting morphological analyses from the Berman 
%Longitudinal Corpus (BLC). Recall that for each utterance in the BLC, 
%there is a transcription tier and a morphological-analysis tier. The latter provides a 
%morphological analysis for each word in the utterance, including roots for the words 
%that have roots. I extracted the morphological analyses and used them to create a list of 
%word-to-category mappings. % resembling created from \textsc{mila-ma}'s analyses.

\subsubsection{Evaluation Metrics} 
\label{sec:metrics}
We evaluate the intrinsic results according to three metrics: \textbf{average purity}, \textbf{BCubed precision}, and
 \textbf{BCubed} recall. For the purpose of describing these metrics,
 let $\mathbf{U}$ denote a \emph{clustering}, i.e., the complete set of $K$ clusters $\mathbf{u}_k$
 discovered by an unsupervised clustering system. Let
 $\mathbf{V}$ denote the set of $J$ gold-standard categories and $X$ the set of $N$ data points 
 that supplies the clusters in $\mathbf{U}$ with their members. 
% 
% There are a total 
% of $N$ data points in $X$. 
% individual data points that have been clustered.
\paragraph{Average purity.}
The standard purity metric seeks to calculate the global correctness of a 
given clustering by determining the proportion of data points that have 
assigned to the correct cluster. Each individual data point has at some 
point been associated with a gold-standard \emph{category} label. Thus, 
in any given cluster, there are in effect as many gold-standard category 
labels as there are data points. Standard purity assumes that most frequent 
 gold-standard category in a given cluster $\mathbf{u}_k$ is the gold-standard label for the entire cluster. The number of correctly clustered data points in $\mathbf{u}_k$ is thus equal to the frequency of  $\mathbf{v}_j$ in cluster $\mathbf{u}_k$. Standard purity sums up the correctly clustered data points over all clusters, and divides this sum by $N$, the total number of distinct data points in the clustering
 %The reason $v_j$ is the most frequent category in $u_k$ is that it is associated with more of $u_k$'s members than any other data point .in the cluster is the same as the frequency of data points that bear the label. 
% Thus, the frequency of the most common gold-standard category in a cluster $U_k$  is deemed the number of correctly clustered items in $U_k$ x$ instances of this category label in the cluster, there are also $x$ data points associated the course equal to the number of data points associated with this category. data points he purity of this particular is thi
 The standard version of purity is computed as 
\begin{equation} \label{eq:pur1}
\text{purity}(\mathbf{U}, \mathbf{V}) = \frac{1}{N} \sum_{k \in K} \text{max}_{j \in J} |\mathbf{u}_k \cap \mathbf{v}_j|
\end{equation}
In other words, 
The problem for our purposes is that this version of purity assumes that each datapoint belongs to exactly only one gold-standard category. In fact, it requires that this be so, for if any data point should belong to more than one cluster belong to more than one cluster, the numerator in \eqref{eq:pur1} would be greater than than 1, and thus the resulting purity would exceed 1.
In the present study, each data point is a word, and the categories are morphological categories. In natural languages, words frequently belong to multiple morphological categories at once.
%however, which contains multi-category examples, this assumption can yield purities greater than 1. 

To avoid purities that exceed 1, we modify equation \eqref{eq:pur1} as follows: 
\begin{equation} \label{eq:pur2}
\text{purity}_{\text{avg}}(\mathbf{U}, \mathbf{V}) =  \frac{1}{K} \sum_{k \in K} max_{j \in J} |\mathbf{u}_k \cap \mathbf{v}_j|
\end{equation}
This new equation represents an \emph{average cluster-wise purity}; i.e., it computes each cluster's internal purity and then averages over these purities. %the mean of the $K$ clusters’ internal purities. 
While this equation yields purities within $[0, 1]$, even 
when clusters overlap, it retains %the well-known bias of the 
purity metric's bias toward small clusters. We thus incorporate other metrics. 

\paragraph{BCubed precision and recall}
The metrics \emph{BCubed precision} (BP) and \emph{BCubed recall} (BR) \citep{bagga-and-baldwin:1998} evaluate a clustering by checking one 
pair of data points at a time, comparing the relationships that the algorithm has posited for each pair against their 
gold-standard relationships. These metrics are well-suited to cases of overlapping 
clusters \citep{amigo-et-al:2009}. 
In such cases, it is possible for two data points $x$ and $y$ to overlap both in their algorithm-assigned clusters and their 
gold-standard categories. Suppose that $x$ and $y$ overlap in $m$ clusters and $n$ categories. Ideally, $m$ would 
equal $n$, in which case there would be a one-to-one correspondence between clusters and gold-standard categories. 
In imperfect cases, however, either $m$ will be less than $n$ or vice versa. BCubed precision essentially 
measures the extent to which $m \leq n$; for if $m>n$, i.e., if $x$ and $y$ co-occur in more clusters than gold-standard categories, then the algorithm has posited at least one 
spurious relationship between $x$ and $y$. BCubed precision thus penalizes false co-memberships between pairs of data points.

By contrast, BCubed recall measures the extent to which $m \geq n$; for if $n<m$, then the algorithm 
has missed at least one gold-standard relationship between $x$ and $y$. BCubed recall thus 
penalizes missing co-memberships. % (i.e., missed gold-standard relationships). 
At the core of BCubed precision is the measure multiplicity precision ($\text{MultiP}$):

\begin{equation}\label{eq:multi-p}
\text{MultiP}(x,y) = \frac{\text{min}(|\mathbf{U}(x) \cap \mathbf{U}(y)|, |\mathbf{V}(x) \cap \mathbf{V}(y)|)}{|\mathbf{U}(x) \cap \mathbf{U}(y)|}
\end{equation}
where $x$ and $y$ are data points in $X$; $\mathbf{U}(x)$ is the set the clusters that contain $x$, 
and $\mathbf{U}(y)$ is the set of clusters containing $y$.
Finally, 
$\mathbf{V}(x)$ and $\mathbf{V}(y)$ are the sets of gold-standard categories for $x$ and $y$. 
Notice that $\text{MultiP}(x,y)$ is a comparison of two items $x$ and $y$, neither of which is the 
particular focus of the measure. $\text{MultiP}(x,y)$ is thus a joint description $x$ and $y$. 
To obtain a precision value that describes $x$ alone, one must compute $\text{Avg}_{y}(\text{MultiP} (x,y))$, the 
average $\text{MultiP}(x,y)$ over $y \in X$, by computing $\text{MultiP}(x,y)$ for every $y$ that shares a cluster with $x$ 
and averaging the resulting values. To obtain a precision value that describes the whole dataset (not just a single $x$), 
one must compute $\text{Avg}_y(\text{MultiP}(x,y))$ for every $x$ in $X$ and then take the average of these averages. 
This average of averages is BCubed Precision. 

\begin{equation}\label{eq:bp}
\text{BP}=\text{Avg}_x [\text{Avg}_{y.\mathbf{U}(x) \cap \mathbf{U}(y) \neq \emptyset}(\text{MultiP}(x,y))]
\end{equation}
The measures multiplicity recall (MultiR) and BCubed recall (BR) are analogous to multiplicity precision and
 BCubed precision. The computation of MultiR is identical to that of MultiP \eqref{eq:multi-p}
 except that gold-standard categories replace clusters in the denominator: 
\begin{equation}\label{eq:multi-r}
\text{MultiR}(x,y) = \frac{\text{min}(|\mathbf{U}(x) \cap \mathbf{U}(y)|, |\mathbf{V}(x) \cap V (y)|)}{|\mathbf{V}(x) \cap \mathbf{V}(y)|}
\end{equation}
Likewise, BCubed recall is is nearly identical to BCubed precision \eqref{eq:bp}, except that MultiR replaces MultiP, 
and the set $\mathbf{V}$ replaces $\mathbf{U}$ in the $\text{Avg}$ function's subscripted expression.
\begin{equation}\label{eq:br}
\textrm{BR} = \textrm{Avg}_x [\textrm{Avg}_{y.\mathbf{V}(x) \cap \mathbf{V}(y) \neq \emptyset}(\text{MultiR}(x,y))]
\end{equation}

\subsection{Extrinsic Evaluation} \label{sec:eval-extrinsic} An \emph{extrinsic evaluation} 
views a system as an embedded component of a larger system, i.e., as an ``upstream'' system that feeds a ``downstream''
system. An extrinsic evaluation makes sense in the case Multimorph because it learns autonomous morphological units---i.e., \emph{morphs}---which are intermediate by nature (see chapter~\ref{autonomous}).
Multimorph is thus naturally conducive to functioning as an embedded system---as an intermediate step in a larger process

Even though the ultimate purpose of an extrinsic evaluation is to evaluate the embedded---or upstream---system,
 but it does so by evaluating the downstream' system, since the
 performance of the downstream system depends on that of the upstream system. 
 The assumption is thus that an evaluation of the downstream system is tantamount to an evaluation of the upstream system.
% the upstream system performs poorly, and 
%an extrinsic evaluation  if the downstream system scores highly,
%the upstream system (the system under evaluation), 
%scores highly. 
%Multimorph learns morphs, which which are intermediate nature (see \ref{autonomous}).
%Multimorph is thus naturally conducive to functioning as an embedded system, as an intermediate step in a larger process.
In this study, the upstream system was Multimorph. The downstream system was
 Morfessor
\citep{creutz:2003, creutz-and-lagus:2005, creutz-et-al:2006, creutz-and-lagus:2007}, an 
unsupervised morphological segmentation 
system takes takes a list of words, induces a lexicon of morphs 
for these words, and divides each input word into its component morphs. % for each word. 
%tic\.{t}ar\.{k}\'{i} %c:\[2\] 8:\[4,5\] 43:\[5,7\] \UTF{1E6D}:\[3\] \UTF{1E33}:\[6\] 22:\[0,1\]
%\begin{enumerate}
%\item 
%Multimorph learns autonomous morphological units---i.e., \emph{morphs}---which are intermediate nature (see \ref{autonomous}). Multimorph is thus naturally conducive to functioning as an embedded system---as an intermediate step in a larger process
%facilitate the learning of morphemes. %My system is thus meant to be an embedded component of a larger process. 
%\item It is very difficult to come up with gold-standard morphs, but gold-standard morphemes are relatively easy to produce, since morphemes, in contrast to morphs, are already well-defined. 
%As intermediate units, the value of morphs lies in their utility, i.e., in their capacity for yielding correct morphemes. What they look like is not important as long as they are effective. is evaluate \emph{morphemes} that have been induced from morphs. 
%On the other hand, it is relatively easy to come up with gold-standard morphemes, since morphemes, in contrast to morphs, are well-defined. 
%\end{enumerate}
%The extrinsic evaluation consisted of the four stages described in the 
%Stages 1 to 3 prepare the output of an MCMM to be fed to Morfessor in Stage 4. 
%To obtain the gold-standard datasets for Morfessor, I manually segmented 
%$\frac{1}{10}$ of the original, unprocessed wordlists, i.e., both the transcribed 
%and orthographic wordlists. Note that this four-stage extrinsic evaluation only considers 
%stem-external, concatenative morphology. This is because Stage 3 effectively removes 
%interdigitation. Morfessor is not even capable of handling interdigitation, since it is 
%a sequential algorithm.
%But how is one to assess the output's quality? How do we tell 
%how \emph{good} it is? I need a way to evaluate the clusters produced by the MCMM.
%But a cluster is just a group of words that Multimorph has seen fit to put together according to criteria of its own devising.
%These criteria could be virtually anything, as Multimorph is directed solely 
%by an algorithm, not by any previously attained knowledge concerning
%the workings of morphology or human language. 
%Thus, the \emph{meaning} of one of Multimorph's 
%clusters may not be immediately obvious.
%\subsection{} \label{sec:paradigms}
%\subsection{Four-stage process.} 
%\label{sec:four-stage-process}
%\label{sec:extrinsic}
%The complete extrinsic-evaluation process required a substantial 
%The extrinsic evaluation of
%Multimorph's output required some post-processing; that is, it had to be prepared to be fed to
%the downstream application Morfessor. The complete 
%The complete extrinsic-evaluation process---beginning with Multimorph's raw output (i.e., the MCMM's clustering) and culminating in Morfessor's output and (crucially) the evaluation of this output---can be broken down into four stages. Basi
%The extrinsic evaluation is a process that takes as input the raw \emph{output} of Multimorph, i.e., the clustering
%encoded in the 
%%raw output is the cluster 
%MCMM's $\mathbf{M}$ and $\mathbf{C}$ matrices. Each experimental trial produced such a
%clustering. 
%The extrinsic evaluation took each experiment's clustering, and through
%through a succession of post-processing \emph{stages}, molded it into a format acceptable to Morfessor and then fed to Morfessor as \emph{input}. Morfessor \citep{creutz:2003, creutz-and-lagus:2005, creutz-and-lagus:2007} thus served as the ``downstream" application in extrinsic evaluation. was an unsupervised morphological segmentation system, takes a list of words as input and induces a morphological segmentation for each word. \
The extrinsic evaluation in this study was thus intended to test the following hypothesis:
\begin{quotation}\noindent
Multimorph finds \emph{useful} intermediate morphological units, i.e., units that reduce the
complexity of an input wordlist in a way that could be helpful to a downstream process.
\end{quotation}
where the downstream process was Morfessor in our case.
The central assumption here was that ``helpfulness to a downstream process''  would manifest as an improved performance in the downstream process (i.e., Morfessor). %the downstream system, namely in our case. 
% systMorfessor's task by \emph{pre-processing} its input wordlist---that is, by identifying intermediate morphological units and thus producing a less complex \emph{intermediate wordlist} that will be easieMorfessor to n analyze for can analyze more easily and effectively than the original wordlist.
%\end{quotation}
%The purpose of the extrinsic evaluation was essentially to test this hypothesis. 

The extrinsic evaluation thus had to include a process for taking Multimorph's raw output---the MCMM's
$\textbf{M}$ and $\textbf{C}$ matrices, deriving from it an intermediate wordlist---i.e.,
a list of intermediate strings corresponding to the original input words---that could %The resulting intermediate
%wordlist could 
then be fed to Morfessor. 
This intermediate wordlist was the experimental wordlist for its experimental trial
and was thus accompanied by a \emph{control wordlist}, i.e., an unaltered wordlist.
Because the different experimental trials---i.e., 
the different combinations $\delta$ and $s$---covered
different sets of input words, a particular pair of experimental and control wordlists had to be created for each
experimental trial.
%this had to done for Each experimental trial; that is, 
%intermediate---or \emph{experimental}---wordlist was accompanied by a \emph{control} 
%wordlist, i.e., an unaltered version of itself. 
Morfessor processed both the experimental and control wordlists, 
thus inducing two distinct unsupervised segmentation 
models, one experimental and one control. 
%Each experimental trial---i.e., each combination of $\delta$ and $\s$--- 
%Because the different experimental trials---i.e., the different combinations $\delta$ and $\s$---covered
%different sets of input words, a particular pair of experimental and control wordlists had to be created for each
%experimental trial.
%---i.e., each combination of $\delta$ and $\s$, since the
%various trials had different word coverages. 
%to Morfessora \emph{control} wordlist ---i.e., a wordlist that Multimorph 
%had not touched,  %---to serve as \emph{control}. Morfessor had to be run on both
%then run on both 
%experimental and control files,  inducing two unsupervised segmentation 
%models---one experimental and one control. 
These two models were evaluated
%; that is, both models were tested on the same test set and the resulting segmentations evaluated 
against the same gold-standard dataset. 


The extrinsic evaluation process can be broken down into four main stages, namely
\begin{enumerate}
\item Interpret clusters as morphs
\item Match morphs to words
\item Compress and encode
\item Decompress and evaluate
\end{enumerate}
Stages 1 to 3 prepared the output of an MCMM to be fed to Morfessor in Stage 4. 
%To obtain the gold-standard datasets for Morfessor, I manually segmented 
%$\frac{1}{10}$ of the original, unprocessed wordlists, i.e., both the transcribed 
%and orthographic wordlists.
%Note that this four-stage extrinsic evaluation only considers 
%stem-external, concatenative morphology. This is because Stage 3 effectively removes 
%interdigitation. Morfessor is not capable of handling interdigitation, since it is 
%a sequential algorithm.
It what follows we discuss these four stages in
 greater detail, describing each in turn.
%input 
%wordlists from Multimorph's raw output, i.e., from MCMM clusterings. 
%For each intermediate---or \emph{experimental}---wordlist, I needed 
%or corresponding ordinary wordlist---i.e., a wordlist that Multimorph 
%had not touched---to serve as a control. Morfessor was then run on both 
%experimental and control files, inducing two unsupervised segmentation 
%models. Both models were evaluated against a gold-standard segmentation 
%and the results compared. 
%Morfessor was run on both the  or \emphMor   determine whether or not Morfessor such intermediate wordlists beanalyze such intermediate, or experimental, wordlist more efficaciously than an ordinary wordlist, I also needed 
%to create control wordlists 
%Multimorph's output is an MCMM
% In then induces a It culminates with Morfessor's output and, crucially, the evaluation of this output. The central 
%As discussed in chapter~\ref{ch:MCMM}, particularly 
%section~\ref{subsec:example}, Multimorph's raw output consists of the matrices $\mathbf{M}$ and $\mathbf{C}$, the 
%former comprising cluster-membership vectors, and the latter cluster centroids. 
%%\mathbf{M}$. 
%Together, these matrices encode a clustering. 
%Before it can be fed to Morfessor, such a clustering must be molded 
%into the prescribed format for a Morfessor training file. This means 
%Multimorph's clustering of input words generated must be 
%converted into a set of word \emph{segmentations}.
%%, i.e., a segmentation for each input word. 
%The complete process comprises
%four major stages. The first three are concerned with 
%preparing the training and test files for Morfessor: 
%First, each of the MCMM's clusters, by way of the cluster centroids 
%in $\mathbf{C}$, must be converted into an explicit morph, 
%i.e., into a form that can be searched for within a string. 
%These morphs then must matched to words and aligned 
%with the characters within each word to yield ultimately a 
%morphological segmentations for each word. 
% each word to compute the segmentations on which Morfessor is trained.  
% In Stage 3, the morphs in the word \textit{hicl\'{a}xti} are replaced 
% by atomic CJK characters.
 %ultimately give rise to segmentations and ultimately 
%including the training and testing of Morfessor models, comprises four major stages. 
%substantial amount of preprocessing; it must be molded   ---can be broken down into four stages. Basi
%We will now discuss each of these stages in turn.

\begin{figure}[t]
% Define block styles
%\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
%    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
%\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
%    text width=10em, text centered, rounded corners, minimum height=4em]
%\tikzstyle{line} = [draw, -latex']
%\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
%    minimum height=2em]
%\tikzstyle{decision} = [diamond, draw, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, text width=7em, text centered, rounded corners, minimum height=4em]
%\begin{tikzpicture}[node distance = 3cm, auto]
\begin{tikzpicture}[node distance=3cm, auto, >=stealth]
    % Place nodes
    \node [block] (stage1) {\textsc{\small\textsf{\textbf{Stage 1}}} \\ Interpret clusters as morphs};
    \node [block, right of=stage1, node distance=4cm] (stage2) {\textsc{\small\textsf{\textbf{Stage 2}}} \\ Match morphs to words};
    \node [block, right of=stage2, node distance=4cm] (stage3) {\textsc{\small\textsf{\textbf{Stage 3}}} \\ Compress and encode};
    \node [block, right of=stage3, node distance=4cm] (stage4) {\textsc{\small\textsf{\textbf{Stage 4}}} \\ Decompress and evaluate};
    %\node [block, below of=identify] (evaluate) {evaluate candidate models};
%    \node [block, left of=evaluate, node distance=3cm] (update) {update model};
%    \node [decision, below of=evaluate] (decide) {is best candidate better?};
%    \node [block, below of=decide, node distance=3cm] (stop) {stop};
    % Draw edges
%    \path [line] (stage1) -- (stage2);
%    \path [line] (stage2) -- (stage3);
%    \path [line] (stage3) -- (stage4); 
   \draw[->] (stage1) -- (stage2);
   \draw[->] (stage2) -- (stage3);
   \draw[->] (stage3) -- (stage4); %node[above] {yes} (d);
%   \draw[->] (c.north) to [out=170,in=45] node[above] {no} (b.north);
%   \draw[->] (d.south) to [out=210,in=20] (e.north);
%   \draw[->] (e) -- (f);
%   \draw[->] (f) -- (g);
%   \draw[->] (g) -- (h);
%    \path [line] (decide) -| node [near start] {yes} (update);
%    \path [line] (update) |- (identify);
%    \path [line] (decide) -- node {no}(stop);
%    \path [line,dashed] (expert) -- (init);
%    \path [line,dashed] (system) -- (init);
%    \path [line,dashed] (system) |- (evaluate);
\end{tikzpicture}
\label{fig:four-stages-flowchart}
\caption{The four stages of the extrinsic evaluation process.}
\end{figure}


%It begins with the raw output of Multimorph's MCMM, namely the matrix of cluster centroids $\mathbf{C}$ and the matrix of cluster-membership vectors $\mathbf{M}$, 
%raw output to Morfessor's output and its evaluation, can be broken down into four stages. We describe each of these stages in this section. 

%i.e., the clustering defined by the values in the $\mathbf{M}$ and $\mathbf{C}$ matrices, to the output of Morfessor (and crucially, the evaluation of this output) can be broken down into four stages. 
%
%had to be processed before it could be undergo thThis subsection 
%will describe each of the four stages. The input to Stage 1 is a cluster centroid vector, 
%i.e., one the $K$ columns in the $J \times K$ matrix $\mathbf{C}$. The $k$th column 
%corresponds to the $k$th cluster. Each $j \in J$ corresponds to a particular feature; the 
%$J$ rows correspond to the same $J$ features present in each original data point 
%$\mathbf{x}_{i}$ as well as each reconstructed data points $\mathbf{r}_{i}$, where 
%$0 \ge i < I$, and $I$ is the total number of data points (original and reconstructed).

\subsubsection{\textsc{Stage 1:} Interpret clusters as morphs} 
%Each cluster 
%%ultimately represents a morphological equivalence class, that is, 
%is a set of words that Multimorph's MCMM has grouped together because their feature-vector representations share certain features. That is,
%the intersection of all active features across all words in the cluster is nonempty. This intersection is shared (or held-in-common) feature subset is given by the cluster's centroid vector, as discussed in section~\ref{subsec:example}.
%%in particular the active feature's in the cluster's centroid. 
%%particularly a subset that Multimorph's MCMM has deemed useful to its task of constructing a global model of the dataset. 
%%These shared features are found in 
%As discussed in section~\ref{subsec:example}, I interpret these shared features---i.e., the as representing a morph, i.e., that 



%\subsection{A Simple MCMM Example}
%\label{subsec:example}
%
%Figure~\ref{fig:example-1} shows a simple MCMM at two distinct stages. Subfigure~\ref{fig:example:subfig1-1} shows the MCMM before convergence, while learning is still in progress. The surface and hidden-unit activities at this stage are still greater than $0$ and less than $1$, though they are on their way to either $0$ or $1$. Subfigure~\ref{fig:example:subfig2-1}, on the other hand, shows the MCMM after it has converged, when the learning process is complete.
%
%The input data matrix for the MCMM in figure~\ref{fig:example-1} consists of three data points (i.e., $I = 3$).
%The hidden cluster activities $\mathbf{M}$, the weights $\mathbf{C}$, the surface-unit vectors $\mathbf{R}$,
%and the mixing function together constitute a model that reproduces the
%observed data points $\mathbf{X}$.
%The nodes $m_{i,k}$ are the hidden units and represent cluster-membership activities. There are three cluster-membership vectors $m_{i}$, one for each $i \in I$, i.e., one for each data point. Because there are two clusters, each cluster-membership vector has two components. For instance, $\textbf{m}_{0} = [0, 1]$, which means the first data point $\textbf{x}_{0}$ does not belong to the first cluster, but \emph{does} belong to the second. By contrast, $\textbf{m}_{1} = [1,0]$ indicates that $\textbf{x}_{1}$ belongs to first cluster, but not to the second. Finally, $\textbf{m}_{2} = [1,1]$ indicates that  $\textbf{x}_{2}$ belongs to \emph{both} clusters. 
%
%These cluster-membership vectors make more sense when we consider the this MCMM's $J \times K$ ($3 \times 2$) weight matrix $\textbf{C}$. The two column-vectors $\textbf{c}_0 = [0,1,0]$ and $\textbf{c}_1 = [1,0,1]$ are the \emph{centroids} of the first and second clusters, respectively. A cluster's centroid is a sort of average or composite of the cluster's member vectors; it represents the feature values that a cluster's members have in common and can thus serve as a label for a cluster. The first data point $\textbf{x}_0 = [0,1,0]$ is thus clearly a the cluster whose centroid  $[0,1,0]$. Similarly, the second data point $\textbf{x}_1 = [1,0,1]$ is clearly of the member of the cluster whose centroid is $[1,0,1]$. But just as clearly, the influence of both clusters is evident in the third data point $\textbf{x}_2 = [1,1,1]$.
%
%Table~\ref{tab:cl-members} shows the members as well as the centroids of each of the two clusters: Notice that the active components in each cluster centroid correspond to the shared component values among the cluster's members.
%I interpret a 
%Each cluster's common feature subset is given by its centroid vector. 
%In particular, it is the set of active features in the 
%centroid vector, as discussed in chapter~\ref{sec:MCMM}.
%%, i.e., the $J$-length 
%%columns of the $J \times K$ matrix  $\mathbf{C}$. 
%%such feature subset has been deemed by Multimorph's MCMM to be \emph{globally} significant, i.e., as a feature subset which, as a unit, contributes to a model [account, theory] of the whole dataset.
%%.e., a set of words with a non-empty intersection of features. I interpret the common feature 
%The purpose of Stage 1 was to 
%The purpose of Stage 1 was to interpret clusters as 
%%concrete 
%morphs. That is, while 


An MCMM's output consists of 
the matrices $\mathbf{M}$ and $\mathbf{C}$. As described in chapter~\ref{sec:MCMM}, the $\mathbf{M}$ contains a cluster-membership vector for each data point (i.e., word), while the matrix $\mathbf{C}$ contains each cluster's ``average'' vector or \emph{centroid}, i.e., the intersection of its members active features (See the example in section~\ref{subsec:example}.)
%cluster's centroid gives the active features that its member data points have
%That is, a cluster is a set of data points, and 
%each data point is vector containing some active features and some inactive features. 
%%each of which can be described as a set of active features. 
%The co-members within a cluster all share some active features; this is the reason they are in the same cluster. share some active features; that is, there is a non-empty intersection. A cluster's centroid is the intersection of the active feature sets among a cluster's members.
%(See the example in section~\ref{subsec:example}.) 
A cluster's centroid thus concisely expresses the relationship between the cluster's members, which, ultimately, is the cluster's meaning. 
Cluster centroids can therefore be used as cluster labels. In our case, however, a centroid is a vector of $J$ feature activities, each of which is a number in the interval $[0,1]$. 
%The $J$ features in a centroid correspond to the $J$ features in a data point, and each activity is a number in the interval $[0,1]$. 
The goal of Stage 1 was to convert each centroid---a numerical, multi-dimensional vector---to a textual unit of morphological structure.

Each cluster implicitly represents a morph, but the task of morphological segmentation requires \emph{explicit} morph representations.
 %for purposes of morphological segmentation. 
 That is, one needs concrete, string-based representations that can be recognized as subsequences within larger strings of characters (i.e., words). 
%an implicit representation in a string needs a more concrete explicit representation in order to recognize the cluster/morph in a string. 
The purpose of Stage 1 was therefore to derive string-relatable morphs from the cluster centroids in $\mathbf{C}$.
%to map each cluster centroid in $\mathbf{C}$ to a string-based representation, in particular a regular expression. 
I chose to represent morphs as regular expressions for two main reasons: First, it is straightforward and efficient to match regular expressions to strings, and second, regular expressions are more flexible than raw, literal strings; for example, the regular expression 
\texttt{a.?b} matches \texttt{ab} as well as strings in which a character occurs between the 
%characters (but possibly zero or one) intervene between
the \texttt{a} and \texttt{b}, whereas the literal string \texttt{ab} can of course only match \texttt{ab}.  
%The morph representations themselves took the form of regular expressions.
%particularly a
%representation that could be matched to strings (i.e., words). 
%%to serve as the 
%%Each morph
%%``face for its respective cluster. 
%In particular, representation can serve as the face for derive an explicit morph to
%In Stage 1, we interpret these common feature subsets as concrete morphs, i.e., 
%% from these com interpret this common subset of features as a morph, and the morph, then, as a metonym for the cluster.%whose members all share a particular morphological unit which share a particular 
%since a morph is ultimately a morphological equivalence class,
%unit of morphological organization, i.e., a morph. But as a 
%cluster, it is too abstract to be directly relatable to actual words, i.e., to strings consisting 
%of characters. 
%The first stage is therefore
%%in the four-stage extrinsic evaluation process is 
%to make the clusters relatable to strings. 
In particular, each 
cluster was equated with a regular expression that is derived from the active 
features of its centroid. A morph's regular expression is built 
up from ``atomic" regular expressions derived from its 
individual active features. 
%The form of an atomic expression depends on the type of its source feature; that is,

Each of the two feature types---positional and precedence---is mapped to a different form of
atomic regular expression. Recall from chapter~\ref{ch:experi} that positional features take 
%upon which it is based. 
%A positional feature takes the form
the form
\begin{equation*}
\alpha\text{@}i
\end{equation*}
%\begin{align}
%\textrm{Positional}  \qquad \alpha\text{@}i \\
%\textrm{Positional}  \qquad \alpha\text{@}i
%\end{align}
where $\alpha$ is a character in the alphabet, and \textit{i} is a positive or negative integer 
giving the index of $\alpha$ within the word in question. As explained in chapter~\ref{ch:experi}, 
positive indices count character positions from the front of the word, while negative indices 
count them from the end. However, because it is not possible to represent indices directly in a regular
expression, we do not express them in atomic regular 
expressions. %for positional features.
Thus, the mapping from a positional feature to an atomic regular expression always takes the form
%mapped onto an atomic regular expression of expressions corresponding to positional features 
\begin{equation} \label{eq:positional-to-regex}
\alpha\text{@}i \qquad \mapsto \qquad \texttt{(}\alpha\texttt{)}
\end{equation}
The parentheses encompassing $\alpha$ are important for Stage 2; they define 
\emph{groups} of matching characters within a matching
string, so groups that can be identified by group indices, i.e., by their positions relative to other groups
in the matching string.
The following examples show five fully-specified instantiations of the mapping rule \eqref{eq:positional-to-regex}:
\begin{exe}
\ex \label{ex:positional-morphs}
\begin{xlist}
\begin{tabbing}
\hspace{0.7in} \= \hspace{0.5in} \=  \hspace{4.5in} \kill
 \texttt{e}\textrm{@}\texttt{[1]} \> $\to$ \>   \texttt{(e)} \\
 \texttt{h}\textrm{@}\texttt{[0]}  \>$\to$ \>     \texttt{(h)} \\
 \texttt{t}\textrm{@}\texttt{[2]}  \> $\to$   \>       \texttt{(t)} \\
 \texttt{t}@\texttt{[-1]}  \>  $\to$   \>    \texttt{(t)} \\
 \texttt{u}\textrm{@}\texttt{[-3]} \>   $\to$   \>   \texttt{(u)} \\
\end{tabbing}
%	\ex \texttt{e}\textrm{@}\texttt{[1]}\qquad  $\to$ \qquad    \texttt{(e)}
%	\ex \texttt{h}\textrm{@}\texttt{[0]}  \qquad $\to$ \qquad     \texttt{(h)}
%	\ex \texttt{t}\textrm{@}\texttt{[2]}   \qquad  $\to$   \qquad       \texttt{(t)}
%	\ex \texttt{t}@\texttt{[-1]} \qquad  $\to$   \qquad  \texttt{(t)}
%	\ex \texttt{u}\textrm{@}\texttt{[-3]} \qquad  $\to$   \qquad  \texttt{(u)}
\end{xlist}
%\ex \label{ex:positional-morphs-back}
%\begin{xlist}
%	\ex \texttt{m}@\texttt{[-2]}%[$\text{(}\texttt{[$i$]}
%	\ex \texttt{i}@\texttt{[-1]}
%	\ex \texttt{t}@\texttt{[-3]}
%\end{xlist}
\end{exe}

% (see section~\ref{}).
%The parentheses around the character $\alpha$
%are important for 
%%\textsc{stage 2};
%Stage 2;
%in particular, they define \emph{groups} of characters within a matching
%string, so groups that can be identified by group indices, i.e., by their positions relative to other groups
%in the matching string.
%, in substitution operations.
%matching characters if morph's regular expression can be matched 
%to a word.) 

Precedence features, as discussed in chapter~\ref{ch:experi}, take the form
\begin{equation*}
\alpha\text{<}\beta
\end{equation*}
That is, ``$\alpha$ precedes $\beta$ within $\delta$ characters,'' where $\delta$ is 
a parameter set once for an entire experimental trial, and thus within a given trial, all precedence features
have the same $\delta$.  In particular, $\delta$ is the difference between the 
index of $\beta$ and the index of $\alpha$ (see section~\ref{sec:invariant-features}).
Each precedence feature is mapped onto an atomic regular expression composed of the following parts:
the character $\alpha$ in parentheses, zero or more instances of the sequence \texttt{.?},  and finally
the character $\beta$ in parentheses. In regular expressions, the symbol \, \texttt{.} \,means ``any character," and 
\, the symbol \texttt{?} \, means ``up to one (i.e., zero or one) instance of the preceding symbol. The sequence \, \texttt{.?} \, 
thus means``zero or one instances of some unspecified character.'' These definitions are summarized in figure~\ref{fig:regex-defs}.
\begin{figure}[ht]
\begin{mdframed}
\begin{tabbing}
 \hspace{1in}  \=  \hspace{5.5in} \kill
\texttt{.} \> any character\\
\texttt{?} \> up to 1 (i.e., 0 or 1) instance(s) of the preceding symbol \\
\texttt{.?} \> up to 1 instance of \emph{any} character\\
%\>\texttt{.?} \> means \> ``0 or 1 instance(s) of \emph{any character}'' \\
\texttt{i.?u}  \>\texttt{i} followed by \emph{up to 1} unspecified character(s), followed by \texttt{u}\\
\texttt{i.?.?u}  \>\texttt{i} followed by \emph{up to 2} unspecified characters, followed by \texttt{u}\\
\texttt{i.?.?.?u}  \>\texttt{i} followed by \emph{up to 3} unspecified characters, followed by \texttt{u}\\
\end{tabbing}
\label{fig:regex-defs}
\caption{Regular-expression building blocks for atomic precedent features. Note that in these definitions, an \emph{unspecified
character} is simply a ``nonspecific'' character and is thus synonymous with ``any character.'' }
\end{mdframed}
\end{figure}
%of any character," and \texttt{i.?u} (for example) means
%``\textit{i} followed by \emph{up to one} instance of \emph{any} character, followed by \textit{u}.

The number of \texttt{.?} sequences is always $\delta - 1$. Thus, if $\delta = 2$, for example, all atomic
features derived from precedence features would have $\delta - 1 \mathbf{= 1}$ 
instance of \texttt{.?}, as in the expression  \,\texttt{i.?u}.
%\begin{equation*}
%\texttt{i.?u}
%\end{equation*}
 % e.g.,  and would thus follow the template $\alpha$\texttt{.?}$\beta$. 
Similarly, if $\delta = 4$, there would be $\delta - 1 \mathbf{= 3}$ instances 
of {.?}, as in the expression \,\texttt{i.?.?.?u}. %yielding the template $\alpha$\texttt{.?}\texttt{.?}\texttt{.?}$\beta$. 
In general, therefore, the mapping of precedence features onto atomic regular expressions 
follows the rule
\begin{equation}\label{precedence-to-regex}
\alpha\text{<}\beta \qquad \mapsto \qquad \texttt{(}\alpha \texttt{)}\,\, {\{\texttt{.?}\}}^{\delta-1} \,\, \texttt{(} \beta \texttt{)}
\end{equation}
where ${\{\texttt{.?}\}}^{\delta-1}$ denotes ${\delta-1}$ instances of the literal regular expression ``\texttt{.?},'' as described above. 
The lefthand side of rule \eqref{precedence-to-regex} represents the feature itself, while the righthand side is a general regular-expression template, similar to the template for positional atomic features in rule \eqref{eq:positional-to-regex} above.
 %${\{\texttt{.?}\}}^{\delta-1}$ denotes ${\delta-1}$ instances of ``\texttt{.?},'' as described above. 
%\begin{equation*}
%\texttt{(}\alpha \texttt{)}\,\, {\texttt{.?}}^{\delta-1} \,\, \texttt{(} \beta \texttt{)}
%\end{equation*}
%where $\alpha$ and $\beta$ are characters, and $\delta$ is the maximum 
%difference allowed 
%between index of $\beta$ and that of $\alpha$ within the word in question. 
%In the regular expression, the sequence \texttt{.?} is repeated ${\delta-1}$ times, 
%where \texttt{.} is the wildcard character and \texttt{?} makes the preceding item optional.  
%For example, the regular expression \texttt{i.?u} means that one character may 
%(or may not) occur between the \texttt{i} and \texttt{u}, while \texttt{i.?.?u} allows 
%for \emph{up to} two intervening characters; i.e., zero, one, or two characters may 
%occur between the  \texttt{i} and \texttt{u}. 

%$\delta = 1$, the sequence \texttt{.?} does not appear in the regular expression.  repeated  depends on the value of the parameter $\delta$; in 
%particular the number of \texttt{?} is equal to $\delta- 1$. Thus, 
%$\delta$ values of 1, 2, and 3 correspond to zero, one, and two ``\texttt{.?}"s,
% respectively. 

If a cluster's centroid has more than one active feature, each
is initially mapped onto its own atomic regular expression.
%atomic regular expression is derived from each such feature, 
%and 
However, if two atomic regular expressions are compatible with each other, %they are combined
%to 
 %the features are compatible, the atomic regular expressions 
they are combined to create a composite expression. For instance, two 
positional features are compatible if their indices are consecutive, 
as they are in the pair \texttt{h@[0]} and \texttt{a@[1]}. These two features would be
merged as follows: 
\begin{centering}
\texttt{h@[0]}, \texttt{a@[1]} $\quad \mapsto \quad$ \texttt{(h)(a)}
\end{centering}
%to form the composite expression \, \texttt{(h)(a)} \,. 
Precedence features 
can also create composite regular expressions. For example, the 
features \, \texttt{d<b}\,  and \, \texttt{b<r} \, would together yield the 
composite expression \, \texttt{(d).?(b).?(r)} \, (that is, if $\delta = 2$). 
In general, two precedence features $\alpha$<$\beta$ and $\gamma$<$\delta$
merge if $\gamma=\delta$.
\begin{equation}
\alpha\texttt{<} \beta \, ,\,\, \gamma \texttt{<} \eta  \quad \mapsto \quad \texttt{(} \alpha \texttt{)}  \,\, 
{\{\texttt{.?}\}}^{\delta-1} \,\, \texttt{(}\beta \texttt{)} \,\,{\{\texttt{.?}\}}^{\delta-1} \, \, \texttt{(} \eta \texttt{)} \qquad \quad \text{if $\beta = \gamma$} 
\end{equation}

Each morph is represented in the four-stage process as a complex object. Here 
we mean \emph{object} in the object-oriented-programming sense, i.e., an instantiation of a \emph{class} with a certain set \emph{attributes}.
In the four-stage process, the morph class had two main attributes:
\begin{enumerate}
\item A unique \emph{morph ID}.
\item A regular expression built up through the process described above
\end{enumerate}
The morph ID was an integer, and the regular expression was a string.

The output of Stage 1 was ultimately a mapping from cluster IDs (i.e., cluster indices) to \emph{morph objects}.
Stage 1 is thus concerned only with clusters and their representation as morphs, not with words. That is,
it does not address the morphological segmentation of individual words, which can
be described as a mapping from word characters to morphs. This mapping is the purview of Stage 2,
to which we now turn.
% Each morph object specifies values for certain attributes, the main ones 
% being a morph ID (i.e., a unique integer) and a regular expression derived 
% from the features of a particular cluster centroid. Stage 1 is thus
%  concerned the clusters in and of themselves and their interpretation. 
  %Stage 2, to which we turn next, is concerned with precisely this sort of mapping.
%  mapping clusters, i.e., 
%  morphs, to the characters of actual words. 


%\begin{figure}[t]
%\centering
%\begin{subfigure}[a]{0.3\textwidth}
%\begin{tabular}{cc}
%Morph ID & Regular Expression \\ \hline
%3 & \texttt{(u)} \\
%31 &  \texttt{(f)} \\
%64 &  \texttt{(h).?.?(i)}  \\
%84 & morph ptn: \texttt{(\v{z})}  \\
%95 &  \texttt{(l).?.?(v)}  \\
%148 &   \texttt{(c).?.?(\'{a})} \\
%151  &  \texttt{(i).?.?(l)} \\
%202 &  \texttt{(i).?.?(\'{a})}  \\
%264 &  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%284 &  \texttt{(l).?.?(t)}  \\
%360 &  \texttt{(r)} \\
%\begin{figure}[t]
%\label{fig:stage2}
%	%\subfigure[List of morphs initially associated with \textit{hicl\'{a}xti}]{
%	\begin{tabbing}
%	\hspace{0.6in} \= \hspace{5.5in} \kill
%	Morph ID \> Regex \\ 
%                3 \> \texttt{(u)} \\
%                31 \>  \texttt{(f)} \\
%                64 \>  \texttt{(h).?.?(i)}  \\
%                84 \>  \texttt{(\v{z})}  \\
%                95 \>  \texttt{(l).?.?(v)}  \\
%                148 \>   \texttt{(c).?.?(\a'{a})} \\
%                151  \>  \texttt{(i).?.?(l)} \\
%                202 \>  \texttt{(i).?.?(\a'{a})}  \\
%                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%                284 \>  \texttt{(l).?.?(t)}  \\
%                360 \>  \texttt{(r)} \\
%	\end{tabbing}
%	%}
%	%\subfigure[Mapping from Morph IDs to character indices]{
%%hicl\'{a}xti \quad 64:[0,1], 264:[2,3,5,6], 202:[1,4], i:[7], 148:[2,4], 151:[1,3], 284,[3,6], 95:[3,4]
%%}
%\end{figure}

%\end{tabular}
%\caption{Morphs associated with \textit{hicl\'{a}xti} and their regular expressions.}
%\label{fig:noise-clusters}
%\end{subfigure}
%\begin{subfigure}[ hicl\'{a}xti ]{0.3\textwidth}
%hicl\'{a}xti \quad 64:\[0,1\], 264:\[2,3,5,6\], 202:\[1,4\], i:\[7\], 148:\[2,4\], 151:\[1,3\], 284:\[3,6\], 95:\[3,4\]
%\label{fig:mapping}
%\end{subfigure}
%\end{figure}

\subsubsection{Stage 2. Match morphs to words}
 
%Each of the \emph{covered} words in an MCMM clustering is a member of at least 
%one and possibly many clusters. 
The objective of Stage 2 is to map each character of each input word to a morph. % characters of input words.
Given an input word, the first step within Stage 2 was to consult its cluster-membership vector in the matrix $\mathbf{M}$ 
and thus assign to it preliminary list 
of clusters (i.e., cluster indices).  
%to each input word by consulting the word's
%cluster-membership vector within the matrix $\mathbf{M}$; 
%a list of the clusters of which the given word is a member. 
%This was accomplished by consulting the word's cluster-membership vector in the  matrix $\mathbf{M}$. 
Stage 2 then consulted the cluster-to-morph mapping computed in Stage 1 to convert each such list to a list of morph objects. %o obtain the \emph{morph object} corresponding to each cluster. 
For example, for the word \textit{hicl\'{a}xti},
this initial step produced the morphs (i.e., pairs of morph IDs and regular expressions)  listed in 
figure~\ref{fig:morph-regex}. In the cluster-membership vector associated with 
\textit{hicl\'{a}xti}, there were seven \emph{active} units, each corresponding to a cluster, and each cluster (via Stage 1) corresponded to a morph object and hence
a morph's regular expression. 
%yielded a total of seven candidate morphs, all of which are listed in figure~\ref{fig:morph-regex}. 


Stage 2 next tried to match each morph's regular expression to a subsequence of
literal characters within the word in question, 
illustrated in figure~\ref{fig:morph-regex} %llustrates the regular-expression searches 
in the case of \textit{hicl\'{a}xti}.
% These regular expression searches are also
%illustrated in figure~\ref{fig:morph-regex}. 
Notice that some of the regular expressions in figure~\ref{fig:morph-regex}---for example,
\texttt{(f)}, \texttt{(\v{z})}, and \texttt{(i).?.?(l)}---do not
match \emph{any} portion of \textit{hicl\'{a}xti}.\footnote{One might ask why such morphs were present at all among the 
candidate morphs for \textit{hicl\'{a}xti}.  Their presence was in large part a consequence of the MCMM's failure to converge, i.e., to reduce its error all the way to zero (or a number very close to zero). Because Multimorph in these experiments always terminated with some error remaining, its clusters were necessarily noisy to some extent. See chapter~\ref{ch:results} for further discussion.}
%Morphs whose regular expressions regular 
%This was due to noise in the MCMM's clustering. 
Stage 2 discarded any morph whose regular expression found no matching subsequence within the word in question to yield an initial pruning of mapping.  Figure~\ref{fig:char-indices-to-morph-IDs} thus exhibits a shortened list of morphs.
% producing somewhat more concise mappings like the one in figure~\ref{} The result  as illustrated by the strikethrough marks in figure~\ref{fig:morph-regex}.
%%the struck-through morph IDs and regular expressions found no matching subsequences in \textit{hicl\'{a}xti}
%%and were thus discarded. 
%% For each word in the input wordlist, an initial candidate set of morphs was obtained and reduced in the same % manner, producing a mapping from characters morphs (i.e., morph IDs) 
%The result was 

\begin{figure}[t]
\centering
%\subfigure[Aligning morphs to characters via morph regular expressions\label{fig:morph-regex}]{
%\centering
\setlength{\extrarowheight}{6pt}
 \begin{tabular}{clc}
\toprule
\raisebox{0pt}[0pt]{Morph ID} & \raisebox{0pt}[0pt]{Regular Expression} & Match in \textit{hicl\'{a}xti}? \\
 \midrule
                \sout{\texttt{3}} & \sout{\texttt{(u)}} & \\
                \sout{\texttt{31}} &  \sout{\texttt{(f)}} &  \\
                \texttt{64} &  \texttt{(h).?.?(i)} & \textbf{\underline{h}\,\underline{i}\,}c\,l\,\'{a}\,x\,t\,i  \\
               \sout{\texttt{84}} &  \sout{\texttt{(\v{z})}} &  \\
                \sout{\texttt{95}} &  \sout{\texttt{(l).?.?(v)}} & \\ %h\,i\,\textbf{\underline{c}\,\underline{l}\,}\'{a}\,x\,t\,i \\
                \texttt{148} &   \texttt{(c).?.?(\'{a}\,)} & h\,i\,\textbf{\underline{c}}\,l\,\textbf{\underline{\'{a}}}\,x\,t\,i \\
                \sout{\texttt{151}}  &  \sout{\texttt{(i).?.?(l)}} &  \\
                \texttt{202} &  \texttt{(i).?.?(\'{a}\,)} & h\,\textbf{\underline{i}}\,c\,l\,\textbf{\underline{\'{a}}}\,x\,t\,i \\
                \texttt{264} &  \texttt{(c).?.?(l).?.?(x).?.?(t)}  & h\,i\,\textbf{\underline{c}}\,\textbf{\underline{l}}\,\'{a}\,\textbf{\underline{x}}\,\textbf{\underline{t}}\,i \\
                \texttt{284} &  \texttt{(l).?.?(t)} & h\,i\,c\,\textbf{\underline{l}}\,\'{a}\,x\,\textbf{\underline{t}}\,i  \\
                \sout{\texttt{360}} &  \sout{\texttt{(r)}} & \\
       \bottomrule
 \end{tabular}
 \label{fig:morph-regex}
 \caption{Aligning morphs to characters via morph regular expressions}\label{fig:morph-regex}
 %} \\ \vspace{5pt}
%\subfigure[Mappings after the elimination of morphs that failed to match a regular expression.\label{fig:char-indices-to-morph-IDs}]{
%\centering
%\setlength{\extrarowheight}{5pt}
%\begin{tabular}{lcc}
%\toprule
%\text{Word} & \text{Morph ID} & \text{Character Indices} \\
%\midrule 
% \texttt{hicl\'{a}xti:} & & \\
%& \texttt{64:} & \texttt{[0,1]} \\
% & \texttt{95:} & \texttt{[3,4]} \\
%  & \texttt{148:} & \texttt{[2,4]} \\
%   & \texttt{151:} &  \texttt{[1,3]} \\
%    & \texttt{202:} &  \texttt{[1,4]} \\
% & \texttt{264:} &  \texttt{[2,3,5,6]} \\
% & \texttt{284:} & \texttt{[3,6]} \\
% & \texttt{i:} & \texttt{[7]} \\
%\bottomrule
%\end{tabular}
%}
%\label{fig:computing-mappings}
%\caption{The mapping from morph IDs to the characters (i.e., character indices) of the word
%\textit{hicl\'{a}xti} following the elimination of morphs the did not match with a regular expression}
\end{figure}

\begin{figure}[t]
\centering
\setlength{\extrarowheight}{6pt}
\begin{tabular}{ccl}
\toprule
Word & Character Index & Morph IDs\\
\midrule
\texttt{hicl\'{a}xti:} & &\\
&  \texttt{0:} & \texttt{[64]} \\
& \texttt{1:} & \texttt{[64, 151, 202]} \\
& \texttt{2:} & \texttt{[148, 264]} \\
& \texttt{3:} & \texttt{[95, 151, 264, 284]} \\
& \texttt{4:} & \texttt{[95, 148, 202]} \\
& \texttt{5:} & \texttt{[264]} \\
& \texttt{6:} & \texttt{[264, 284]} \\
& \texttt{7:} & \texttt{i} \\
\bottomrule
\end{tabular}
\label{fig:chars-to-morphs-tab}
\caption{Mapping from characters of the word \textit{hicl\'{a}xti} to morphs (i.e., morph IDs). The characters are represented by their indices within the word. Since index 7 has not been matched to any morph, it has been assigned the literal character at index 7, namely \textit{i}.} % instead of a morph ID.}
\end{figure}
%\begin{figure}[t]
%\centering
%\subfigure[Aligning morphs to characters via morph regular expressions\label{fig:morph-regex}]{
%\centering
%\setlength{\extrarowheight}{6pt}
% \begin{tabular}{clc}
%\toprule
%\raisebox{0pt}[0pt]{Morph ID} & \raisebox{0pt}[0pt]{Regular Expression} & Match in \textit{hicl\'{a}xti}? \\
% \midrule
%                \sout{\texttt{3}} & \sout{\texttt{(u)}} & \\
%                \sout{\texttt{31}} &  \sout{\texttt{(f)}} &  \\
%                \texttt{64} &  \texttt{(h).?.?(i)} & \textbf{\underline{h}\,\underline{i}\,}c\,l\,\'{a}\,x\,t\,i  \\
%               \sout{\texttt{84}} &  \sout{\texttt{(\v{z})}} &  \\
%                \sout{\texttt{95}} &  \sout{\texttt{(l).?.?(v)}} & \\ %h\,i\,\textbf{\underline{c}\,\underline{l}\,}\'{a}\,x\,t\,i \\
%                \texttt{148} &   \texttt{(c).?.?(\'{a}\,)} & h\,i\,\textbf{\underline{c}}\,l\,\textbf{\underline{\'{a}}}\,x\,t\,i \\
%                \sout{\texttt{151}}  &  \sout{\texttt{(i).?.?(l)}} &  \\
%                \texttt{202} &  \texttt{(i).?.?(\'{a}\,)} & h\,\textbf{\underline{i}}\,c\,l\,\textbf{\underline{\'{a}}}\,x\,t\,i \\
%                \texttt{264} &  \texttt{(c).?.?(l).?.?(x).?.?(t)}  & h\,i\,\textbf{\underline{c}}\,\textbf{\underline{l}}\,\'{a}\,\textbf{\underline{x}}\,\textbf{\underline{t}}\,i \\
%                \texttt{284} &  \texttt{(l).?.?(t)} & h\,i\,c\,\textbf{\underline{l}}\,\'{a}\,x\,\textbf{\underline{t}}\,i  \\
%                \sout{\texttt{360}} &  \sout{\texttt{(r)}} & \\
%       \bottomrule
% \end{tabular}
% } \\ \vspace{5pt}
%\subfigure[Mappings after the elimination of morphs that failed to match a regular expression.\label{fig:char-indices-to-morph-IDs}]{
%\centering
%\setlength{\extrarowheight}{5pt}
%\begin{tabular}{lcc}
%\toprule
%\text{Word} & \text{Morph ID} & \text{Character Indices} \\
%\midrule 
% \texttt{hicl\'{a}xti:} & & \\
%& \texttt{64:} & \texttt{[0,1]} \\
% & \texttt{95:} & \texttt{[3,4]} \\
%  & \texttt{148:} & \texttt{[2,4]} \\
%   & \texttt{151:} &  \texttt{[1,3]} \\
%    & \texttt{202:} &  \texttt{[1,4]} \\
% & \texttt{264:} &  \texttt{[2,3,5,6]} \\
% & \texttt{284:} & \texttt{[3,6]} \\
% & \texttt{i:} & \texttt{[7]} \\
%\bottomrule
%\end{tabular}
%}
%\label{fig:computing-mappings}
%\caption{The mapping from morph IDs to the characters (i.e., character indices) of the word
%\textit{hicl\'{a}xti} following the elimination of morphs the did not match with a regular expression}
%\end{figure}
%  \label{fig:morph-regex}
% \caption{Searching for matching subsequences in \texttt{hicl\'{a}xti}}
% %\caption{Searching for matching subsequences in \texttt{hicl\'{a}xti}}
%% \textit{\textsc{Stage 1}}: A regular expression is derived from each cluster's centroid vector. Each cluster corresponds to a morph, and the regular expression is thus a representation of the morph's form.}
%%  The are  is assign to each\textit{hicl\`{a}xti} is a member are \emph{Morphs detected in the word} hicl\'{a}xti. Each morph identified by a unique \emph{morph ID}. The regular expression next to each morph ID represents the morph's form.}
% \end{table}
The morph-to-character alignment depicted figure~\ref{fig:morph-regex} yielded
a mapping from character indices to morphs (i.e., morph IDs), exemplified in 
figure~\ref{fig:chars-to-morphs-tab} for \textit{hicl\'{a}xti}. Note that some characters are mapped to more than one morph. 
Stage 3 addressed the problem of simplifying complex sequences of mappings like that displayed in figure~\ref{fig:chars-to-morphs-tab}.


%one-to-many mappings to a coherent and compact sequence characters and morphs to one-to-one mappings.  to a compact morph sequence. 

% from morphs 
%(i.e., morph IDs) to characters (i.e., character indices), as shown in figure~\ref{fig:char-indices-to-morph-IDs}.
%Note the final \textit{i} at the bottom of figure~\ref{fig:char-indices-to-morph-IDs}. This is an ``orphan" alphabetic character, that is, a character that could not be matched to a morph.
%% associated with a morph. 
%Such stranded characters occurred from time to time in the output of Stage 2. 
%I decided to retain them, in effect treating them as another kind of morph. 
%%retained so that every character would be accounted for in subsequent stages. 
%It is also important to note
%that some of the morphs in %figure~\ref{fig:char-indices-to-morph-IDs} 
%figure~\ref{fig:chars-to-morphs-tab} 
%\emph{overlap}; that is, some share one or more character indices. 
%For example, 
%We will revisit the problem of overlapping morphs in our discussion of Stage 3.


Stage 2 ultimately computed mappings like the ones in figure~\ref{fig:m2c-for-each-word} 
%for each word in the input wordlist. An excerpt from these mappings is shown in figure~\ref{fig:m2c-for-each-word}. 
Notice the orphan characters in the mappings for \textit{bia\'a\v{s}ta}, \textit{naxa\v{s}\'{i}}, etc. Overlapping morphs are also evident in figure~\ref{fig:m2c-for-each-word}; for example, in the entry for \textit{bab\'{e}\textsubdot{t}en}, four different morphs, namely those with the IDs 98, 40, 13, and 48, take responsibility for the character with index 1 (i.e., the \textit{a}).

\begin{figure}[t]
\small
\begin{mdframed}
\begin{tabbing}
\hspace{1in} \= \hspace{5.5in} \kill
 \vdots \> \\
 \texttt{ba\textipa{Q}\a'{e}rev} \> \texttt{131:[5,6], 198:[1,3], 135:[1,4], 75:[5], 13:[0,1], 90:[1,2], 59:[3,5]} \\
\texttt{biq\a'{a}\v{s}ta} \> \texttt{229:[2], 103:[3,6], 202:[1,3], 140:[1,2], 113:[1,4], 274:[0,1], t:[5], 170:[ 2,3]} \\
\texttt{\v{s}el\a'{o}} \> \texttt{9:[0,1], 57:[1,2], 169:[3], 271:[2]} \\
\texttt{hahagig\a'{i}m} \> \texttt{64:[2,5], 1:[0,3], 4:[7,8], g:[6], 28:[1,4], 17:[1,3], 56:[3,5], 252:[7]} \\
\texttt{k\v{s}e\textsubdot{s}ix\a'{a}qt} \> \texttt{202:[4,6], 43:[0,1,2], 140:[4,7], 337:[1,4], 243:[5,6], 55:[3,5,7], 61:[6,8], 414:[2,5]} \\
\texttt{bab\a'{e}\textsubdot{t}en} \> \texttt{198:[1,3], 40:[1,4], 75:[5], 13:[0,1], n:[6], 48:[1,2], 276:[2,5], 59:[5]} \\
\texttt{naxa\v{s}\a'{i}} \> \texttt{\v{s}:[4], 163:[0,3], 147:[2,3], 227:[2,5], 17:[1,3], 19:[2]} \\
\texttt{habanq} \> \texttt{48:[1,2], 1:[0,3], 71:[1,4], 69:[3,5], 17:[3]} \\
\texttt{q\a'{a}qa\textipa{P}im} \> \texttt{96:[4], 100:[0,3], 263:[1,3], 265:[2,5], 170:[0,1], 41:[3,6], 56:[3,5], 63:[4]} \\
%\textipa{P}etp\a'{o}r \> 283:3,5 297:2,5 \a'{o}:4 51:1 335:0,2\\
%lebdiq\a'{a} \> 6:1,4 170:5,6 140:4,5 l:0 274:2,4 86:1,3 153:3,6 185:1,2\\
%gur\a'{e}yha \> 132:2,3 h:5 394:3,6 203:0,2 u:1 y:4\\
%ba\textipa{Q}\a'{e}rev \> 131:5,6 198:1,3 135:1,4 75:5 13:0,1 90:1,2 59:3,5\\
%biq\a'{a}\v{s}ta \> 229:2 103:3,6 202:1,3 140:1,2 113:1,4 274:0,1 t:5 170:2,3\\
%\v{s}el\a'{o} \> 9:0,1 57:1,2 169:3 271:0,2\\
%hahagig\a'{i}m \> 64:2,5 1:0,3 4:7,8 g:6 28:1,4 17:1,3 56:3,5 252:5,7\\
%k\v{s}e\textsubdot{s}ix\a'{a}qt \> 202:4,6 43:0,1,2 140:4,7 337:1,4 243:5,6 55:3,5,7 61:6,8 414:2,5\\
%bab\a'{e}\textsubdot{t}en \> 198:1,3 40:1,4 75:5 13:0,1 n:6 48:1,2 276:2,5 59:3,5\\
%naxa\v{s}\a'{i} \> \v{s}:4 163:0,3 147:2,3 227:2,5 17:1,3 19:1,2\\
%habanq \> 48:1,2 1:0,3 71:1,4 69:3,5 17:1,3\\
%q\a'{a}qa\textipa{P}im \> 96:4 100:0,3 263:1,3 265:2,5 170:0,1 41:3,6 56:3,5 63:3,4\\
%\v{s}elift\a'{o}ax \> 133:2,3 6:1,3 72:3,6 9:0,1 271:0,2 337:0,3 19:7,8 22:3,5,7 57:1,2 284:2,5 221:1,4\\
%wet\a'{u}ki \> i:5 121:3 18:0,1 t:2 k:4\\
%hay\a'{a}yin \> 1:0,1 101:1,4 65:1,3 237:5,6 15:3,5 336:0,3 126:2,3\\
%qer\a'{a}vti \> 131:1,4 229:0 359:2,5 136:1,3 170:0,3 112:2,3 i:6 106:1,2 223:4,5\\
%mecayr\a'{i}m \> 2:1,3 162:5,6 101:3,4 135:3,5 c:2 10:0,1 34:0 45:3,6 177:0,3 4:6,7 27:1,4\\
%\v{s}a\textsubdot{k}\a'{a}xnu \> 160:0,3 65:1,3 152:1,2 19:1,4 36:3,5,6\\
%car \> c:0 135:1,2\\
%ni\v{s}pe\textsubdot{k}\a'{u} \> 113:1,2 331:4,5 60:0,1 157:1,3 \a'{u}:6\\
%mibad \> 177:0,3 123:0,1 b:2 35:3,4\\
%\textipa{P}amr\a'{u} \> 41:1,2 135:1,3 261:2,3 \textipa{P}:0 53:1,4\\
%garb\a'{e}y \> 132:2,4 198:1,4 135:1,2 203:0,2 48:1,3 y:5 220:0,1\\
 \vdots \> \\
\end{tabbing}
\vspace{-0.4cm}
%\end{mdframed}
\label{fig:m2c-for-each-word}
\caption{Output of Stage 2: Morph-to-character mappings for each word}
\end{mdframed}
\end{figure}
%This was done by searching the word for each morph object's regular expression. 







%[As noted earlier in section~\ref{sec:intrinsic}), in order for the $i$th word in the input wordlist to be considered a member of the $k$th cluster, there must be at least one feature 
%index $j \in J$ such that $m_{i,k}c_{i,j} \geq \theta_{\textrm{mc}}$, 
%where $m_{i,k}c_{i,j}$ is the weighted vote of the $k$th cluster (or hidden unit) 
%regarding the activity of the $j$th surface unit (or feature), namely $r_{i,j}$. 
%Thus, in order for the $i$th word to be a member of \emph{any} cluster, it 
%must have at least one active surface unit, i.e., at least one vote  $m_{i,k}c_{i,j}$ 
%(for some $k\in K$, $j \in J$) must be greater or equal to $\theta_{\text{mc}}$. 
%Sometimes there are no such votes for a given word, in which case it is not a 
%member of \emph{any} cluster. We say that such a word is not 
%not \emph{covered} by the clustering.]

 %A \emph{covered} word is thus a word for w
%f a word threshold $\theta_{\text{mc}}$ i An MCMM clustering does not necessarily include every word; that is, some words may not be an \emph{active} member of any cluster in the clustering.]
%In any case, the point is that ... In any given clustering, most clusters are likely to be noisy to some 
%extent. 

%Some words belong to many clusters, and sometimes 
%the relationships between a cluster and its member words are not especially
%clear. %Specially care 
%%In any given clustering, most clusters are likely to be noisy to some 
%%extent. 
%Sometimes the active features of a cluster do not seem match the a particular member word's 
%characters.
%%Table~\ref{tab:morph-regex}} lists the morphs; i.e., their morph IDs 
%For example, one 
%%and regular expressions, which one 
%experiment ($s = 4,\delta = 3$) associated the
%word \textit{hicl\'{a}xti} with the clusters listed in table~\ref{tab:morph-regex}. 
%Note that each cluster in this table is represented by two pieces of information: 
%a morph ID, which ensures a unique 
%identity for each morph, and a regular expression, which represents the form of the morph in question. 

%By ``associated," we mean that the word \textit{hicl\'{a}xti} 
%was member of the clusters from which these morphs were derived.
%4_3_K1000_N12272_basic_181015_00-21_k-1000.M2C_map
%4_3_K1000_N12272_basic_181015_00-21_k-1000.original_order
%2_2_K6000_N12272_basic_180621_21-24_k-1000.chinese

%\begin{exe}
%\label{ex:noise-clusters}
%	\begin{tabbing}
%	\hspace{1in} \= \hspace{5.5in} \kill
%	Morph ID \> Regex \\ 
%                3 \> \texttt{(u)} \\
%                31 \>  \texttt{(f)} \\
%                64 \>  \texttt{(h).?.?(i)}  \\
%                84 \>  \texttt{(\v{z})}  \\
%                95 \>  \texttt{(l).?.?(v)}  \\
%                148 \>   \texttt{(c).?.?(\a'{a})} \\
%                151  \>  \texttt{(i).?.?(l)} \\
%                202 \>  \texttt{(i).?.?(\a'{a})}  \\
%                264 \>  \texttt{(c).?.?(l).?.?(x).?.?(t)}  \\
%                284 \>  \texttt{(l).?.?(t)}  \\
%                360 \>  \texttt{(r)} \\
%	\end{tabbing}
%\end{exe}	
%Stage 2 must take such a list and establish from it a mapping from word characters to morphs. To this end, it considers each candidate morph's regular expression, attempting to match it with the word in question. If there is no match, the morph is discarded. For example, the expressions \texttt{(\v{z})} and \texttt{(f)} fail to match \texttt{hicl\'{a}xti}. 


\subsubsection{\textsc{Stage 3:} Compress and encode}
%In stage 3, a single, optimal path through each word is computed from Stage 2's mapping. 
%Overlapping clusters we mentioned/introduced the problem in our discussion of Stage 2. Ideally, 
%every.
%While it is true that MCMMs are capable of attributing a single feature to more than one cause (i.e., morph),
%it is also true that overlapping morphs within the same analysis are not conducive 
%to the task of morphological segmentation. 
%Moreover, as noted earlier in this chapter, Multimorph's clusterings 
%were not without noise, and thus some overlapping or redundant 
%morph-to-character mappings for any given word are likely to be
%spurious. % figure~{fig:m2c-for-each-word} 
\begin{figure}[t]
\begin{mdframed}
\centering
% Define block styles
%\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
%    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
%\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
%    text width=10em, text centered, rounded corners, minimum height=4em]
%\tikzstyle{line} = [draw, -latex']
%\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
%    minimum height=2em]
%\tikzstyle{decision} = [diamond, draw, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\begin{tikzpicture}[node distance=1.55cm, auto, >=stealth]
%	\def \rowtwoht{4.25cm}
%	\def \weightlevel{2.75cm}
%	\def \rowoneht{1.25cm}
%	\def \basement{0cm}
	\tikzstyle{annot}=[text width=15ex]
	\tikzstyle{reduced}=[text width=3ex]
	% labels
\tikzstyle{block} = [rectangle, draw, text width=1em, text centered, rounded corners, minimum height=3em, inner sep=7pt]
%\begin{tikzpicture}[node distance = 3cm, auto]
%\begin{tikzpicture}[node distance=1.7cm, auto, >=stealth]
    % Place nodes
    	%\node[annot] (hidden-layer) at (0cm,\rowtwoht) {hidden units ($\mathbf{m}_i$)};
%	\node[annot] (weights) at (0cm,\weightlevel) {weights ($\mathbf{C}$)};
%	\node[annot] (r-layer) at (0cm,\rowoneht) {reconstructed units ($\mathbf{r}_i$)};
  \node[annot](annot1) at (-1cm, 0cm){\vspace{3pt}Characters\vspace{3pt}\\Indices};
  \node[annot,below of=annot1] at (-1cm, -1.5cm)(annot2){Morph IDs};
    \node [block, right of=annot1] (idx0) {\vspace{3pt}\texttt{h}\vspace{3pt}\\\textsc{\small\textsf{\textbf{0}}}};
    \node [below of=idx0,node distance=1.35cm] (idx0-1) {64};
        \node [block,right of=idx0] (idx1) {\vspace{3pt}\texttt{i}\vspace{3pt}\\\textsc{\small\textsf{\textbf{1}}}};
    \node [below of=idx1,node distance=1.35cm] (idx1-1) {64};
    \node [below of=idx1-1,node distance=1.2cm] (idx1-2) {151};
    \node [below of=idx1-2,node distance=1.2cm] (idx1-3) {202};
            \node [block,right of=idx1] (idx2) {\vspace{3pt}\texttt{c}\vspace{3pt}\\\textsc{\small\textsf{\textbf{2}}}};
    \node [below of=idx2,node distance=1.35cm] (idx2-1) {148};
    \node [below of=idx2-1,node distance=1.2cm] (idx2-2) {264};
                \node [block,right of=idx2] (idx3) {\vspace{3pt}\texttt{l}\vspace{3pt}\\\textsc{\small\textsf{\textbf{3}}}};
    \node [below of=idx3,node distance=1.35cm] (idx3-1) {95};
    \node [below of=idx3-1,node distance=1.2cm] (idx3-2) {151};
    \node [below of=idx3-2,node distance=1.2cm] (idx3-3) {264};
       \node [below of=idx3-3,node distance=1.2cm] (idx3-4) {284};
                    \node [block,right of=idx3] (idx4) {\vspace{3pt}\texttt{\'{a}}\vspace{3pt}\\\textsc{\small\textsf{\textbf{4}}}};
    \node [below of=idx4,node distance=1.35cm] (idx4-1) {95};
    \node [below of=idx4-1,node distance=1.2cm] (idx4-2) {148};
    \node [below of=idx4-2,node distance=1.2cm] (idx4-3) {202};
                        \node [block,right of=idx4] (idx5) {\vspace{3pt}\texttt{x}\vspace{3pt}\\\textsc{\small\textsf{\textbf{5}}}};
    \node [below of=idx5,node distance=1.35cm] (idx5-1) {264};
                            \node [block,right of=idx5] (idx6) {\vspace{3pt}\texttt{t}\vspace{3pt}\\\textsc{\small\textsf{\textbf{6}}}};
    \node [below of=idx6,node distance=1.35cm] (idx6-1) {264};
    \node [below of=idx6-1,node distance=1.2cm] (idx6-2) {284};
                                \node [block,right of=idx6] (idx7) {\vspace{3pt}\texttt{i}\vspace{3pt}\\\textsc{\small\textsf{\textbf{7}}}};
    \node [below of=idx7,node distance=1.35cm] (idx7-1) {i};
%    \node[annot](red-id-1) at (-1cm, -6cm){Final:};
%       \node[right of=red-id-1,node distance=2.1cm](red-id-2){64};
%       \node[right of=red-id-2](red-id-3){148};
%       \node[right of=red-id-3](red-id-4){264};
%       \node[right of=red-id-4](red-id-5){i};
       %\node[reduced,right of=red-id-2](red-id-2) {\vspace{3pt}\texttt{\'{a}}\vspace{3pt};
%    \node [block, right of=idx0, node distance=2.5cm] (idx1) {\textsc{\small\textsf{\textbf{1}}} \\ 64 \\ 151 \\ 202 };
%    \node [block, right of=idx1, node distance=2.5cm] (idx2) {\textsc{\small\textsf{\textbf{2}}} \\ 148\\ 264};
%        \node [block, right of=idx2, node distance=2.5cm] (idx3) {\textsc{\small\textsf{\textbf{3}} \\ 95 \\ 151 \\ 264 \\ 284}; 
%    \node [block, right of=idx3, node distance=2.5cm] (idx4) {\textsc{\small\textsf{\textbf{4}}} \\ 95 \\ 148 \\ 202};
%    \node [block, right of=idx4, node distance=2.5cm] (idx5) {\textsc{\small\textsf{\textbf{5}}} \\ 264};
%    \node [block, right of=idx5, node distance=2.5cm] (idx6) {\textsc{\small\textsf{\textbf{6}}} \\ 264 \\ 284};
%        \node [block, right of=idx6, node distance=2.5cm] (idx7) {\textsc{\small\textsf{\textbf{7}}} \\ i};
    %\node [block, below of=identify] (evaluate) {evaluate candidate models};
%    \node [block, left of=evaluate, node distance=3cm] (update) {update model};
%    \node [decision, below of=evaluate] (decide) {is best candidate better?};
%    \node [block, below of=decide, node distance=3cm] (stop) {stop};
    % Draw edges
%    \path [line] (stage1) -- (stage2);
%    \path [line] (stage2) -- (stage3);
%    \path [line] (stage3) -- (stage4); 
\draw[->] (idx0-1) -- (idx1-1);
\draw[->] (idx1-1) -- (idx2-1);
\draw[->] (idx2-1) -- (idx3-3);
\draw[->] (idx3-3) -- (idx4-2); 
\draw[->] (idx4-2) -- (idx5-1); 
\draw[->] (idx5-1) -- (idx6-1); 
\draw[->] (idx6-1) -- (idx7-1); 
   %node[above] {yes} (d);
%   \draw[->] (c.north) to [out=170,in=45] node[above] {no} (b.north);
%   \draw[->] (d.south) to [out=210,in=20] (e.north);
%   \draw[->] (e) -- (f);
%   \draw[->] (f) -- (g);
%   \draw[->] (g) -- (h);
%    \path [line] (decide) -| node [near start] {yes} (update);
%    \path [line] (update) |- (identify);
%    \path [line] (decide) -- node {no}(stop);
%    \path [line,dashed] (expert) -- (init);
%    \path [line,dashed] (system) -- (init);
%    \path [line,dashed] (system) |- (evaluate);
\end{tikzpicture}
\label{fig:best-paths}
\caption{Finding the ``optimal path,'' i.e., the optimal morph sequence, through the word \textit{hicl\'{a}xti}, where morphs are represented by their IDs. A \emph{morph sequence} is a mapping from morphs to characters.}
\end{mdframed}
\end{figure}
Figure~\ref{fig:chars-to-morphs-tab} illustrates several one-to-many mappings from 
characters to morphs. Such mappings are common following Stage 2. 
One of Stage 3's objectives is to take these mappings and select for each character the single 
\emph{best} morph. The morph choice for any given character depends on the choices 
for other characters. For instance, notice in figure~\ref{fig:chars-to-morphs-tab} that the first 
character of \textit{hicl\'{a}xti} maps to morph 64, and second character maps to 
morphs  64, 151, and 202. Morph 64 is selected (by default) for the first character, 
a fact that immediately makes morph 64 the most attractive choice for the second character.
The task for selecting the best morph for each character is tantamount to selecting the 
best \emph{path} through the morphs associated with a given word. 
This is illustrated in figure, in which the set of morph IDs associated with 
each index is presented as a set of options at that index.

These choices are made with an eye to finding the path with the maximal weight. The task 
was thus essentially a kind of
\emph{best-first} search \citep{pearl:1984}. The weight of a path was derived from
 the weights of its morphs, and the weight of each morph is derived from the weights of its component features. 
%More specifically, each morph's weight is a real number between 0 and 1 derived from the activities of its component features.  is therefore given a numerical weight in the interval $[0,1]$.
%As described in the discussion of Stage 1 above, each morph is composed of one or more features. 
%-either precedence features, 
%positional features, or a combination of these two types. 
Recall from chapter~\ref{ch:MCMM}, particularly section~\ref{sec:mixing-function}, that each feature corresponds 
to a particular element $r_{i,j}$ in the \emph{reconstruction} 
vector $\mathbf{r}_{i}$. Specifically, the element $r_{i,j}$ is the \emph{activity} 
of the $j$th feature. A feature's activity is a number between 0 and 1 indicating the degree to 
which the feature is \textsc{off} or \textsc{on}, respectively. Feature activities 
are thus already ``weights'' of a sort. Since features compose morphs, we compute 
a morph's weight by taking the average of the activities of its component features. 
A path's weight is then the average of its morphs' weights. However, to favor 
sequences of identical morphs, such as the two 64s at the beginning of \textit{hicl\'{a}xti}, 
we introduce the following exception to the basic average rule: 
\begin{quote}\noindent
Given a path of morph IDs, 
whenever a morph ID is a repeated, and three or fewer character positions 
separate the two instances, the path's weight is \emph{doubled}; that is, 
it becomes \emph{two times} the average of the weights of its component morphs.
\end{quote}

Figure~\ref{fig:best-paths} illustrates the 
 best path through the morph IDs associated with the word \textit{hicl\'{a}xti}.
 We restate this optimal morph sequence here:
\begin{equation}\label{eq:best-path}. 
\texttt{hicl\'{a}xti} \quad \mapsto \quad \texttt{64, 64, 148, 264, 148, 264, 264, i}
\end{equation}
We now come to the second major objective of Stage 3, which is to abstract away from individual characters by eliminated repeated instances of morph IDs. 
The most straightforward morph sequences to deal with are uninterrupted occurrences of the same morph ID, such as the consecutive 64s in \eqref{best-path}. Stage 3 mapped consecutive instances of a morph ID onto a \emph{single} instance of the same morph ID. Thus,  
\begin{equation*}
\texttt{64, 64} \quad \mapsto \quad \texttt{64}
\end{equation*}
However, sometimes optimal paths
exhibit morph interdigitation, as does \eqref{eq:best-path}, in fact, in which morphs 148 and 248 are interleaved. 
Because Morfessor does not deal 
with interdigitation, Stage 3 collapsed sequences of interleaved morphs into two adjacent monolithic
morphs; that is, 
\begin{equation*}
\texttt{148, 264, 148, 264, 264} \quad \mapsto \quad \texttt{148, 264}
\end{equation*}
Orphaned characters such as the \textit{i} in \eqref{eq:best-path} 
were retained in place. The abstracted, or compressed, version of path \eqref{eq:best-path} thus turns out to be
\begin{equation}\label{eq:reduced-best-path}
\texttt{hicl\'{a}xti} \quad \mapsto \quad \texttt{64, 148, 264, i}
\end{equation}
The final step within Stage 3 is to replace the morph IDs in \eqref{eq:reduced-best-path} and other such compressed paths with atomic unicode characters. Each morph ID occurring in the compressed paths was uniquely mapped to a distinct unicode character---in particular, a character 
from the CJK unicode block. The mappings for morph IDs 64, 148, and 264 
were as in figure~\ref{tab:morphs-chinese}.
\begin{figure}[t]
\setlength{\extrarowheight}{8pt}
\centering
\begin{tabular}{ccc}
\toprule
Morph ID & & CJK char \\
\midrule
64 & $\mapsto$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-2}}\\
148 & $\mapsto$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-1}} \\
264 & $\mapsto$ & \raisebox{-0.12cm}{\includegraphics[scale=.3]{cjk-char-0-0}} \\
%i & $\mapsto$ & i \\
\bottomrule
\end{tabular}
\label{tab:morphs-chinese}
\caption{Mapping morphs to atomic symbols from the CJK unicode block}
\end{figure} 
These CJK characters and the orphaned \textit{i} were then joined together to form a single string as follows:
%\begin{figure}[h]
\begin{center}
\raisebox{-0.1cm}{\includegraphics[scale=0.4]{reduced-morph-IDs}} \quad $\mapsto$ \quad \raisebox{-0.1cm}{\includegraphics[scale=0.4]{cjk-sequence}} \quad $\mapsto$ 
\quad \raisebox{-0.10cm}{\includegraphics[scale=0.4]{conjoined-cjk-sequence}}
\end{center}
The experimental input files to Morfessor consisted of strings like \raisebox{-0.10cm}{\includegraphics[scale=0.4]
{conjoined-cjk-sequence}}. Figure~\ref{fig:morfessor-input} shows an excerpts from one of these input files.
%Each morph was thus treated as a single, atomic entity, with no internal structure. 
%The last step in Stage 3 is to \emph{encode} the morph-ID sequence 
%in example (\ref{ex:comp-no-indices}) by replacing each
%ID number with a unique, atomic unicode character---in particular, a character 
%from the CJK unicode block, as illustrated in figure~\ref{fig:map-and-replace}.
%The resulting sequence was then joined together to form a single string, as in 
%subfigure~\ref{subtab:cjk-replacement}.
%Otherwise, repeated instances the same morph ID were straightforwardly mapped onto a \emph{single} 
%instance of the morph ID; for example,
%\begin{equation*}
%64, 64 \mapsto 64
%\end{equation*}
%
%\begin{equation*}
%64, 148, 264, i
%\end{equation*}

%repeated morph ID results in a doubling of the path's weight as long as the morph ID's immediately preceding instance is within three positions of the repeated instances
%
% Given a path of morph IDs, if a morph ID is repeated in the path, and if its recurrence is 3 or more  the  same morph ID is recurs within or fewer character positions let $\mu$ be one of morph IDs at index $n$. If $\mu$ is also present at indices $n-3$, $n-2$, or $n-1$ (i.e., within three preceding positions), the weight of this path is \textit{doubled}. That is, it is \emph{two times} the average of its component morphs.
%
%and/or positionfeatures weighting to morphs.  in which each feature (i.e., surface unit) was inherently 
%weighted by its reconstructed activity (see section~\ref{sec:mixing-fiction}), 
%and each morph's weight was then computed as the average of the activities of its component features.
%The sequence consists of a list of pairs; eachcharacter in a given word tio the morphs that 
%def update\_wt(weighted_sequence, morphID):
%		try: int\_ID = int(morphID)
%		except TypeError:
%			morph_obj = morphID
%			item_wt = 1.0
%		except ValueError:
%			morph_obj = morphID
%			item_wt = 1.0
%		else:
%			morph\_obj = morph\_dict[int\_ID][-1]
%			item\_wt = morph\_obj.get\_weight()
%		seq\_wt = weighted\_sequence[0]
%		sequence = weighted\_sequence[-1]
%		new\_avg = (seq_wt+item\_wt)/2.0
%		for n in range(1,3):
%			try: item = sequence[-n]
%			except IndexError: 
%				break
%			else:
%				if item == morphID:
%					return 2.0*new\_avg
%		return new\_avg
		
%\begin{figure}[ht]
%\centering
%\setlength{\extrarowheight}{8pt}
%\begin{tabular}{lcc}
%\toprule
%\text{Word} & \text{Morph ID} & \text{Char Indices} \\
%\midrule 
% \texttt{hicl\'{a}xti:} & & \\
%& \texttt{64} & \texttt{[0,1]} \\
%%\texttt{148:[2,4]} \texttt{[2,3,5,6]} \\
%%\texttt{64:[0,1]} \texttt{151:[1,3]} \texttt{148:[2,4]} \\ \texttt{284:[3,6]} 
%%\texttt{64:[0,1]} \texttt{2o2:[1,4]} \texttt{264:[2,3,5,6]}
%%\texttt{64:[0,1]} \texttt{148:[2,4]} \texttt{284:[3,6]}
% & \texttt{95:} & \texttt{[3,4]} \\
%  & \texttt{148:} & \texttt{[2,4]} \\
%   & \texttt{151:} &  \texttt{[1,3]} \\
%    & \texttt{202:} &  \texttt{[1,4]} \\
% & \texttt{264:} &  \texttt{[2,3,5,6]} \\
% & \texttt{284:} & \texttt{[3,6]} \\
% & \texttt{i:} & \texttt{[7]} \\
%\bottomrule
%\end{tabular}
%\label{fig:computing-mappings}
%\caption{Finding the optimal morph path.}
%\end{figure}



%We did not want overlapping segments within a single analysis; that is, within the same analysis,
%each character index 
%\begin{algorithm}[h]
%\KwData{\textbf{M}, \textbf{C}, \textbf{X}, \textbf{R}}
%\KwResult{Optimized \textbf{M} and \textbf{C} matrices}
%def update\_weight(self):
%		sum\_wt = 0.0
%		for my\_fwp in self.fwp\_list:
%			sum_wt += my\_fwp.get\_weight()
%		%try: self.weight = sum_wt/float(len(self.fwp_list))
%		%except ZeroDivisionError: self.weight = 0.0
%\label{alg:update-wt}
%\caption{The weight of a morph is computed as the average of the activities of its  component features.}
%\end{algorithm}
		
%def avg\_wt(morphID\_list, morph\_dict):
%	s = 0.0
%	\#print "avg_wt"
%	morph_objs = list\_of\_morph\_objs(morphID_list, morph_dict)
%	for morph_obj in morph_objs:
%		s += morph\_obj.get\_weight()
%	try: quotient = s/float(len(morph\_objs))
%	except ZeroDivisionError: return 0.0
%	else: return quotient

%STEPS: [(0, [64]), (1, [64, 151, 202]), (2, [148, 264]), (3, [95, 151, 264, 284]), (4, [95, 148, 202]), (5, [264]), (6, [264, 284]), (7, u'i')]
	
%In the case of (\ref{ex:mapping}), for example,-->  
%some morphs can be eliminated because they share character indices with other morphs. 
%In fact, redundant, overlapping morphs
%\emph{must} be reduced as much as possible. 

% After eliminating morphs \texttt{151}, \texttt{202}, and \texttt{284}, 
%we arrive at the following compressed sequence of morph IDs computed at the end of Stage 2
%is optimized and compressed to yield a single best path through the word. 
% the best sequence or ``path" and then compress it.
 %That is, remove repeated instances of the same morphID.
% \begin{figure}[ht]
% %\subfigure[(Reduced) mappings from morphs to characters\label{fig:char-indices-to-morph-IDs}]{
%\centering
%\setlength{\extrarowheight}{5pt}
%\begin{tabular}{lcc}
%\toprule
%\text{Word} & \text{Morph ID} & \text{Char Indices} \\
%\midrule 
% \texttt{hicl\'{a}xti:} & & \\
%& \texttt{64:} & \texttt{[0,1]} \\
% & \sout{\texttt{95:}} & \sout{\texttt{[3,4]}} \\
%  & \texttt{148:} & \texttt{[2,4]} \\
%   & \sout{\texttt{151:}} &  \sout{\texttt{[1,3]}} \\
%    & \sout{\texttt{202:}} &  \sout{\texttt{[1,4]}} \\
% & \texttt{264:} &  \texttt{[2,3,5,6]} \\
% & \sout{\texttt{284:}} & \sout{\texttt{[3,6]}} \\
% & \texttt{i:} & \texttt{[7]} \\
%\bottomrule
%\end{tabular}
%%}
%\label{fig:computing-mappings}
%\caption{The mapping from morph IDs to the characters (i.e., character indices) of the word
%\textit{hicl\'{a}xti} following the elimination of unmatched morphs}
%\end{figure}

%\begin{exe} \ex \label{ex:comp-with-indices} 
%hicl\'{a}xti \quad \texttt{64:[0,1], 148:[2,4], 264:[2,3,5,6], i}
%\ex %\includegraphics[width=.3\linewidth]{cjk-mappings-0}
%\includegraphics[scale=.3]{cjk-mappings-0}
%\end{exe}

%\begin{figure}[h]
%\centering
%%\includegraphics[width=.3\linewidth]{example-image}\quad
%\subfigure[Segmentation: Morphs and their constituent characters]{ 
%\centering
%\includegraphics[width=.3\linewidth]{cjk-mappings-0}
%} \\[\baselineskip]
%\subfigure[Morphs mapped to CJK characters]{ 
%\centering
%\qquad \qquad \includegraphics[width=.3\linewidth]{cjk-mappings-2}
%%\quad\includegraphics[width=.3\linewidth]{example-image-b}
%%\\[\baselineskip]% adds vertical line spacing
%%\includegraphics[width=.3\linewidth]{example-image}\quad\includegraphics[width=.3\linewidth]{example-image-a}\quad\includegraphics[width=.3\linewidth]{example-image-b}
%}
%\label{tab:morphs-chinese}
%\caption{Mapping morphs to atomic symbols from the CJK block of unicode characters}
%\end{figure}

%Note the \textit{i} at the end of the sequence. This is an ``orphan" alphabetic character that was never associated with a morph. Such stranded characters were not uncommon. They were retained so that every character would be accounted for.
%The idea is to abstract away from the individual characters as much as possible and allow morph to be a single, atomic entity, with no internal structure.] 
%The purpose of this stage, however, is to abstract away from the individual characters and treat each morph
%%as much as possible, so that each morph could be treated 
%as a single, atomic entity, with no internal structure.
%Thus, the compressed sequence in (\ref{ex:comp-with-indices}) would be look more like the following:
%\begin{exe} \ex \label{ex:comp-no-indices} 
%hicl\'{a}xti \qquad  $\to$ \qquad \texttt{64, 148, 264, i}
%\end{exe}
%The last step in Stage 3 is to \emph{encode} the morph-ID sequence in example (\ref{ex:comp-no-indices})by replacing each
%ID number with a unique, atomic unicode character---in particular, a character 
%from the CJK unicode block, as illustrated in figure~\ref{fig:map-and-replace}.
%The resulting sequence was then joined together to form a single string, as in 
%subfigure~\ref{subtab:cjk-replacement}.

% and then joining the characters together to form 
%a string, e.g., a four-character string in the case of (\ref{ex:comp-no-indices}).

%\begin{table}[ht!]
%\setlength{\extrarowheight}{6pt}
%\centering
%\begin{tabular}{ccc}
%\toprule
%Morph ID & & CJK char \\
%\midrule
%64 & $\to$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-2}}\\
%148 & $\to$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-1}} \\
%264 & $\to$ & \raisebox{-0.12cm}{\includegraphics[scale=.3]{cjk-char-0-0}} \\
%i & $\to$ & i \\
%\bottomrule
%\end{tabular}
%\label{tab-cjk-mapping}
%\caption{Replacing morph (i.e., morph IDs) with CJK chars in the word \textit{hicl\'{a}xti}}
%\end{table}

%\begin{figure}[t]
%\centering
%\setlength{\extrarowheight}{6pt}
%\subfigure[Mapping\label{subtab:cjk-mapping}]{
%\centering
%\begin{tabular}{ccc}
%\toprule
%Morph ID & & CJK char \\
%\midrule
%64 & $\to$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-2}}\\
%148 & $\to$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-1}} \\
%264 & $\to$ & \raisebox{-0.12cm}{\includegraphics[scale=.3]{cjk-char-0-0}} \\
%i & $\to$ & i \\
%\bottomrule
%\end{tabular}
%} \\
%%\label{tab-cjk-mapping}
%\subfigure[In Stage 3, the morphs in the word \textit{hicl\'{a}xti} are replaced by atomic CJK characters.\label{subtab:cjk-replacement}]
%{
%\centering
%%\begin{tabular}{ccc}
%%\toprule
%%Morph ID & & CJK char \\
%%\midrule
%%hicl\'{a}xti & $\to$ & hicl\'{a}xti \\
%\includegraphics[scale=.3]{cjk-mappings-2}
%%64 & $\to$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-2}}\\
%%148 & $\to$ & \raisebox{-0.13cm}{\includegraphics[scale=.3]{cjk-char-0-1}} \\
%%264 & $\to$ & \raisebox{-0.12cm}{\includegraphics[scale=.3]{cjk-char-0-0}} \\
%%i & $\to$ & i \\
%%\bottomrule
%%\end{tabular}
%}
%\label{tab:map-and-replace}
%\caption{Subfigures~\ref{subtab:cjk-mapping} and \ref{subtab:cjk-replacement}, baby!}
%\end{figure}

%\begin{figure}[t]
%\centering
%%\includegraphics[width=.3\linewidth]{example-image}\quad
%\subfigure[Segmentation: Morphs and their constituent characters]{ 
%\centering
%\includegraphics[width=.3\linewidth]{cjk-mappings-0}} \\
%[\baselineskip]
%\subfigure[Morphs mapped to CJK characters]{ 
%\centering
%\qquad \qquad \includegraphics[width=.3\linewidth]{cjk-mappings-2}}
%%\quad\includegraphics[width=.3\linewidth]{example-image-b}
%%\\[\baselineskip]% adds vertical line spacing
%%\includegraphics[width=.3\linewidth]{example-image}\quad\includegraphics[width=.3\linewidth]{example-image-a}\quad\includegraphics[width=.3\linewidth]{example-image-b}
%\end{figure}

\begin{figure}[t]
%\centering
\begin{mdframed}
\vdots \vspace{6pt}
\includegraphics[scale=0.3]{input-to-morfessor-1} \\
\vdots
\label{fig:morfessor-input}
\caption{Excerpt from a Morfessor input file} 
\end{mdframed}
\end{figure}



%I used the characters of the vast CJK unicode block for this purpose, 
%but in principle any set of characters could be used, as long as they are atomic and there is a distinct symbol for each morph ID. 
%there is a sufficiently large number of them s
%no character is used twice. For the sake of the present discussion, we shall 
%call this compressed and encoded dataset the \emph{experimental} set, as it is 
%the experimental dataset \emph{within} the extrinsic evaluation. That is, it is 
%the experimental input to Morfessor in Stage 4 (see below). 

%We must note here that there was a \emph{control} dataset for each experimental 
%dataset (and an experimental dataset for each combination of the $s$ and $\delta$ 
%parameters. Each control set consisted of the same words as its corresponding experimental 
%set, except that the control words were the original, untouched words. Ten percent of the 
%original words had been previously selected at random. These were segmented manually in 
%order to serve as a gold-standard file. 
%  ten percent sample of the data was previous %, having undergone none of the stage 

%< Run morfessor on the control wordlist. This list contains the same words as the compressed dataset, except they are not compressed.>
 
\subsubsection{\textsc{stage 4:} Decompress and evaluate}
 In Stage 4, the final stage, %Morfessor was actually run on 
 Morfessor was fed both the experimental and control files for each experimental trial---i.e.,
 each combination of $\delta$ and $s$ values (see chapter~\ref{ch:experi}).
 %were fed to Morfessor
 %each of these files is in turn fed to Morfessor as input, 
Morfessor then output a \textit{segmentation} file for both types of input file; that is, it
produced both an experimental and a control segmentation for 
each $\delta$ and $s$ valuation. 
%i.e. both segmentation model and a control segmentation model. 
Figure~\ref{fig:morfessor-output} 
shows an excerpt from an 
experimental segmentation file. The `+' signs indicate 
the boundaries between the morphological segments that Morfessor induced. 
Note that many of the
Morfessor-induced segments in figure~\ref{fig:morfessor-output} consist of multiple 
CJK characters. Since each CJK character corresponds to a morph, 
this means that many of the Morfessor-induced segments consisted of multiple
MCMM-induced morphs.

\begin{figure}[t]
%\centering
\begin{mdframed}
\vdots \vspace{6pt}
\includegraphics[scale=0.3]{output-from-morfessor-0} \\
\vdots
\label{fig:morfessor-output}
\caption{Excerpt from a Morfessor output file}
\end{mdframed}
\end{figure}
 %Morfessor then output an experimental and a control model. 
%  each representing a segmentation model), one control and one experimental, the latter still consisting of encoded words, except that the encoded words are now segmented.
% original (or ``normal") words, while the other comprised segmentation of the encoded versions of the original words. 
 %Note, however, that even the encoded words of the experimental file had started out as original words, they so altered in their encoded forms that they generally bore no resemblance to their original forms.
% described  computes a segmentation for each compressed/encoded word in the experimental set. We also run Morfessor on the control dataset, thus obtaining segmentations of the original words. We at this point have two segmented files, but the words in the experimental file are still encoded. 
%A ten percent sample of the data was previous  10 percent sample of these segmented compressed words and \emph{decompress} them, i.e.,
Both the experimental and control models had to be evaluated against the same gold standard.
But to make this possible, Stage 4 first had to \emph{decode} (or \emph{decompress}) the strings in the experimental segmentation file; that is, it had to take experimental-string segmentations like the ones in 
figure~\ref{fig:morfessor-output} and convert the CJK characters back to the original character sequences.
%an excerpt of such a file is shown in figure~\ref{fig:morfessor-output}. To decode strings like the ones in
%figure~\ref{fig:morfessor-output}, the 
%To evaluate both files against the same gold-standard, the experimental file 
%had to be decoded. 
To avoid altering Morfessor’s segmentation decisions, 
each morfessor-induced segment 
%(consist of multiple morphs) 
was decoded separately. The decoded segments were then reassembled. 
Morfessor's built-in evaluation utility was used to evaluate each model against the gold standard. 
For example, 
in the case of \textit{hicl\'{a}xti}, our example word throughout this chapter, 
Morfessor was given the encoded string 
\begin{center}
\raisebox{-0.08cm}{\includegraphics[scale=0.4]{conjoined-cjk-sequence}}
\end{center}
and output the \emph{segmented} encoded string
\begin{center}
\raisebox{-0.08cm}{\includegraphics[scale=0.37]{segmented-encoded-string}}
\end{center}
%\raisebox{-0.08cm}{\includegraphics[scale=0.4]{conjoined-cjk-sequence}}
%\end{center}
%%\includegraphics[scale=0.4]{conjoined-cjk-sequence}
%(\ref{fig:morfessor-input}), which was four symbols long, and output a 
%segmentation consisting of two segments, 
%with the delimiter placed just 
%before the \textit{i}. 
%The first segment thus comprised the three
The first segment comprised the three  
CJK symbols corresponding to to the morphs 64, 148, and 264, 
and the second segment was the \emph{i}. In the final decoding process, 
the morphs 64, 148, and 264 together map to the substring \textit{hicl\'{a}xt}, 
and thus Morfessor's segmentation becomes \textit{hicl\'{a}xt + i}.
%the the preserving Morfessor's segmentations/segments). 
%[example]
%The reason we that now have to decompress these words (segmentations) is that we need to evaluate them and the control segmentations against the same gold standard segmentations. The experimental segmentations thus need to be comparable to the control segmentations; i.e., they at least need to be composed of roughly the same characters. 

%\subsubsection{Stage 1: Identify the morph characters} The first stage is to map each cluster's set of active features to a particular sequence of alphabetic characters. This sequence is regarded as the \emph{morph} corresponding to the cluster. Also in this stage, each morph is labeled as a prefix, stem-component, or suffix.
%
%\begin{enumerate}
%  \item Mapping from features to root or pattern characters:
%    \begin{enumerate}
%   	\item Only precedence [and bigram features] can map to root and pattern characters. Positional features never can.
%   	\item There must be at least one precedence feature [or one bigram feature] for each root-character bigram. (Note that precedence features are themselves basically bigrams.) These features must also overlap; e.g., the features \texttt{a<b} and \texttt{b<c}, which overlap at \textit{b}, indicate the root \textit{a.b.c}. %Note that roots can also be indicated by bigram features as well as combinations of bigram and precedence features (e.g., \texttt{a+b} and \texttt{b<c}).
%     \end{enumerate}

%   \item Mapping from features to prefix characters:
%   \begin{enumerate}
       %\ex \label{ex-1a} 
%       \item If there are at least two features of the form \texttt{a<b} and \texttt{a<c}, such that \textit{b} $\ne$ \textit{c}, then \textit{a} is at least part of a prefix. But what if the precedence features have an abstract component, as in the following?
%       \begin{itemize}
%       \item \texttt{x<C}
%       \item \texttt{x<V}
%       \end{itemize}
%       How does one determine inequality between abstract characters? Answer: C $\ne$ V. So there can still be inequality, just less of it. 
       %If there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, such that \textit{t} $\ne$ \textit{b}, \textit{t} $\ne$ \textit{c} (see above), then the prefix is either \textit{at-} or \textit{ta-}, respectively. 
%       \ex When there is a character \textit{a} satisfying rule \ref{ex-1a} has more than one character can only be determined by \emph{bigram} features. In particular, if there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, such that \textit{t} $\ne$ \textit{b}, \textit{t} $\ne$ \textit{c} (see above), then the prefix is either \textit{at-} or \textit{ta-}, respectively.
       %\ex 
%       \item If there is at least one positional feature of the form \texttt{a@[}$x$\texttt{]}, where $x$ is a positive integer, then \textit{a} is at least part of a prefix. If there are two or more consecutive positional features, i.e., features like \texttt{a@[}$x$\texttt{]}, \texttt{b@[}$x+1$\texttt{]}, \texttt{c@[}$x+2$\texttt{]}, and so on, then the prefix is the entire string of characters indicated by these features.
%    \end{enumerate}
%
%  \item Mapping from features to suffix characters:
%   \begin{enumerate}
%   \item If there are at least two features of the form \texttt{b<a} and \texttt{c<a}, such that \textit{b} $\ne$ \textit{c}, then \textit{a} is at least part of a suffix. If there is additionally a bigram feature \texttt{a+t} or \texttt{t+a}, where \textit{t} is a different character than either \textit{b} or \textit{c} (see above), then the prefix is either \textit{-at} or \textit{-ta}, respectively.
%
%   \item If there is at least one positional feature of the form \texttt{a@[}$x$\texttt{]}, where $x$ is a negative integer, then \textit{a} is at least part of a suffix. If there are two or more consecutive positional features, i.e., features like \texttt{a@[}$x${]}, \texttt{b@[}$x-1${]},\texttt{ c@[}$x-2${]}, and so on, then the suffix is the entire string of characters indicated by these features.
%   \end{enumerate}
%  \item If a cluster centroid has conflicting active features, then the cluster is void; it does not correspond to any morph.
%\end{enumerate}
%%EXAMPLES
%
%\begin{exe}
%%Cluster #0 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active features: e+w (1.0), e+i (1.0), l@[0] (1.0), e@[0] (1.0), e$<$w (0.9713), l+w (0.9466), e$<$i (0.8878), l<w (0.7655), e@[1] (0.7321), e+t (0.7049) \\
%Morph: None \\
%Pertinent rule: 4
%%
%%Cluster #2 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active feature: w+i (1.0), l+i (1.0), i+m (1.0), i+i (1.0), w$<$i (1.0), i@[-2] (1.0), m@[-1] (1.0), l<i (0.9941), i<i (0.8206), l+m (0.7164) \\
%Morph: -liim (suffix) \\
%Pertinent rule: 3b
%%
%%Cluster #5 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%%Active features: m+i (1.0), i+m (1.0), m<i (1.0), i@[-2] (1.0), m@[-1] (1.0), m+m (0.9825), m@[0] (0.8077), w+m (0.7786), m+w (0.7006), m<m (0.6840)
%%Morph: None
%%Pertinent rule: 4
%%
%%Cluster #6 from 3_3_1_K-50_N-6888_2015-01-24_14-48
%\ex Active features: b@[0] (1.0), b$<$w (0.9285), b$<$i (0.9232), b+t (0.6000), b+r (0.5867), b$<$t (0.4608), b+m (0.4576), b@[1] (0.4512) \\
%Morph: b- (prefix) \\
%Pertinent rules: 2a and 2b 
%%
%%Cluster #13 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: p$<$q (1.0), p$<$d (0.9999), q$<$d (0.9999) \\
%Morph: p.q.d. (root)  (The feature p*d is not relevant, as it turns out.) \\
%Pertinent rule: 1b
%%
%%Cluster #1 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: w$<$t (1.0), w@[-2] (1.0), t@[-1] (1.0) \\
%Morph: -wt (suffix) \\
%Pertinent rule: 3b
%%
%%Cluster #26 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: r$<$i (0.99), r@[-3] (0.98), r+i (0.96), i@[-2] (0.9), i+r (0.84), r+m (0.8), h+r (0.8), r$<$m (0.79), r+t (0.74), w+r (0.68) \\
%Morph: None (There are active features for both a prefix and a suffix.) \\
%Pertinent rule: 4
%%
%%Cluster #184 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active feature: p$<$g (1.0) \\
%Morph: None \\
%Pertinent rule: 1b 
%%
%%Cluster #201 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: x$<$z (1.0), z$<$q (0.98) \\
%Morph: x.z.q. (root) \\
%Pertinent rule: 1b
%%
%%Cluster #219 from 4_star_K-350_N-_2014-07-20_03-45.K@303
%\ex Active features: d$<$h (1.0), h$<$z (1.0), z$<$d (0.99), z$<$t (0.99) \\
%Morph: None \\
%Pertinent rule: 4
%\end{exe}
%
%Via Chinese: Every symbol in an encoded morfessor segment is itself a distinct morph.
%
%\subsubsection{Stage 2: Match morph characters to word characters} Once a morph has been gleaned from a cluster's centroid vector, the characters of the morph must be matched to the corresponding characters in the cluster's member words. 
%
%The $\mathbf{M}$ matrix contains the cluster activities for each word. By consulting $\mathbf{M}$, we can ascertain the set of clusters to which each cluster belongs. Each cluster can be thought of as essentially equivalent to a particular morph, since the identities of morphs come from clusters. Indeed, Stage 1 ``extracts" from each cluster a $K$-length vector that  contains the key alphabetic characters associated with the cluster. Thus, via the cluster activities, each word is mapped to its set of morphs.
%
%At this point we will know which morphs are associated with each word, but we do not yet know the order of the morphs in words with more than one. Additionally, we will know, for each morph, whether it is a prefix ($P$), suffix ($SU$), or stem-component ($ST$). For example, suppose that the word \textit{whxlwm} (`and the dream') belongs to four clusters, namely those corresponding to the morphs \texttt{w}_{P}, \texttt{h}_{P}, \texttt{xlm}_{ST}, \texttt{w}_{ST}, where the subscripts $P$ and $ST$ stand for \textit{prefix} and \textit{stem-component}, respectively.
%
%First, the prefixes are matched to word characters. Then stem-components are matched, and finally any suffixes are matched (there are no suffixes in the present example). Whenever a morph character matches a word character, the word character is popped from the word, and the morph character is popped from the morph. The word character is then linked to the morph in question. 
%
%Thus, in our example, the matching algorithm proceeds as follows: First, each of the prefixes \texttt{h}_{P} and \texttt{w}_{P} (the order should not matter) are compared to the first character of \textbf{whxlwm}. The \texttt{w}_{P} matches, so we get \{ \texttt{w}_{P}:\texttt{w} \}. The \texttt{w} is popped 
%
%from \textbf{whxlwm} as well as from \texttt{w}_{P}, thus completing the prefix morph \texttt{w}_{P} and removing it from consideration. Next, \texttt{h}_{P} (the only remaining prefix) is matched to the \texttt{h} of \textbf{hxlwm}, leaving us with \{  \texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h} \}, \textbf{xlwm}, and no more prefixes.
%
%Now only the stem-components remain. \texttt{w}_{ST} fails to match the first character 
%of \textbf{xlwm}. However, the \texttt{m} of \texttt{xlm}_{ST} does, yielding
% \{ \texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:m\}, 
% \textbf{lwm}, an stem-components {\texttt{lm}_{ST} and \texttt{w}_{ST}. 
% The \texttt{q} of {\texttt{lm}_{ST} matches the \texttt{q} in \textbf{qm}, giving us 
% \{\texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:\texttt{mq}\}, 
% \textbf{wm}, and stem-components \texttt{m}_{ST} and 
% \texttt{w}_{ST}. Next, \texttt{w}_{ST} matches the \texttt{w} in \textbf{wm}, 
%completing the stem-component \texttt{w}_{ST}. We now have \{\texttt{w}_{P}:\texttt{w} \texttt{h}_{P}:\texttt{h}, \texttt{xlm}_{ST}:\texttt{mq}, \texttt{w}_{ST}:\texttt{w}\}, \textbf{m}, and  
% \texttt{m}_{ST}.
% Finally, \texttt{m}_{ST} matches \textbf{m}, completing \{\texttt{xlm}_{ST}\} and consuming the word's last remaining character.
%%	\begin{verbatim}
%%		.*(x).*(\u00F3).?.?(t).*
%%	\end{verbatim}
%%	\texttt{.*(x).*(\'{o}).?.?(t).*}
%\subsubsection{Stage 3: Compress}. Here, each morph to an atomic symbol, 
%so that, e.g., a three-character morph becomes in effect single-character item, 
%as do all morphs consisting of more than one character. In this way, most of 
%the words is shortened (in terms of character-count) and thus compressed.
%Each atomic morph symbol is a unique unicode character. For example, 
%consider the 
%Hebrew words \textit{magdil} and \textit{gadol}, which share the root 
%\textit{g.d.l}. 
%Suppose that the Stage 2 outputs for these words are as in \eqref{ex:magdil} and 
%\eqref{ex:gadol}, respectively. 
%\begin{exe}  \ex \label{ex:unicode} \begin{xlist}
%	\ex magdil \quad ma-, \,\, g.d.l, \,\, i 
%	\label{ex:magdil}
%	\ex gadol \, \quad  g.d.l, \,\, a.o
%	\label{ex:gadol}
%	\end{xlist}
%\end{exe}
%Altogether, there are four \emph{unique} morphs in \eqref{ex:unicode}, namely \textit{ma-}, \textit{g.d.l}, 
%\textit{i}, and \textit{a.o}.
%Stage 3 will map each of these to a unique symbol, as in \eqref{ex:map}, for instance.
%\begin{exe}
%	\ex  \textit{ma-} $\mapsto$ \$ \quad \textit{g.d.l} $\mapsto$ \% \quad
%\textit{i} $\mapsto$ \& \quad \textit{a.o} $\mapsto$ \#
%\label{ex:map}
%\end{exe}
%Finally, each word is reassembled with atomic symbols being substituted for the morphs. 
%They are put together in the original order of their corresponding morphs.
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex magdil \quad \$\%\&
%	\label{ex:re-magdil}
%	\ex gadol \, \quad \%\#
%	\label{ex:re-gadol}
%	\end{xlist}
%\end{exe}
%% The atomic symbols are put together in the order of their corresponding character sequences, as illustrated in \eqref{ex:reassembled}. 
%Note, however, that in the case of interdigitation, the relative order of atomic symbols corresponding to interleaved sequences must be decided arbitrarily.
%
%\subsubsection{Stage 4: Test} 
%Two input files, a test and a 
%control file, are now fed to Morfessor. 
%The test file is the output of Stage 3.  %will contain the test data, i.e., the compressed words from Stage 3. 
%The control file consists of the original, unaltered words; its purpose is to serve as a baseline for measuring the effect 
%of the compression carried out in Stage 3. 
%% But what gives
%The idea here is to see if the MCMM's morphs, 
%now represented as atomic symbols, aide the 
%process of morphological segmentation.  % But how can we tell if the atomic symbols help?
%% What are they supposed to help?  Are they supposed to aide in the process of discovering the actual morphemes (i.e., the non-compressed, non-reduced morphemes)? If so, we need to somehow translate atomic symbols back into character sequences, but retaining the segmentation divisions computed on the strings of atomic symbols.
%Morfessor induces morphological segmentations for each file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, the atomic symbols were converted back into morphs (i.e., Stage 3 was undone).
%\begin{exe}
%	\ex   \$ $\mapsto$  \textit{ma-}
%	\quad \% $\mapsto$ \textit{g.d.l} 
%	\quad  \& $\mapsto$ \textit{i}
%	\quad  \# $\mapsto$ \textit{a.o}
%\label{ex:map}
%\end{exe}
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex  \$ \, + \, \% \, + \,  \& \quad $\mapsto$ \quad  ma \,+ \, gdl  \, + \, i
%	\label{ex:reconverted-magdil}
%	\ex  \% \, + \, \# \quad $\mapsto$  \quad gdl \, + \, ao
%	\label{ex:reconverted-gadol}
%	\end{xlist}
%\end{exe}
% The result is a test segmentation file whose words have the same characters as those of the control file, but possibly quite different segmentations. The two segmentations were evaluated against a common gold standard.
%%That is, do they make the task easier? 
%%Do they improve segmentation accuracy? 
%%Morfessor %then
%% induces morphological segmentations for each file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation will at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, change the atomic symbols back to morphs (i.e., undo Stage 3). Finally, evaluate the two segmentations against a common gold standard.
%
%\subsection{Gold-standard data}
%Recall that Morfessor is a nonlinear \emph{sequential} algorithm; that is, it possesses nonlinearity, but not nonsequentiality (see section~\ref{sec:nls})
%and is thus incapable of detecting non-concatenative roots and patterns. 
%Moreover, interdigitation is lost when atomic symbols are substitute for character sequences. 
%Two basic types gold-standard data were therefore required: one to assess Morfessor's output and one to evaluate the 
%stem-internal root-and-pattern morphology.
% 
%\paragraph{Morfessor gold standard}
%%I will need gold-standard segmentations against which to assess Morfessor's output.  
%Morfessor produces two output (or analysis) files, one for the test file (processed words) and one for the control file (original or uprocessed words)
%To obtain gold-standard datasets for Morfessor, I manually segmented $\frac{1}{10}$ of the original, unprocessed wordlist.
%This amounts to a total of three unprocessed wordlists: a list of transcribed words, a list of transcribed words with stress marked, and a list of words spelled according to orthographic conventions. 
%
%\paragraph{Non-concatenative gold standard}
%To evaluate my system's performance on non-concatenative morphology, I need gold-standard roots for both the transcribed wordlists and the orthographic data. For the transcribed data, I extracted root annotations from the CHILDES morphological analyses. 

%For the orthographic wordlist, I will take advantage of the root annotations provided in the original dataset of \cite{daya-et-al:2008} (see section~\ref{sec:data}).
% 
%\paragraph{Gold-standard annotation}
%How are we going to figure this out? The control will serve as the baseline. 
%But we also need a gold standard segmentation to provide a frame of reference for comparing the test and control segmentations. 
%That is, the respective accuracies of the test and control segmentations 
%is measured with respect to the gold standard.

%How large does the gold-standard set need to be? 1000 words? 1/10 of the total data set?

%\subsection{Ancillary (Gold-Standard-Based) Evaluation}
%This evaluation will involve mapping \textsc{mila-ma}'s categories onto different sets of gold-standard categories. How will these sets differ? The idea, I guess, is to use sets with varying amounts of granularity, or different amounts of modification, or perhaps different types of modification.
%For example, do we want the categories to create a strict partition? Maybe the gold-standard categories could/should overlap.
%
%An MCMM clusters its input vectors (= words) according to shared hidden-unit activations. 
%Its clusters overlap one another; i.e.,
%a single word can belong to several clusters at once. % because a word can contain multiple morphemes. 
%%Evaluating overlapping clusters is more complicated than evaluating disjoint clusters. 
%To evaluate these clusters,
%I will use the measures \emph{BCubed Precision} and \emph{BCubed Recall}, 
%which are specially designed for overlapping clusters \citep{amigo-et-al:2009}.
%%Precision and recall evaluate a system's output against an external gold standard. 
%
%I will use a
%finite-state morphological analyzer, namely the MILA Morphological Analysis tool (\textsc{mila-ma}) 
%\citep{hebrew-resources:2008} to generate gold-standard categories. It is, however, non-trivial to apply \textsc{mila-ma} to this purpose.
%is not entirely straightforward. First, there is the general problem of evaluating an unsupervised clustering algorithm
%against external criteria. One usually has an idea about what the output should be, 
%but without a training set,
%it is difficult to be specific about this. 
%Second, one must consider that
%\textsc{mila-ma}'s particular categories may not be appropriate for cluster evaluation in every case.
%
%For example, \textsc{mila-ma} tends to use atomic categories such as \textsc{masc} and \textsc{pl} as opposed to \textsc{masc.pl}. In the case of Hebrew, \textsc{masc} and \textsc{pl} are abstract because neither corresponds to an actual morpheme. That is, the Hebrew \textsc{masc.pl} suffix \textit{-im} is fusional; it cannot be split into separate \textsc{masc} and \textsc{pl} substrings (cf. \textsc{masc.sg} forms, 
%which have no ending, and the \textsc{fem.pl} suffix \textit{-wt}).
%A clustering algorithm would be 
%disinclined to treat \textsc{masc} and \textsc{pl} as \textsc{mila-ma} does, since this would mean creating separate \textsc{masc} and \textsc{pl} clusters  despite the lack of a \emph{particular} \textsc{masc} suffix and a \emph{particular} \textsc{pl} suffix.
%%lack of shared formal elements. clusters because such clusters would not be based on shared formal elements; \textsc{masc} 
%%would contain words ending in -$\emptyset$ and \textit{-im}, and \textsc{pl} would contain words ending in \textit{-wt} and \textit{-im}.
%For this and similar reasons, \textsc{mila-ma}'s categories will need to be mapped to a somewhat adapted set 
%of gold-standard categories.

%****************
%\begin{description}
%\item[Stage 1: Extract.] Derive morphs from cluster centroids. That is, for each cluster centroid vector,  map the \emph{active} features to a particular sequence of alphabetic characters. The mappings is governed by a set of mapping rules. The resulting sequence of alphabetic characters is the morph. Repeat this process for each cluster (i.e., cluster centroid).
%
%\textbf{Example:} Suppose that \texttt{z<k} and \texttt{k<r} are the active features in a given cluster's centroid. These features would map to the (potentially discontinuous) character sequence \textit{zkr}. The morph would thus be the root \textit{z.k.r}.
%
%\item[Stage 2: Match.] Map morph characters to word characters. That is, given a cluster and its morph (obtained in Stage 1), go through the cluster's words, and in each word, determine which characters are the morph's characters. Label these characters as components of the morph in question. Repeat this process for each cluster/morph.
%Each morph is a (possibly discontinuous) sequence of alphabetic characters. 
%Given a cluster and its newly extracted morph, identity the morph's characters in each of the clusters words. 
%That is, for each word, match the morph's characters to the \emph{correct} word characters. Note that there is potential for ambiguity here. Suppose, for example, that the morph in question is the \textit{-wt}. It's easy enough to find a single \texttt{t} in a string of letters, but \texttt{t} is a frequently occurring letter, and it could easily occur elsewhere in the word. I have to make sure my matching algorithm selects the right character in cases like this. Repeat this process for each cluster. 
%For each word $w$ in a given cluster, identify the characters in $w$ that correspond to the morph's characters. counterparts of each the morph characters to their counterpart character in the word in question. with their matching characters in the word in 
%the characters of the morph to the corresponding characters in the cluster's member words.
%
%\paragraph{Example:}  
%Consider a cluster whose member words are \textit{mazkir}, \textit{hizkir}, \textit{zoker}, \textit{zokrim}, and \textit{zikron}. The morph in this case is the root \textit{z.k.r}. Stage 2 identifies the root consonants in each word and labels them as components of the morph \textit{z.k.r}. Here, the morph's characters are ``labeled" via boldface type:
%%\footnote{In reality, of course, a larger and more sophisticated labeling/indexing system is necessary, as every morph will require a distinct label/index.}:  
%\textit{ma\textbf{zk}i\textbf{r}},
%\textit{hi\textbf{zk}i\textbf{r}}, \textit{{z}o\textbf{k}e\textbf{r}}, \textit{\textbf{z}o\textbf{kr}im},
%and \textit{\textbf{z}i\textbf{kr}on}.
%The process is repeated for each cluster/morph.
%
%\item[Stage 3: Compress.] Map each morph to a single unique unicode character.
%\textbf{Example:} Consider the 
%Hebrew words \textit{magdil} and \textit{gadol}, which share the root \textit{g.d.l}. 
%Suppose that the Stage-2 outputs for these words are as in \eqref{ex:magdil} and \eqref{ex:gadol}, respectively. 
%\begin{exe}  \ex \label{ex:unicode} \begin{xlist}
%	\ex magdil \quad ma-, \,\, g.d.l, \,\, i 
%	\label{ex:magdil}
%	\ex gadol \, \quad  g.d.l, \,\, a.o
%	\label{ex:gadol}
%	\end{xlist}
%\end{exe}
%Altogether, there are four \emph{unique} morphs in \eqref{ex:unicode}, namely \textit{ma-}, \textit{g.d.l}, 
%\textit{i}, and \textit{a.o}.
%Each of these is mapped to a unique atomic symbol, as in \eqref{ex:map}.
%\begin{exe}
%	\ex  \textit{ma-} $\mapsto$ \$ \quad \textit{g.d.l} $\mapsto$ \% \quad
%\textit{i} $\mapsto$ \textit{i} \quad \textit{a.o} $\mapsto$ \#
%\label{ex:map}
%\end{exe}
%%Let the atomic symbols inherit the sequential order of their counterpart morphs. 
%In general, the atomic symbols inherit the ordering of the original morphs. 
%The exceptional cases are those of interdigitation. When two morphs are interleaved, 
%they are unordered with respect to each other.
%However, when they are mapped to atomic symbols, they necessarily 
%take on an arbitrary relative order because there is no way to interleave two 
%\emph{atomic} units: either $A$ precedes $B$ or $B$ precedes $A$; 
%there is no other option.
%%but with the atomic symbols now taking the places of of the original morphs. Put the symbols in the same order as their morph counterparts.  hat for morphs that are two or more characters long.. They are put together in the same order as the original morphs. This reducing or elsince it replaces whole character sequences, even discontinuous ones, with atomic symbols (see section~). 
%\begin{exe}  
%	\ex \label{ex:reassembled} \begin{xlist}
%	\ex magdil \quad \$\%\textit{i}
%	\label{ex:re-magdil}
%	\ex gadol \, \quad \%\#
%	\label{ex:re-gadol}
%	\end{xlist}
%\end{exe}
%%Now, when the characters of the two morphs are interleaved, i.e. in the case of interdigitation, the relative order of the morphs is indeterminate. However, when these two morphs are mapped to atomic symbols, they necessarily take on an arbitrary relative order. That is, either $A$ precedes $B$ or $B$ precedes $A$; there is no other option. 
%The mapping from morphs to atomic symbols thus abstracts interdigitation.  
%
%\item[Stage 4: Test.]
%%-- concatenative morphs.]
%Feed both the control file (i.e., the file containing the original wordlist) and the test file (i.e., the output of Stage 3) to
%Morfessor \citep{creutz-and-lagus:2005, creutz-and-lagus:2007}. Morfessor then induces morphological segmentations for each input file, yielding a \emph{control segmentation} and a \emph{test segmentation}. The words in the test segmentation will at this point still consist of the atomic symbols from Stage 3. Without disrupting Morfessor's segmentation decisions, change the atomic symbols back to morphs (i.e., undo Stage 3). Finally, evaluate the two segmentations against a common gold standard.
%%control file, will 
%%%now be fed to Morfessor. 
%%The test file is the output of Stage 3.  The control file will contain the original, unaltered words. 
%%It will provide a baseline 
%%for measuring the effect of the compression carried out in Stage 3. 
%%The idea here is to see if the MCMM's morphs, 
%%now represented as atomic symbols, aide the process of morphological segmentation. 
%%That is, do they make the task easier? 
%%Do they improve segmentation accuracy? 
%\end{description}

