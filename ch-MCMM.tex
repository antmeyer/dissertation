\chapter{The Multiple Cause Mixture Model}
\label{ch:MCMM}

\section{Introduction}
\label{sec:mcmm:intro}
Multimorph is an unsupervised learning system and thus must be
driven by an unsupervised learning algorithm, a, 
a procedure that induces categories from raw, unlabeled data.
This chapter discusses Multimorph's core learning algorithm, the Multiple Cause Mixture Model (MCMM) \citep{saund:94} 

This chapter is a culmination of the chain of reasoning established in the preceding three chapters.
In chapters~\ref{ch:intro} and \ref{ch:lit-review}, we saw that it was precisely 
the set of \emph{nonlinear-nonsequential} models that were capable of representing 
nonconcatenative morphological structure, and that it was the properties \emph{nonlinearity}
and \emph{nonsequentiality} that give rise to this capacity. Furthermore, we in 
chapter~\ref{ch:graph} that bipartite graphs (or bipartite graphical algorithms) were a 
subset of NLNS algorithms. In fact, the bipartite structure of such graphs turns out to 
be a particularly coherent way to capture nonlinearity and nonsequentiality (see chapter~\ref{}).
The present chapter homes in on a particular kind of bipartite learning algorithm, namely 
the MCMM. Thus, the present chapter, when taken in the context of the preceding chapters, 
explains the rational and theoretical significance of my choosing an MCMM 
in particular to serve as Multimorph's learning algorithm.
%The importance of MCMMs to the present study is that an MCMM serves as Multimorph's core learning algorithm. 
%This chapter thus in part motivates my choosing the MCMM to fill this role. It also---together with the preceding three chapters---explains the theoretical significance of this choice.

%.. In particular, it motivates my choosing an MCMM to fill this role. In doing so, it explains the nature, structure, and function of MCMMs, and why an MCMM is an appropriate learning mechanism for Multimorph. 

%The preceding chapters provide the chain of reasoning that leads to our choosing the MCMM.

%For Multimorph, I chose a particular kind of bipartite learning model called the Multiple Cause Mixture Model (MCMM), developed by \citet{saund:94}.

%MCMMs are conducive to ULM, particularly to the UL of non-concatenative morphology, mainly because they are bipartite graphs, since it is their bipartiteness that makes them nonlinear and nonsequential. The bipartiteness of MCMMs is evident in Section~\ref{sec:architecture} (see in particular figure~\ref{fig:mcmm}), which elucidates the architecture
%of an MCMM. But while their bipartiteness is most significant for our purposes, MCMMs have other attractive properties. [which we shall discuss in this chapter.] For example, an MCMM is neither sum nor a product of experts, 

%In this chapter, therefore, we thus present a particular bipartite graphical model,  as Multimorph's core learning algorithm. In particular, I chose the bipartite graphical model known as the MCMM.
This chapter is organized as follows: Section first lays out the key components 
of an MCMM's structure, i.e., the attributes that all MCMMs share. 
Section~\ref{sec:context} then proceeds to place MCMMs in a larger of 
unsupervised learning algorithms, demonstrating 
the similarities and differences between MCMMs and other types of
 learning algorithms such as Restricted Boltzmann Machines (RBM) \citep{smolensky:1986, hinton:2002}
 and Latent Dirichlet Allocation (LDA). Section~\ref{sec:mixing-function} 
 focuses a particularly important component of an MCMM, namely its 
 \emph{mixing function}, which is a type of activation function, that is, a 
 function that determines the activities of nodes in neural networks.  
Finally, section~\ref{sec:mcmm-learning} discusses the means by which 
MCMMs learns, paying particular attention to Multimorph's MCMM.

%demonstrates the bipartite nature of an MCMM; compare figure~\ref{fig:mcmm} in section~\ref{sec:architecture} to the canonical bipartite structure shown in figure \ref{fig:gt-bipartite} in chapter~\ref{ch:graph}. whose bipartiteness becomes clearly evident in section~\ref{sec:architecture} Section~\ref{sec:architecture} also breaks the architecture of an MCMM, describing its basic components and how they relate to each other. hen delves deeper into the nature the MCMM, comparing and contrasting it to related learning algorithms, such as the Restricted Boltzmann Machine (RBM). Finally, section~\ref{sec:mcmm-learning} discusses the particular means by which Multimorph's MCMM learns.

% learning process itself in detail. The following section then describesSection~ fact that will become plainly evident in section~{sec:architecture}  reasoning to motivate the use of bipartite graphical learning model, specifically the MCMM, as Multimorph's algorithm.  takes this this line of argumentation to its logical conclusion i  a bipartite graphical model, namely the MCMM, tononconcatenative morphology, and thus a bipartite graphical learning algorithm is going to be at least furthermore, a bipartite archict
%In this chapter, we pre 
%From  that bipartite graphs intrinsically nonlinear and nonsequential, and that bipartite graphical learning models are 
%Thus, a bipartite architecture implies both nonlinearity and nonsequentiality and hence a capacity for representing both nonconcatenative and concatenative morphological structure. 
%
%Proposition x thus motivates the choice of bipartite graphical model to serve as Multimorph's learning suitable for learning  have the capacity to represent both nonconcatenative and concatentative morphological struThis brings us to the primary concern of the present the chapter: the choice of Multimorph's core learning algorithm. Motivated by the results of the previous chaptersparticular, the preceding chapters motivate and thus capable of representing nonconcatenative morphology. All of this is Multimorph's learning algorithm

%I chose the Multiple Cause 
%Mixture Model (MCMM) proposed by \citet{saund:94} to fill this role. The present chapter motivates this decision, and in doing so, provides a detailed exposition of the MCMM. Section~\ref{sec:architecture} first provides an overview of an MCMM's architecture, i.e., a description of its major components and the relationships between these components. Section~\ref{sec:architecture} then delves deeper into the nature the MCMM, comparing and contrasting it to related learning algorithms, such as the Restricted Boltzmann Machine (RBM).   comparing it to other including a description of their architecture in detail. describes the will be motivated in the present present chapter.  The present chapter will describe   primary objective of this chapter  motivate this decision; first of all, it will show in section~{sec:architecture}that it is a bipartite graph and thus qualifies as a nonlinear-nonsequential algorithm. \citet{saund:94}  nonlinear-nonsequential models are argued that nonlinearity and nonsequentiality were essential properties learning nonconcatenative
%morphology must be   
%Multimorph's learning algorithm is an instance of an Multiple Cause 
%Mixture Model (MCMM) proposed by \citet{saund:94}, and thus MCMMs
%is a major focus of this chapter.

%By \emph{model}, 
%we mean a set of assumptions about how the world works. Where Multimorph is concerned, 
%the ``world" is the morphology of natural languages, which, significantly, includes \emph{non-concatenative} morphology. 
%autosegmental morphology \citep{mccarthy:1981}. 
%In chapter~\ref{ch:graph}, we argued
%that in order
%for a model to be able to deal with nonconcatenative morphology,
% it must be both \textbf{nonlinear} \textbf{and}
% \textbf{nonsequential},
% i.e., satisfy both the \textsc{Nonlinearity} criterion and the \textbf{Nonsequentiality} criterion; 
% see definitions (\ref{def:nl}) and (\ref{def:ns}) and 
% proposition (\ref{prop:nlns}). We also saw in chapter~\ref{ch:graph} that these two conditions are equivalent to the two parts of the definition of biparteness. In particular, \dots.
 
% Therefore, if a model is a bipartite graphical model, then it is both nonlinear and nonsequential, which means
% that it has the capacity to model nonconcatenative morphology:
%  \begin{proposition}
%% In order for a model to be able to handle non-concatenative morphology, it must be a bipartite graph.
%% \end{proposition}
%  \label{prop:bipartite}
%If a model is a bipartite graph, it can handle nonconcatenative morphology. Moreover, if it can handle nonconcatenative morphology, it can handle concatenative morphology.
% \end{proposition}
 
%if a model is a bipartite graph, 
%it has the capacity to deal with nonconcatenative morphology.
%% of morphology are treated as the same basic phenomenon; the capacity to model nonconcatenative morphology is actually the more general capacity, since a concatenative process is essentially a nonconcatenative process with zero interdigitation. 
%That is, if there were such a thing as a ``morph discontiguity factor,'' that is, a some sort of measure of the degree to which the phonemes of different morphs tend to be interleaved (i.e., the tendency of morphs to be discontinuous), 
%%separation between phonemes of the same morpheme discontiguous the  phonemes of the same morph separated by intervening phonemes (from another morph) by those of other morphs), 
%then this metric would be zero for ``strictly concatenative'' morphological processes, and some number greater than zero for ``nonconcatenative" ones. The point is that there no categorical difference between concatenative and nonconcatenative processes. Fundamentally, the two type of processes are in fact one and the same  process.  Thus, the capacity to model noncatenative morphology implies the capacity to model concatenative morphology. 
%Noncatenative morphology can thus be regarded as the more general case.
% \begin{proposition}
%% In order for a model to be able to handle non-concatenative morphology, it must be a bipartite graph.
%% \end{proposition}
%  \label{prop:bipartite}
%If a model is a bipartite graph, it can handle nonconcatenative morphology. Moreover, if it can handle nonconcatenative morphology, it can handle concatenative morphology.
% \end{proposition}

 %call for a bipartite graph, since they are essentially equivalent to
% The Multiple Cause Mixture Model (MCMM) is a general 
% framework for unsupervised learning developed by \cite{saund:94}. 
% Section~\ref{sec:architecture} in this chapter will describe the architecture 
% of an MCMM, i.e., its key components and the relationships between these 
% components. It will become clear in this section that MCMMs are bipartite 
% graphs and thus, by proposition~\ref{prop:bipartite}, an MCMM is capable 
% of learning nonconcatenative morphology. 
 %serve as Multimorph's core learning framework.
 %  In this chapter, we shall first demonstrate an MCMM qualifies as a bipartite graph.  
%In addition to demonstrating the bipartite \emph{bona fides} of MCMMs, this chapter will, in section~\ref{sec:architecture}, discuss the architecture of an \ac{MCMM},
%i.e., is key components and the relationships between these components. 
%This chapter will also, in 
%sections~\ref{sec:mixing-function} and \ref{sec:mcmm-learning}, describe the process whereby
%an \ac{MCMM} learns. 

\section{Basic MCMM Architecture}
\label{sec:architecture}

In \citet{saund:94}, the term \emph{multiple cause mixture model} refers to a family of algorithms rather than to a  a single, 
specific algorithm. But though the term is general to an extent, it also denotes a certain category of algorithms.
And since MCMMs constitute an definite category, 
all MCMMs share certain key attributes.
These attributes are 
%in large part 
the subject of this section. 

An MCMM is a graphical model consisting of two layers of nodes (or units): a \emph{surface} (or \emph{visible}) 
layer and a \emph{hidden} layer.
These two layers are illustrated in figure~\ref{fig:mcmm}, where $\mathbf{m}_i$ 
is the vector of hidden units, and $\mathbf{r}_i$ is the layer of surface units. The layer $\mathbf{r}_i$ is actually a (working) reconstruction of the original vector of surface units $\mathbf{x}_i$, but we will nonetheless generally refer to $\mathbf{r}_i$ as the surface layer, and its components or nodes as surface units. 
\footnote{Saund uses $\mathbf{d}$ 
rather than $\mathbf{x}$ to denote this vector. We use $\mathbf{x}$ because it seems to be more widely
used elsewhere to denote data points. We also use $\mathbf{d}$ to denote the \emph{direction vector} later in this chapter, so using $\mathbf{x}$ to denote data points helps to avoid confusion.}; 
The vector it is the observed, 
real-world data. The vector $\mathbf{r}_i$ is the ``working" 
reconstruction of original data vector $\textbf{x}_i$. It is dynamic, evolving as learning progresses and  
becoming increasingly similar to $\mathbf{x}_i$. 
%This is the essence of the learning process, and the mechanisms that drive it are
%discussed in section~\ref{sec:mcmm-learning}.

Notice in figure~\ref{fig:mcmm} that there are no connections between $\mathbf{x}_i$ and $\mathbf{r}_i$, nor between 
$\mathbf{x}_i$ and $\mathbf{m}_i$. The vector $\mathbf{x}_i$ is thus not an integrated component of the graphical 
model. Its role is merely to serve as a target for the reconstruction vector $\mathbf{r}_i$. 
The goal of the system as a learning model is therefore to get $\mathbf{r}_i$ to 
match $\mathbf{x}_i$, that is, to reconstruct the component values of $\mathbf{x}_i$ in 
the vector $\mathbf{r}_i$. The process that achieves this reconstruction is the learning process and is discussed in detail in section~\ref{sec:mcmm-learning}.
% shall discuss this reconstruction process in more detail in section REF.

\begin{figure}[htb]
\begin{center}
%\small
\begin{tikzpicture}[shorten <=1pt,->,draw=black!100]
	\def \rowtwoht{4.25cm}
	\def \weightlevel{2.75cm}
	\def \rowoneht{1.25cm}
	\def \basement{0cm}
	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{dots}=[text width=5ex, text centered]
	\tikzstyle{annot}=[text width=17ex, text centered]
	% labels
	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {hidden units ($\mathbf{m}_i$)};
	\node[annot] (weights) at (0cm,\weightlevel) {weights ($\mathbf{C}$)};
	\node[annot] (r-layer) at (0cm,\rowoneht) {reconstructed units ($\mathbf{r}_i$)};
	\node[annot] (d-layer) at (0cm,\basement) {input (original) units ($\mathbf{x}_i$)};
	
	\node[dots] 	(m3)	at (7.75cm,\rowtwoht)	 	{. . .};
	\node[dots] 	(r4) 	at (8.5cm,\rowoneht)   		{. . .};
	\node[dots] 	(d4) 	at (8.5cm,\basement)   		{. . .};
	
	\footnotesize
	% hidden layer
	\node[m-node] 	(m0)	at (3.25cm,\rowtwoht)		{$m_{i,0}$};
	\node[m-node] 	(m1)	at (4.75cm,\rowtwoht)		{$m_{i,1}$};
	\node[m-node] 	(m2)	at (6.25cm,\rowtwoht)	 	{$m_{i,k}$};
	\node[m-node] 	(m4)	at (9.25cm,\rowtwoht)	 	{$m_{i,K}$};
	
	% reconstructed vector
	\node[r-node] 	(r0)	at (2.5cm,\rowoneht)		{$r_{i,0}$};
	\node[r-node] 	(r1)	at (4cm,\rowoneht)		{$r_{i,1}$};
	\node[r-node] 	(r2)	at (5.5cm,\rowoneht)	 	{$r_{i,2}$};
	\node[r-node] 	(r3)	at (7cm,\rowoneht) 		{$r_{i,j}$};
	\node[r-node] 	(r5) 	at (10cm,\rowoneht)   		{$r_{i,J}$};
	%\node[r-node] 	(r6) 	at (9.75cm,\rowoneht)   	{$r_{i,J}$};
	
	% data vector
	\node[d-node] 	(d0)	at (2.5cm,\basement)		{$x_{i,0}$};
	\node[d-node] 	(d1)	at (4cm,\basement)		{$x_{i,1}$};
	\node[d-node] 	(d2)	at (5.5cm,\basement)	 	{$x_{i,2}$};
	\node[d-node] 	(d3)	at (7cm,\basement) 		{$x_{i,j}$};
	\node[d-node] 	(d5) 	at (10cm,\basement)   		{$x_{i,J}$};
	%\node[d-node] 	(d6) 	at (9.75cm,\basement)   	{$x_{i,J}$};
	
	\path 
		(m0)	edge	node	{}	(r0)
		(m0)	edge	node	{}	(r1)
		(m0)	edge	node	{}	(r2)
		(m0)	edge	node	{}	(r3)
		(m0)	edge	node	{}	(r5)
		
		(m1)	edge	node	{}	(r0)
		(m1)	edge	node	{}	(r1)
		(m1)	edge	node	{}	(r2)
		(m1)	edge	node	{}	(r3)
		(m1)	edge	node	{}	(r5)
		
		(m2)	edge	node	{}	(r0)
		(m2)	edge	node	{}	(r1)
		(m2)	edge	node	{}	(r2)
		(m2)	edge	node	{}	(r3)
		(m2)	edge	node	{}	(r5)
		(m3)	edge	node[right=1mm]	{$c_{j,k}$}	(r4)
		%	
		(m4)	edge	node	{}	(r0)
		(m4)	edge	node	{}	(r1)
		(m4)	edge	node	{}	(r2)
		(m4)	edge	node	{}	(r3)
		(m4)	edge	node	{}	(r5);
		
\end{tikzpicture}
\end{center}
\caption{Architecture of a Multiple Cause Mixture Model (MCMM)} 
\label{fig:mcmm}
\end{figure}

The vector $\mathbf{x}_i$ is the $i$th row in the 
input corpus, i.e., the $I \times J$ input data matrix $\textbf{X}$. Each 
row in $\textbf{X}$ is a data point, a feature-vector representation of a word. 
The $J$ columns in  $\textbf{X}$ each represent a particular feature.
The identical subscript on $\mathbf{x}_i$ 
and $\mathbf{r}_i$ means that these vectors correspond to the same data 
point, and thus ultimately the same word. Notice that the hidden 
layer $\textbf{m}_i$ bears this same index and is thus also tied to the $i$th word. The index $i$
means that it 
is the 
\emph{cluster-membership vector} of the $i$th word; that is, it is the $i$th row in the row 
in the $I \times K$ matrix 
$\mathbf{M}$, and each of $\mathbf{M}$'s $K$ columns corresponds to a particular 
\emph{cluster}. Thus, the activity of each cell $m_{i,k}$ indicates whether or not the $i$th 
data point $\mathbf{x}_i$ (via its reconstruction $\mathbf{r}_i$) is a member of the 
$k$th cluster.

%corresponds to the $i$th word.  The vector $\mathbf{r}_i$ 
%is the \emph{reconstruction} vector; that is, it is a (working) 
%reconstruction of $\mathbf{x}_i$. There is a reconstruction 
%vector associated with each original data point \mathbf{x}_i$. 
%is the $i$th data point among a total of $I$ data points. Each data point is a 
%feature-vector representation of a word. Each row in $\textbf{X}$ contains $J$ 
%columns, one column for each feature. The dimensions input data matrix 
%$\textbf{X}$ are $I \times J$. 
%$\mathbf{r}_i$, the reconstruction of $\mathbf{x}_i$, is the $i$th row in the 
%$I \times J$ matrix $\mathbf{R}$. The subscript on the hidden-unit vector 
%$\mathbf{m}_i$ indicates that it is related to $\mathbf{x}_i$ and $\mathbf{r}_i$. 
%Each $J$ corresponds to a particular surface unit, i.e., feature. 
%The hidden-unit vector $\mathbf{m}_i$ is a row in the larger $I \times K$ matrix 
%$\mathbf{M}$. Each of $\mathbf{M}$'s $K$ columns corresponds to a particular 
%\emph{cluster}, and thus, the activity of each $m_{i,k}$ indicates whether the $i$th 
%data point $\mathbf{x}_i$ (via its reconstruction $\mathbf{r}_i$) is a member of the 
%$k$th cluster.  
%%columns contains $I$, each the particular vector of hidden causes for the $i$th surface 
%vector. $\mathbf{}$ has $K$ columns.

%The hidden units are connected to surface units by a matrix of weights $\mathbf{C}$. 
%Each individual arc $c_{j,k}$ has a value in the interval $[0,1]$. This value 
%represents the weight on the connection between $m_{i,k}$ and $r_{i,j}$.
%Each node, i.e., each hidden unit and each surface unit, has an activity value in $[0,1]$ that
%indicates whether it is \textsc{on} (active) or \textsc{off} (inactive).
%The activity of $r_{i,j}$ is determined by a \emph{mixing function}, which takes as inputs the 
%hidden-unit activities $\mathbf{m}$ and their respective weights $\mathbf{c}_j$
%(section~\ref{sec:mixing-function}).

\section{Relationship to Other Unsupervised\\Learning Frameworks}
\label{sec:context}

In this section, we relate multiple cause mixture models to other frameworks for unsupervised learning. 
Multiple cause mixture models simultaneously belong to two distinct unsupervised-learning contexts. 
On the one hand, they are neural networks, and more specifically, they are \emph{autoencoders}, which are neural networks that learn without supervision \citep{dayan-and-zemel:95}.
On the other hand, the very name 
\emph{multiple cause mixture model} 
contains the term \emph{mixture model}, which itself refers to a large class of unsupervised learning models. 
This section is therefore divided into two subsections: First, section~\ref{sec:autoencoders} 
considers MCMMs as neural networks, i.e., as autoencoders, using neural-network terminology 
to relate them to other autoencoders. Section~\ref{sec:autoencoders} then 
considers MCMMs from a \emph{mixture-model} perspective, discussing their relationship to standard mixture models as well as \emph{mixed-membership} (or \emph{admixture}) models. 

\subsection{Autoencoders}
\label{sec:autoencoders}
% So-and-so classifies Saund's MCMM as a kind of autoencoder. 
%He considered a form of autoencoder network in which the hidden units signal features and the hidden-output weights describethe way in which  features generate predictions  of the inputs. 
\citet[][p. 2]{dayan-and-zemel:95} describe the MCMM as a ``form of autoencoder network.'' 
They mean \emph{autoencoder} in a general sense, as a term for a \emph{class} that encompasses several subtypes.  
In this general sense, an \emph{autoencoder} is any unsupervised graphical algorithm that
learns a compressed encoding of its input data. 
% The compressed encoding from which the original can be generated, or \emph{reconstructed} which through a \emph{generative} process, i.e., a process of learning to generate its input data. To do this, it must construct an internal theory, so to speak, of its input data's structure. This theory takes the form of a compressed encoding of the input data, and in particular, a compressed encoding from which the original data can be generated, or \emph{reconstructed}.  
%The learning process of is essentially one of trial and error; 
An autoencoder learns by iteratively testing and revising its internal model, improving it slightly with each iteration. 
%The autoencoder tests the suitability of a working encoding through \emph{reconstruction}, i.e., by attempting to recover or reconstruct the original data from the working encoding.
%Thus, at the beginning of each iteration, the autoencoder has 
The internal model consists of the values of its weight along with 
the values in each hidden-unit vector $\textbf{m}_i$. The values 
of the weights are universal, applying to the whole data set, while 
each hidden-unit vector $\textbf{m}_i$ is specific to the $i$th data point. 
An autoencoder's learning process consists of two
loops, an \emph{inner loop} that iterates over the data-point indices $i \in I$, 
visiting each individual data point in turn, and an \emph{outer loop} that repeats 
the inner loop as many times as necessary. 
%(until the total reconstruction error falls below certain threshold).
An autoencoder tests the fitness of its working model by attempting to reconstruct each input data 
point from its internal representation (or encoding) of the data point, % (i.e. from the represenationthe working the model, 
and measuring the \emph{reconstruction error}, the discrepancy between the reconstructed and original data vectors.  
It then makes adjustments to the model so as to reduce this error.

In the following, we consider three types
of autoencoders, namely the classical autoencoder, the Restricted Boltzmann Machine (RBM),
and the MCMM. We compare them across three dimensions: \emph{form}, \emph{node activation}, 
(i.e., the means of computing node activities), and \emph{optimization procedure}
(i.e., method for minimizing the model's error).

%These adjustments, which mark the end of the current iteration, should result in a slightly better internal representation for next iteration.  

%\subsubsection{Classical Autoencoder}
%\paragraph{Form.}
%\label{sec:classical-auto}

\begin{figure}[tb]
%%\begin{minipage}{.3\textwidth}
\begin{center}
\small
\begin{tikzpicture}[shorten <=1pt,->,draw=black!100]
	\def \rowtwoht{5cm}
	\def \weightstwo{3.75cm}
	\def \rowoneht{2.5cm}
	\def \weightsone{1.25cm}
	\def \basement{0cm}
	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{d-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{dots}=[text width=5ex, text centered]
	\tikzstyle{annot}=[text width=17ex, text centered]
	% labels
	\node[annot] (r-layer) at (0cm,\rowtwoht) {reconstructed units ($\mathbf{r}_i$)};
	\node[annot] (weights) at (0cm,\weightstwo) {weights ($\mathbf{B}$)};
	\node[annot] (hidden-layer) at (0cm,\rowoneht) {hidden units ($\mathbf{m}_i$)};
	\node[annot] (weights) at (0cm,\weightsone) {weights ($\mathbf{A}$)};
	\node[annot] (d-layer) at (0cm,\basement) {input (original) units ($\mathbf{x}_i$)};
	
	\node[dots] 	(m3)	at (7.75cm,\rowoneht)	 	{. . .};
	\node[dots] 	(r4) 	at (8.5cm,\rowtwoht)   		{. . .};
	\node[dots] 	(d4) 	at (8.5cm,\basement)   		{. . .};
	
	\footnotesize
	% hidden layer
	\node[m-node] 	(m0)	at (3.25cm,\rowoneht)		{$m_{i,0}$};
	\node[m-node] 	(m1)	at (4.75cm,\rowoneht)		{$m_{i,1}$};
	\node[m-node] 	(m2)	at (6.25cm,\rowoneht)	 	{$m_{i,k}$};
	\node[m-node] 	(m4)	at (9.25cm,\rowoneht)	 	{$m_{i,K}$};
	
	% reconstructed vector
	\node[r-node] 	(r0)	at (2.5cm,\rowtwoht)		{$r_{i,0}$};
	\node[r-node] 	(r1)	at (4cm,\rowtwoht)		{$r_{i,1}$};
	\node[r-node] 	(r2)	at (5.5cm,\rowtwoht)	 	{$r_{i,2}$};
	\node[r-node] 	(r3)	at (7cm,\rowtwoht) 		{$r_{i,j}$};
	\node[r-node] 	(r5) 	at (10cm,\rowtwoht)   		{$r_{i,J}$};
	%\node[r-node] 	(r6) 	at (9.75cm,\rowoneht)   	{$r_{i,J}$};
	
	% data vector
	\node[d-node] 	(d0)	at (2.5cm,\basement)		{$x_{i,0}$};
	\node[d-node] 	(d1)	at (4cm,\basement)		{$x_{i,1}$};
	\node[d-node] 	(d2)	at (5.5cm,\basement)	 	{$x_{i,2}$};
	\node[d-node] 	(d3)	at (7cm,\basement) 		{$x_{i,j}$};
	\node[d-node] 	(d5) 	at (10cm,\basement)   		{$x_{i,J}$};
	%\node[d-node] 	(d6) 	at (9.75cm,\basement)   	{$x_{i,J}$};
	
	\path
		(d0)	edge	node	{}	(m0)
		(d0)	edge	node	{}	(m1)
		(d0)	edge	node	{}	(m2)
		%(d0)	edge	node	{}	(m3)
		(d0)	edge	node	{}	(m4)
		%	
		(d1)	edge	node	{}	(m0)
		(d1)	edge	node	{}	(m1)
		(d1)	edge	node	{}	(m2)
		%(d1)	edge	node	{}	(m3)
		(d1)	edge	node	{}	(m4)
		%
		(d2)	edge	node	{}	(m0)
		(d2)	edge	node	{}	(m1)
		(d2)	edge	node	{}	(m2)
		(d2)	edge	node	{}	(m4)
		%
		(d3)	edge	node	{}	(m0)
		(d3)	edge	node	{}	(m1)
		(d3)	edge	node	{}	(m2)
		(d3)	edge	node	{}	(m4)
		%
		(d4)	edge	node[right=2mm]	{$a_{j,k}$}	(m3)
		%
		(d5)	edge	node	{}	(m0)
		(d5)	edge	node	{}	(m1)
		(d5)	edge	node	{}	(m2)
		(d5)	edge	node	{}	(m4)
	 
		(m0)	edge	node	{}	(r0)
		(m0)	edge	node	{}	(r1)
		(m0)	edge	node	{}	(r2)
		(m0)	edge	node	{}	(r3)
		(m0)	edge	node	{}	(r5)

		(m1)	edge	node	{}	(r0)
		(m1)	edge	node	{}	(r1)
		(m1)	edge	node	{}	(r2)
		(m1)	edge	node	{}	(r3)
		(m1)	edge	node	{}	(r5)

		(m2)	edge	node	{}	(r0)
		(m2)	edge	node	{}	(r1)
		(m2)	edge	node	{}	(r2)
		(m2)	edge	node	{}	(r3)
		(m2)	edge	node	{}	(r5)

		(m3)	edge	node[right=3mm]	{$b_{k,j}$}	(r4)	
		
		(m4)	edge	node	{}	(r0)
		(m4)	edge	node	{}	(r1)
		(m4)	edge	node	{}	(r2)
		(m4)	edge	node	{}	(r3)
		(m4)	edge	node	{}	(r5);		
\end{tikzpicture}
\end{center}
\caption{The ``classical'' autoencoder, one member of the general autoencoder family}
%: an input layer ($\mathbf{d}$), a hidden layer ($\mathbf{m}$), and an output layer
\label{fig:autoencoder}
\end{figure}

\begin{figure}[t]
\begin{center}
%\small
\begin{tikzpicture}[shorten <=1pt,<->,draw=black!100]
	\def \rowtwoht{4.25cm}
	\def \weightlevel{2.75cm}
	\def \rowoneht{1.25cm}
	\def \basement{0cm}
	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{dots}=[text width=5ex, text centered]
	\tikzstyle{annot}=[text width=17ex, text centered]
	% labels
	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {hidden units ($\mathbf{m}_i$)};
	\node[annot] (weights) at (0cm,\weightlevel) {weights ($\mathbf{C}$)};
	\node[annot] (r-layer) at (0cm,\rowoneht) {reconstructed units ($\mathbf{r}_i$)};
	\node[annot] (d-layer) at (0cm,\basement) {input (original) units ($\mathbf{x}_i$)};
	
	\node[dots] 	(m3)	at (7.75cm,\rowtwoht)	 	{. . .};
	\node[dots] 	(r4) 	at (8.5cm,\rowoneht)   		{. . .};
	\node[dots] 	(d4) 	at (8.5cm,\basement)   		{. . .};
	
	\footnotesize
	% hidden layer
	\node[m-node] 	(m0)	at (3.25cm,\rowtwoht)		{$m_{i,0}$};
	\node[m-node] 	(m1)	at (4.75cm,\rowtwoht)		{$m_{i,1}$};
	\node[m-node] 	(m2)	at (6.25cm,\rowtwoht)	 	{$m_{i,k}$};
	\node[m-node] 	(m4)	at (9.25cm,\rowtwoht)	 	{$m_{i,K}$};
	
	% reconstructed vector
	\node[r-node] 	(r0)	at (2.5cm,\rowoneht)		{$r_{i,0}$};
	\node[r-node] 	(r1)	at (4cm,\rowoneht)		{$r_{i,1}$};
	\node[r-node] 	(r2)	at (5.5cm,\rowoneht)	 	{$r_{i,2}$};
	\node[r-node] 	(r3)	at (7cm,\rowoneht) 		{$r_{i,j}$};
	\node[r-node] 	(r5) 	at (10cm,\rowoneht)   		{$r_{i,J}$};
	%\node[r-node] 	(r6) 	at (9.75cm,\rowoneht)   	{$r_{i,J}$};
	
	% data vector
	\node[d-node] 	(d0)	at (2.5cm,\basement)		{$x_{i,0}$};
	\node[d-node] 	(d1)	at (4cm,\basement)		{$x_{i,1}$};
	\node[d-node] 	(d2)	at (5.5cm,\basement)	 	{$x_{i,2}$};
	\node[d-node] 	(d3)	at (7cm,\basement) 		{$x_{i,j}$};
	\node[d-node] 	(d5) 	at (10cm,\basement)   		{$x_{i,J}$};
	%\node[d-node] 	(d6) 	at (9.75cm,\basement)   	{$x_{i,J}$};
	
	\path 
		(m0)	edge	node	{}	(r0)
		(m0)	edge	node	{}	(r1)
		(m0)	edge	node	{}	(r2)
		(m0)	edge	node	{}	(r3)
		(m0)	edge	node	{}	(r5)
		
		(m1)	edge	node	{}	(r0)
		(m1)	edge	node	{}	(r1)
		(m1)	edge	node	{}	(r2)
		(m1)	edge	node	{}	(r3)
		(m1)	edge	node	{}	(r5)
		
		(m2)	edge	node	{}	(r0)
		(m2)	edge	node	{}	(r1)
		(m2)	edge	node	{}	(r2)
		(m2)	edge	node	{}	(r3)
		(m2)	edge	node	{}	(r5)
		(m3)	edge	node[right=1mm]	{$c_{j,k}$}	(r4)
		%	
		(m4)	edge	node	{}	(r0)
		(m4)	edge	node	{}	(r1)
		(m4)	edge	node	{}	(r2)
		(m4)	edge	node	{}	(r3)
		(m4)	edge	node	{}	(r5);
		
\end{tikzpicture}
\end{center}
\caption{Restricted Boltzmann Machine} 
\label{fig:rbm}
\end{figure}

\paragraph{Form.}
%Whereas the MCMM has two layers of nodes ($\textbf{m}$ and $\textbf{r}$), 
% The ``classical'' autoencoder,
%shown in figure~\ref{fig:autoencoder} has three layers (or vectors) of nodes, namely 
%%, as 
%%illustrated in
%%in figure~\ref{fig:autoencoder}. 
%%\begin{enumerate}
%%\item The \textbf{input} vector $\textbf{x}$
%%\item The \textbf{hidden} vector $\textbf{m}$
%%\item The \textbf{output} vector \textbf{reconstruction} layer $\textbf{r}$ 
%%\end{enumerate}
%the \textit{input} vector $\textbf{x}$, the \textit{hidden} vector $\textbf{m}$, and the \textit{output} (or \textit{reconstruction}) $\textbf{r}$.
%Each of these vectors has a counterpart in an MCMM; in particular, each corresponds to the component in figure~\ref{fig:mcmm} that shares the same label.  
%However, there are important differences between these two types of autoencoders. 
%The main differences pertain to form (or general architecture), node activation, 
%i.e., the means of computing node activities, and the \emph{optimization procedure}, 
%i.e., the method for minimizing the model's error.

MCMMs and RBMs are both bipartite graphs, whereas the classical autoencoder is 
not (see the description of bipartite graphs in chapter~\ref{ch:graph}).
The classical autoencoder, shown in figure~\ref{fig:autoencoder}. has distinct input 
and output layers. Each original data vector $\textbf{x}_i$ enters the network at the input layer, is compressed 
at $\textbf{m}_i$, and reconstructed at $\textbf{r}_i$. Because there are three layers 
of nodes in an autoencoder, there must be two layers of weights ($\textbf{A}$ and $\textbf{B}$ in figure~\ref{fig:autoencoder}), both which are oriented in the same direction.
Classical autoencoders thus function similarly to supervised feed-forward neural networks. 
In fact, the only difference between a classical autoencoder and a supervised 
feed-forward network is that a classical autoencoder's target vectors are its input vectors. 

MCMMs and RBMs, by contrast, both lack an input layer that is distinct from the reconstruction layer. In both MCMMs and RBMs,
the original (input) data vectors $\textbf{x}_i$ are actually not even part of the graph, since there are no weights (i.e., arcs) that connect the original data vectors to the other vectors (namely, the hidden and reconstruction node layers). 
Speaking strictly in terms of form, therefore, MCMMs and RBMs belong together in a class that excludes the classical autoencoder.

%MCMMs and RBMs are identical.
%Both are bipartite graphs; that is, both have two layers of nodes such that no nodes within the same layer are connected
%%direct connections between nodes of the same layer 
%(see the description of bipartite graphs in chapter~\ref{ch:graph}).  Moreover, in a RBM, as in an \ac{MCMM}, 
%one of these node layers functions as a hidden layer, and the other as the visible 
%(or surface) layer \citep{mohamed-and-hinton:2010}. 
%(i.e, the the vectors  $\textbf{m}_i$ and  $\textbf{r}_i$, respectively).
%In this respect, MCMMs and RBMs differ significantly from the classical autoencoder, wherein the
%In an MCMM, 
%%the oriThe most conspicuous difference is that MCMMs bipartite graph, having just two layers of nodes, namely \textbf{m}$ and \textbf{r}$. Recall that in an MCMM, 
%the original data vector $\textbf{x}$ is actually not part of the graph itself, since it is not connected by weights to either  $\textbf{m}$ or  $\textbf{r}$.
 %In a classical autoencoder, by contrast
%namely the original data vector $\textbf{d}$, the hidden-node vector $\textbf{m}$, and reconstruction vector $\textbf{r}$. These vectors are also present in the MCMM in \ref{fig:mcmm}. One key difference between an MCMM and a classical autoencoder, however, is that the the original data vector 
%$\textbf{d}$ is not a true layer in MCMM, since there are no connecting 
%weights weights between it and $\textbf{m}$ or $\textbf{r}$. This is not 
%the case in a classical encoder, where 
%original data vectors $\textbf{x}_i$ constitute a fully integrated layer of the network, namely its input layer, which is distinct from the reconstruction (or output) layer.
%\textit{input layer}. Each original data vector $\textbf{x}_i$ enters the network at the input later, is compressed 
%at $\textbf{m}_i$, and reconstructed at $\textbf{r}_i$. Because there are three layers 
%of nodes in an autoencoder, there must be two layers of weights, labeled 
%$\textbf{A}$ and $\textbf{B}$ 
%in \ref{fig:autoencoder}. The reconstruction error is computed at $\textbf{r}$, and the appropriate updates are 
%\emph{backpropagated} to the $\textbf{B}$ and then $\textbf{A}$ weights.
%
%An MCMM has no designated input layer. The classical autoencoder, by contrast, has distinct input and output layers. Classical autoencoders thus function as a supervised feed-forward neural network. In fact, the only difference between a classical autoencoder and a supervised feed-forward network is that a classical autoencoder's target vectors are its input vectors.

\paragraph{Node Activation.} The RBM and classical autoencoder essentially use the same function for computing node activities, namely the \emph{sigmoidal weighted sum}.
%are essentially the same, generally  sharing the same activation function.
However, MCMMs, as we shall see, call for a different type of activation function, a type of function that \citet{saund:94} calls a \emph{mixing function}.
% that has different properties to those of sigmoidal weighted sum.
The sigmoidal weighted sum is a composition of two functions: the ``outer" function is logistic sigmoid, 
	\begin{equation} %\label{eq:sigws}
	\label{eq:sig}
	\sigma(t) = \frac{1}{1 + e^{-t}} 
	\end{equation} %\label{eq:sigws
The ``inner'' function replaces the $t$ in equation~\eqref{eq:sig}. It is a weighted-sum of the form 
\begin{equation} \label{eq:wtd-sum}
\sum_{j} x_{j} w_{j,k} \qquad  \text{or} \qquad \textbf{x}^{\textsf{T}}\textbf{w}_k
\end{equation}
%$\sum_{j} x_{j} w_{j,k}$, which is equivalent to the \emph{inner product} of vectors $\textbf{x}$ and  $\textbf{w}$, that is, $\textbf{x}^\textsf{T}\textbf{w}_k$, 
where $\textbf{x}$ would be a vector of $J$ node activities and $\textbf{w}_k$ the vector of $J$ weights connecting the nodes of $\textbf{x}$ to the $k$th node in a different layer of nodes, say $\textbf{m}$. 
%The two vectors in this case are (1) a vector of node activities, and (2), a vector of weights.  component is the inner product of two vectors, 
%namely the activities of the preceding node layer activities and that layer's corresponding weight vector (i.e., the set of weights linking the preceding node layer to the current node layer),
%weight vector connecting preceding layer to the current node in the 
%current layer, 
%as in $\sum_{j} x_{j} a_{j,k}$, where $a_{j,k}$ is a weight. % in $\mathbf{A}$.
The logistic sigmoid \eqref{eq:sig} maps the weighted sum (no matter how large or small it may be) to a number in the interval $[0,1]$, thus constraining node activities to this interval.
%	\begin{equation} %\label{eq:sigws}
%	\label{eq:sig}
%	\sigma(x) = \frac{1}{1 + e^{-x}} 
%	\end{equation} %\label{eq:sigws
In a classical autoencoder, shown in figure~\ref{fig:autoencoder}, the activity of each the hidden node $m_k$ is computed according to equation \ref{eq:sig-m-ac}. 
\begin{align}
\label{eq:sig-m-ac}
m_{k} &= \sigma\big(\sum_{j} r_{j} a_{j,k}\big) \\
%m_{i,k} = P(m_{i,j}|r_{i,j}) =\sigma\big(\sum_{j} c_{j,k} r_{i,j}\big)
\label{eq:sig-r-ac}
%r_{i,j} = P(r_{i,j}|m_{i,k})  = \sigma\big(\sum_{i,k} c_{j,k} m_{i,k}\big)
r_{j} &= \sigma\big(\sum_{k}  m_{k} b_{k,j} \big)
\end{align}
For example, to obtain the activity of the hidden node $m_{i,2}$.
we compute $\sum_{j} x_{i,j} a_{j,2}$, where each $x_{i,j}$ is the activity of one of the $J$ nodes in the $i$th input vector $\textbf{x}_i$, and $a_{j,2}$ is the the weight connecting the $j$th node in $\textbf{x}_i$ to the hidden node $m_{i,2}$.
%, which is the equivalent to the inner product \textbf{x}_i \cdot \textbf{c}^{\textsf{T}}_4$ 
We then feed the resulting sum to the sigmoid function, thereby obtaining an activity for $m_{i,2}$ that is within $[0,1]$. 
We do the same for each hidden node in $\textbf{m}_i$
The process is the same for the reconstruction nodes, except that the just-computed hidden-node activities now become the input values, and the matrix $\textbf{B}$ now supplies the weight vectors instead of $\textbf{A}$.
% the incoming node activities are those in the hidden layer, and the weight vector is
%the corresponding column in the $\textbf{B}$ matrix.
% Thus, for node, say, 
%$r_{i,6}$ (i.e., $j=6$), the weighted sum is
%%we would sum over the $K$ hidden-unit activities (each multiplied by its respective weight in 
%%$\textbf{b}^{\textsf{T}}_{6}$: 
%$\sum_{k} m_{i,k} b_{k,6}$.  
%%That is, we sum over the $K$ hidden-unit activities, with each activity multiplied by its respective weight in the column (or row?) $\textbf{B}_{:,6}$). 
%We  feed each such sum to the logistic sigmoid function, %as we do for every node. 
%which restrains node activities to the interval $[0,1]$.
%$\textbf{b}^{\textsf{T}}_{6}
% we in the reconstruction vector, specifically The latter takes the output of the linear sum, which could be any real number, possibly much less than 0 or much greater than 1, and maps it onto a number in $[0,1]$.

An RBM is essentially a 
classical autoencoder in bipartite form. The activation functions of an RBM are the following: 
%displayed as equations~\ref{eq:sig-m} and  
\begin{align}
\label{eq:sig-m}
m_{k} &= \sigma\big(\sum_{j} r_{j} c_{j,k} + \delta_{j}\big) \\
%m_{i,k} = P(m_{i,j}|r_{i,j}) =\sigma\big(\sum_{j} c_{j,k} r_{i,j}\big)
\label{eq:sig-r}
%r_{i,j} = P(r_{i,j}|m_{i,k})  = \sigma\big(\sum_{i,k} c_{j,k} m_{i,k}\big)
r_{j} &= \sigma\big(\sum_{k}  m_{k} c_{k,j} + \theta_{k}\big)
\end{align}
where $\delta$ and $\theta$ are biases. 
These are clearly similar to the classical autoencoder's activation functions~\eqref{eq:sig-m-ac} and \eqref{eq:sig-r-ac},
but there are a few differences. One is of course the presence of the biases in the 
RBM activation functions. But the main difference 
lies in the fact that the RBM's functions~\eqref{eq:sig-m} and \eqref{eq:sig-r} 
both refer to the same weight matrix $\textbf{C}$, whereas the classical 
autoencoder's  functions \ref{eq:sig-m-ac} and \ref{eq:sig-r-ac} each 
refer to a different weight matrix 
($\textbf{A}$ in equation~\ref{eq:sig-m-ac} and $\textbf{B}$ in equation~\ref{eq:sig-r-ac}). 
An RBM's learning process is thus bi-directional: 
The hidden-unit activities are conveyed to the matrix to the reconstruction 
units via the weight matrix $\textbf{C}$, and then the reconstruction 
activities are conveyed back to the hidden units via the same weight matrix, resulting in a cyclic process. 
%computed via equation~\eqref{eq:sig-m} in one direction via the weight matrix $\textbf{C}$, and the reconstruction activities are then
%computed via equation~\eqref{eq:sig-r} in the reverse direction v

%But despite these differences, RBMs and classical autoencoders can be grouped together where activation functions are concerned, since they both (essentially) use sigmoidal weighted sum. MCMMs, however, call for different type of activation function, a type of function that \citet{saund:94} calls a \emph{mixing function}. We shall discuss the properties of mixing functions in section~\ref{sec:mixing-function}.
%are nonetheless more similar than they are different. nevertheless share 
%the same basic activation function, the sigmoidal weighted sum. As we shall see, this is a point that places RBMs and classical autoencoders together on one side of a dividing line and MCMMs on the other. 

%where $\delta_{k}$ and $\theta_{j}$ are \emph{biases}.  The activation functions of an RBM are 
%displayed as equations~\eqref{eq:sig-m} and \eqref{eq:sig-r}. One difference 
%between these activation functions and those of the classical autoencoder is the introduction of the biases
%$\delta_{k}$ and $\theta_{j}$. 

%Anotherthat equations activation functions and those of  a classical autoencoder is that whereas a classical autoencoder has two sets of weights (for three layers of nodes), an RBM has just one (to connect two layers of nodes)
%%An RBM works somewhat differently due to its bipartite architecture. 
%%In particular, 
%The \emph{same} matrix of weights that connects the hidden nodes $\textbf{m}$ to the
%reconstruction nodes $\textbf{r}$ also connects the reconstruction nodes to the hidden nodes, as illustrated in figure~\ref{rbm}. The learning process is thus bi-directional: 
%Another difference between RBMs
%The reconstruction error is computed, the weights are updated, and another cycle begins.
% Then another round begins: The hidden-unit activities are re-computed from the new reconstruction activities and the updated weights $\textbf{C}$, and then the reconstruction activities are re-computed from the new hidden-unit activities and the same updated weights $\textbf{C}$. 
%Equations~\ref{eq:sig-r} and  equation~\ref{eq:sig-m} are nonetheless very similar to the classical autoencoder's \emph{biases} to weighted sum. These biases are a distinctive feature of RBMs. Like the weights, they are parameters that must be learned
%
%But despite these differences, RBM and the classical autoencoder nonetheless share 
%the same basic activation function, the sigmoidal weighted sum. %, a significant departure from the MCMM.

But despite these differences, RBMs and classical autoencoders can be grouped together where activation functions are concerned, since they both (essentially) use sigmoidal weighted sum.
The sigmoidal weighted sum is a very common activation function in neural networks, 
but it is not the only way to compute the activity of a node. Any activation function, to borrow
an analogy from \citet{saund:94}, is essentially a voting rule, i.e.,  a policy 
for combining multiple input votes into a single output decision.  
In the context of neural-network models such as RBMs, MCMMs, and 
classical autoencoders, the output decision is always activity of a given node, 
and the input votes are the \emph{weighted} activities of the preceding layer's nodes. 
That is, each vote is a product of the form $x_{j} w_{j,k}$, where $x_{j}$ is the activity 
of the $j$th in a given layer of nodes, and $w_{j,k}$ is the weight associated with that 
node. The question being voted upon in this case is the activity of the $k$th node in the 
\emph{next} layer, and there are
a total of $J$ votes to consider.
%connecting $x_{i,j}$ to the $k$th node of another layer, 
%whose activity is the question being voted upon. There are always a total of 
%$K$ votes for the activation to take into account, where $K$ is the number 
%of nodes in the preceding layer. 

The sigmoidal weighted sum is in effect a sort of average, and thus tends to obscure
the influence of minority subsets of inputs. \citet{saund:94} argues 
that the averaging nature of the sigmoidal weighted sum is inappropriate when one wants to encourage
a small subset of votes to take responsibility for a given surface unit's activity, as in MCMMs.
\citet{saund:94} thus proposes for MCMMs a special kind of activation function called a \emph{mixing function}.
We discuss the properties of mixing functions in 
section~\ref{sec:mixing-function}.

%One of the simplest possible activation functions is a weighted sum of the form
%$\sum_{k} h_{k} w_{k,j}$. It qualifies as an activation function because
%it maps the votes $h_{k} w_{k,j}$ (for $k \in K$) to a single output value. 
%However, this value can be any real number, and it grows (or shrinks) 
%in direct proportion to the quantity and sizes of its input values.
% Since most applications require that node activities stay within a 
% certain interval, usually $[0,1]$ or $[-1,1]$, the output of such a sum
% must be fed to a nonlinear ``squashing'' function, such as the logistic sigmoid, 
% which enforces this constraint.

%The sigmoidal weighted sum is in effect a sort of average. To take an average 
%over a set of values is to put them them all in melting pot, so to speak. 
%An average tends to obscure the influence of minority subsets of inputs.
%Moreover, the weights in a classical autoencoder range from $-1$ to $1$. 
%A negative weight can thus be paired with a positive hidden-unit activity to produce 
%a negative vote. The presence of both positive and negative votes means that votes 
%can counteract each other and even cancel each other out. 
%These properties 
%The averaging nature of the sigmoidal weighted sum allows innumerable possible 
%configurations to produce the same node activity. \citet{saund:94} argues 
%that such flexibility is inappropriate when one wants to encourage
%a small subset of votes to take responsibility for a given surface unit's activity, as in MCMMs.

\citet{saund:94} thus proposes for MCMMs a special kind of activation function called a \emph{mixing function}.
We discuss the properties of mixing functions in 
section~\ref{sec:mixing-function}.

\paragraph{Optimization procedure.}  In a classical autoencoder, the two layers of 
weights---first $\textbf{B}$ and then $\textbf{A}$---are directly updated via 
back-propagation. 
The values of the hidden units are not directly manipulated; rather, they change 
%are a consequence of the weights $\textbf{A}$. % (along with values in the input vector). 
%Thus, in a classical autoencoder, the values of the hidden units change 
only as a 
consequence of the direct adjustments made to the weights. The same is true in an RBM: 
an RBM's hidden-unit activities change in response to adjustments 
make to the weights and biases change, but they are not
adjusted directly.

In an MCMM, by contrast, both $\textbf{m}_i$ and $\textbf{C}$ are directly manipulated. This means that
an MCMM's learning process begins with two unknown variables, the matrices $\textbf{M}$ and 
$\textbf{C}$, and it must induce both during the course of learning. We shall discuss how an MCMM accomplishes in section~ref{sec:two-in-one}.
%That is, an MCMM requires two complimentary process, \textsc{Optimize-M} 
%and \textsc{Optimize-C} (see section~\ref{sec:num-opt}), each directly manipulating 
%the values in its respective matrix. 

\subsection{Mixture and Mixed-Membership Models}

A mixture model attempts to account for the data points in a data set 
by assuming that multiple probability
distributions are responsible for generating the data. 
It also assumes that \emph{exactly one}
of these underlying distributions is responsible for each data point. 
 These underlying 
distributions can be interpreted as clusters. Under this interpretation, each data point is 
is assigned to cluster (distribution) that is responsible for generating it. 
However, since the process is probabilistic,
data points are often not mapped to clusters with 100 percent certainty. 
There may be, for example, a 60 percent chance that a given datapoint belongs to one 
cluster, a 30 percent chance it belongs to another, and a 10 percent chance 
it belongs to yet another. In a \emph{hard clustering}, the data point is 
assigned to the most single probable category, and the other less probable 
possibilities are forgotten. However, the uncertainty can also be left 
unresolved, so that, in the case of our example, all three possibilities, 
along with their respective likelihoods, would be output. This is a 
\emph{soft} clustering. But even in the case of a \emph{soft} clustering, 
we still assume that the data point in question \emph{fundamentally} belongs to only one cluster; 
the ``softness" of the clustering just expresses the model's uncertainty 
as to which cluster this is. %the data point belongs.

By contrast, a mixed-membership model, e.g., Latent Dirichlet Allocation \citep{blei-et-al:2003},
%\footnote{
%Like latent Dirichlet allocation, MCMMs have
%been applied to document classification; \cite{sahami-et-al:96} 
%use an \ac{MCMM} to group $I$ documents into $K$ clusters.  Documents are 
%represented as vectors $\mathbf{x}_i$ of $J$ features. The features, i.e. surface units, 
%each indicate the absence or presence of a particular word (cf. the $\mathbf{x}$ 
%vector in figure~\ref{fig:mcmm}). The topics in \cite{sahami-et-al:96} are represented 
%as the MCMM's hidden units $\mathbf{m}_i$. Documents are grouped into topic 
%clusters by the learning process described below in section~\ref{sec:mcmm-learning}.}, 
views each data point as a complex object, i.e., as an object that itself consists of multiple objects, 
\emph{each of which} is generated by one of multiple probability distributions (or clusters). 
For example, in a mixed-membership \textit{topic model}, the complex objects are the 
\emph{documents} within a given corpus. The elemental objects that compose 
the documents are \emph{words}. The clusters correspond to \emph{topics}, which generate 
the words that together compose the documents. That is, each document is a set of words, and each 
word is generated by a particular topic. The document is associated to 
(or ``becomes a member of'') each topic that is
responsible for at least some of its words. 

However, these topics (if more than one topic is involved) cannot all claim equal responsibility
for the document in question. One topic might be responsible for 40 percent of the words, for instance, 
and another for 30 percent, and so on. 
%A single topic 
%cannot be responsible for the whole document unless no other topics 
%are involved.  
However, it is important to note that these percentages 
do not indicate uncertainty as they would in mixture models. 
%The assumption
%of multiple causes (i.e., clusters) is built 
%into mixed-membership models, whereas mixture models assume a single cause for each 
%data object. 
The reason that mixed-membership models yield percentages is 
that they decompose each data object into many elemental objects and allow different 
causes (e.g., topics) to claim responsibility for different 
elemental objects within the same document. 

Like mixed-membership models, multiple-cause mixture models (MCMMs) 
allow multiple causes to take part in accounting for a single data point. 
That is, as in mixed-membership models, MCMMs provide a means for 
different causes to take responsibility for different parts of data point. 
Thus, both MCMMs and mixed-membership models transcend the single-cause assumption of standard 
mixture models.
% and their assumption that each data point ultimately has 
%only one cause.

At the same time, however, MCMMs differ from mixed-membership models in that MCMMs do not deal in 
percentages of any sort.
%; that is, in MCMMs, a cause (or cluster) cannot be, for instance, 
%40 percent responsible for a (complex) data object. Rather, in MCMMs,
In MCMMs, 
cluster membership is always binary: either True or False. %, 0 or 1---100 percent or 0 percent, as it were. 
This behavior is a consequence of the nature of \emph{mixing functions}, the activation functions of MCMMs (see section~\ref{sec:mixing-function}). 
While MCMMs do allow individual clusters to claim responsibility 
for particular features, they ultimately do not attribute different proportions of a given feature vector
to different clusters. In an MCMM, as long as at least one feature is caused by a given cluster, the \emph{whole} feature
vector (or data point) is assigned to that cluster in addition to whatever other clusters are responsible for one or more of its features. 
%(i.e., individual components within data points). This is in fact a key property of MCMMs. 
%The difference between MCMMs and mixed-membership models stems from 
%both the neural nature and the mixing functions MCMMs: An MCMM's mixing 
%function requires a hidden unit's activity to be 1 in order for it count as an 
%``\textsc{on}'' vote for a particular surface unit, and 0 to be an ``\textsc{off}'' vote. 
%Of course, during learning, many hidden-unit activities will be between 0 and 1, 
During learning, many hidden-unit activities will be between 0 and 1,
but the learning process pushes them ever closer to one of these two values. 
Activity values between 0 and 1 can be interpreted as ``almost off'' or ``almost on,'' depending on where exactly they lie in this interval,
but not as real percentages as in mixed-membership models. 

This binary treatment 
%of causes 
is arguably more suitable for
morphological learning than a framework in which `` ownership percentages'' are assigned to causes.
We generally do not think of words' morphological compositions in 
terms of percentages.  That is, we tend not to say things like, ``60 percent 
of this word belongs to this morphological category, and 40 percent belongs 
to that category.'' Of course, such a statement would be particularly incoherent 
if we were speaking in terms of morphosyntactic categories, such as past tense. 
But it is also problematic in the case of form-based categories. Proportions or percentages of elemental components
do not have coherent interpretations in morphology as they do in topic modeling.
%of characters (or form-describing binary features) simply do not  
%meaning in morphology, not as one comparable to the that of word proportions 
%in topic modeling, at any rate. 
In morphology, it is the simple presence/absence 
of a morphological unit that is important, not the proportion of characters 
or features it can claim. Moreover, the percentages of mixed-membership 
models correspond to degrees of membership:  
``[M]ixed membership models assume that individuals or observational units 
[i.e., words] may only partly belong to population mixture categories [i.e., topics] \dots. 
The degree of membership then is a vector of continuous, 
non-negative latent variables [i.e., hidden units] that add up to 1'' \citep[][p. 4]{airoldi-et-al:2014}. 
By contrast, in morphology, there are no degrees of membership (and thus
no a requirement that they should add up to one.)

%\begin{table}[htb]
%\centering
%\begin{tabular}{ccc}
%Topic-Modeling & & ULM \\
%document & $\to$ & word \\
%topic & $\to$ & morph/morpheme \\
%word & $\to$ & word-internal feature or character \\
%\end{tabular}
%\caption{Some topic-modeling terms and their ULM equivalents. That is, for instance, \emph{documents}
%in the context of topic modeling are equivalent to \emph{words} in the context of the unsupervised learning
%of morphology.}
%\label{tab:tm-to-ulm}
%\end{table}	
				
\section{Mixing Function}
\label{sec:mixing-function}

In this section, we discuss the \emph{mixing function}, 
the component within an MCMM responsible for determining 
activities of the surface nodes in $\textbf{r}_i$.
A mixing function takes as input the hidden-unit activities $\textbf{m}_i$ accompanied by a vector of weights  
linking the hidden units to a particular surface node. It then outputs the activity 
of this surface node. 
%and their corresponding weights is  multiplied by 
%particular vector weights---and outputs the activity of a particular surface 
%(or reconstruction) node. 
\citet{saund:94} describes mixing functions as
\emph{voting rules}, i.e., rules whereby multiple votes are combined in some way to yield a single decision, 
Each vote is a product of the form $m_{i,k}c_{j,k}$ for $k \in K$, and the decision is a single node activity. 
%for a particular surface node, 
%activity value for the $j$th surface node $\textbf{r}_{i,j}$. 
%All mixing functions
%are activation functions, but not all activation functions are mixing functions.
%That is, mixing functions constitute a special class of activation function; they 

Mixing functions are 
activation functions that are specially tailored to serve the needs of MCMMs. 
These needs are as follows:
\begin{enumerate}
\item Hidden-unit activities 
must be interpretable as binary indicators of cluster membership, where $1$ 
indicates membership, and $0$ non-membership. Thus, in the case of 
Multimorph, a given word belongs to the 
$k$th cluster if its $k$th hidden-unit activity is $1$.
\item Each column in the matrix $\textbf{C}$ must be interpretable as the centroid 
(or ``average" vector) of a particular cluster. That is, the $k$th column 
$\textbf{C}$ (whose dimensions are $J \times K$) must be interpretable as the centroid vector of the $k$th cluster. This means that the $j$th element in the $k$th column of $\textbf{C}$ must be interpretable as the average $j$th feature value across the data points that belong to the $k$th cluster. 
%  Note that each column in $\textbf{C}$ is composed of $J$ elements, each of which corresponds to one of the $J$ features that constitute a data point. 
%(i.e., ``average'' vectors). 
%The $j$th vector of weights $\textbf{c}_j$ 
%(i.e., the $j$th row 
%of the weight matrix $\textbf{C}$) must be interpretable as the centroid 
%(i.e., the ``average" vector) of the $k$th cluster. 
\end{enumerate}
In short, both the hidden-unit values and weights must have coherent interpretations. 
Recall from section~\ref{sec:context} that the sigmoidal weighted sum is 
 does not satisfy these criteria. It does not encourage 
interpretable valuations for parameters and is therefore 
not a mixing function, even though it is an activation function.  All mixing functions
are activation functions, but not all activation functions are mixing functions.

Just as there are many possible activation functions, there are many 
possible mixing functions.
\citet{saund:94} proposes two, each designed 
to suit a particular data domain.  The two data domains, namely 
the ``\textsc{write-black}'' and ``\textsc{write-white-and-black}'' 
domains are distinguished by the nature of the voting task that 
determines the values of surface nodes. 

 In the \textsc{write-black} domain, the decision in question is 
 whether a given surface unit should be ``black" (i.e., 1) or ``not black'' (i.e., 0). 
 The votes for each such decision are 1 or 0 (or between 1 and 0 prior to convergence) 
 and combined according to the  Noisy-Or mixing function \citep{pearl:1988}, 
 defined in \eqref{eq:noisy-or}.
\begin{equation}\label{eq:noisy-or}
r_{i,j} = 1 - \prod\limits_{k} (1 - m_{i,k} c_{j,k})
\end{equation}
 The Noisy-OR function is essentially an\textit{OR} gate, or, to put it another way, 
 an``at-least-one" gate. That is, the surface node $r_{i,j}$ is $1$ (and thus ``\textsc{on}") as long as 
 \emph{at least one} cause is active.
 If two or more causes are active,
$r_{i,j}$ is still going to be $1$. The output of Noisy-OR does not change 
as the number of active causes changes; it is 1 as long as there is at least one active cause (or ``yes'' vote).

The \textsc{write-white-and-black} domain differs from \textsc{write-black} 
in that it requires the mixing function to be capable of outputting negative surface-unit 
activities as well as positive, i.e., values ranging from $-1$ to $1$.
Clearly, the Noisy-OR function does not satisfy this requirement.
Saund thus proposes the following mixing function for the {write-white-and-black} domain.
 \begin{align}\label{eq:wwb}
  r_{i,j} &=
    \begin{cases}
    %\label{eq:wwb-top}
      \frac{\sum\limits_{k} m_{i,k} c_{j,k}}{\sum\limits_{k} m_{i,k} |c_{j,k}|} & \text{$\forall m_{i,k}, c_{j,k} : m_{i,k} c_{j,k} \in \{-1,0,1\} $}\\ 
      %\label{eq:wwb-bottom}
      0 & \text{if \, $\sum\limits_{k} m_{i,k} |c_{j,k}|= 0$} 
    \end{cases}
 \end{align}
where $c_{j,k} \leq 1$, $0 \leq  m_{i,k} \leq  1$,  $-1 \leq  r_{i,j} \leq  1$, 
and the components $x_{i,j}$ of the original data points
are each either $1$ or  $-1$. Note the absolute value in the denominator in the upper case of equation~\eqref{eq:wwb}. 
%of the expression $\frac{\sum_{k} m_{i,k} c_{j,k}}{\sum_{k} m_{i,k} |c_{j,k}|}$. 
This absolute value allows the numerator and denominator to differ in sign so that 
the $r_{i,j}$ can be either positive or negative (and thus match the signs of the components of the original data points).

 Notice also that equation~\eqref{eq:wwb} contains weighted sums. In fact, same weighted sum
in the numerator of the upper case of equation~\eqref{eq:wwb} is the same weighted sum that appears in the sigmoidal weighted sum, namely $\sum\limits_{k} m_{i,k} c_{j,k}$.
 Nevertheless, there are important differences 
 between equation~\eqref{eq:wwb} and the sigmoidal weighted sum. First, there is 
 no logistic sigmoid in equation~\eqref{eq:wwb}; it is the denominator $\sum_{k} m_{i,k} |c_{j,k}|$ 
 that constrains output of \eqref{eq:wwb} to the interval $[-1,1]$. The other 
 difference is the nonlinearity arises from the conditions on the output \eqref{eq:wwb}; 
 to avoid division by zero, it is necessary to stipulate that $r_{i,j}=0$ if 
 $\sum_{k} m_{i,k} |c_{j,k}| = 0$, a result that occurs only if every vote is 0, 
 in which case no cluster can claim responsibility for the value of $r_{i,j}$. But 
 because $0$ never occurs in the original data points in the \textsc{write-white-and-black} 
 domain, the learning process would tend drive activities of the reconstruction nodes 
 away from $0$. That is, because the learning process seeks to minimize the discrepancy 
 between $\textbf{r}_i$ and $\textbf{x}_i$, $0$ cannot a valid final value for any $r_{i,j}$.  
 Thus, the weights and the hidden-unit vectors will be shaped to produce surface-node 
 activities that are either $-1$ or $1$ in accordance with the component values of 
 the original data points. 

To ensure that the activity of $r_{i,j}$ is either $-1$ or $1$, at least one hidden 
unit $m_{i,k}$ must have an activity of 1, and the weight associated with this 
hidden unit, namely $c_{j,k}$, must be either $-1$ or $1$. This would associate 
the surface unit $r_{i,j}$ to at least one cluster. Ultimately, mixing 
function \eqref{eq:wwb} encourages hidden-unit activities to move toward 
either $0$ or $1$ and the weights toward either $-1$ and $1$. These are 
values with clear cluster-oriented interpretations: The $1$'s and $0$'s of the 
hidden-unit vector indicate cluster membership and nonmembership, respectively, 
while the weights adopt the same of values, namely $\{-1, 1\}$ present in the original data points. 
This is important, since, as noted above, an MCMM's weight vectors have to be 
interpretable as composite or average vectors for the clusters.

In preliminary experiments for the present work, I tried both the Noisy-OR 
and the \textsc{write-white-and-black} mixing function. The latter did 
not work well; in fact, under the \textsc{write-white-and-black} mixing function, the system 
failed to make progress toward learning to reconstruct the original 
data points. This may have been due to the sparseness of the data points 
in this study; each data point is vector of hundreds of feature slots, but 
only a handful have the value $1$. are feature-vector representations of words. 
There are hundreds of features, and each has a designated slot in each feature vector.
(We will discuss these features in detail in chapter~\ref{ch:experi}.)

%in fact, it seemed to be fundamentally ill-suited to the present study's learning task. proved not to work well given for the purposes of the present study,  particularly the nature of this study's data points. Recall this study's data points are feature-vector representations of words, and that these feature vectors are sparse. That is, for any given word, most features are not present. 
In a \textsc{Write-Black} formulation, absent features have the value $0$, but in the write-white-and-black formulation, they are $-1$. Therefore, in a sparse feature vector under the write-white-and-black formulation, the vast majority of features are $-1$. The cumulative effect of all these negatives in \eqref{eq:wwb} seemed to drown out the few positive positive features, making it impossible to reproduce the original data vectors. 

Noisy-OR, on the other hand, did allow learning to take place; that is, 
the system was able to reduce its error with Noisy-OR as its mixing function. 
Apparently, this study's sparse feature vectors (i.e., data points) are more aptly 
placed in the \textsc{Write-Black} domain than the \textsc{write-white-and-black} domain.

\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}[shorten <=1pt,->,draw=black!100]
	\def \rowtwoht{4.25cm}
	\def \weightlevel{2.75cm}
	\def \rowoneht{1.25cm}
	\def \basement{0cm}
	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{r-node-2}=[circle,draw=black!40,thick,inner sep=2pt,minimum size=7mm]
	%\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=7mm]
	\tikzstyle{dots}=[text width=5ex, text centered]
	%\tikzstyle{annot}=[text width=17ex, text centered]
%	% labels
%	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {hidden units ($\mathbf{m}_i$)};
	%\node[annot] (weights) at (0cm,\weightlevel) {weights ($\mathbf{C}$)};
%	\node[annot] (r-layer) at (0cm,\rowoneht) {reconstructed units ($\mathbf{r}_i$)};
	%\node[annot] (d-layer) at (0cm,\basement) {input (original) units ($\mathbf{x}_i$)};
	
	\node[dots] 	(m3)	at (7.75cm,\rowtwoht)	 	{. . .};
	\node[dots] 	(r4) 	at (8.5cm,\rowoneht)   		{. . .};
	%\node[dots] 	(d4) 	at (8.5cm,\basement)   		{. . .};
	
	\footnotesize
	% hidden layer
	\node[m-node] 	(m0)	at (3.25cm,\rowtwoht)		{$m_0$};
	\node[m-node] 	(m1)	at (4.75cm,\rowtwoht)		{$m_{i,0}$};
	\node[m-node] 	(m2)	at (6.25cm,\rowtwoht)	 	{$m_{i,1}$};
	\node[m-node] 	(m4)	at (9.25cm,\rowtwoht)	 	{$m_{i,K}$};
	
	% reconstructed vector
	\node[r-node-2] 	(r0)	at (2.5cm,\rowoneht)		{$r_0$};
	\node[r-node-2] 	(r1)	at (4cm,\rowoneht)		{$r_{i,0}$};
	\node[r-node] 	(r2)	at (5.5cm,\rowoneht)	 	{$r_{i,1}$};
	\node[r-node-2] 	(r3)	at (7cm,\rowoneht) 		{$r_{i,j}$};  
	\node[r-node-2] 	(r5) 	at (10cm,\rowoneht)   		{$r_{i,J}$};
	%\node[r-node] 	(r6) 	at (9.75cm,\rowoneht)   	{$r_{i,J}$};
	
	% data vector
%	\node[d-node] 	(d0)	at (2.5cm,\basement)		{$x_{i,0}$};
%	\node[d-node] 	(d1)	at (4cm,\basement)		{$x_{i,1}$};
%	\node[d-node] 	(d2)	at (5.5cm,\basement)	 	{$x_{i,2}$};
%	\node[d-node] 	(d3)	at (7cm,\basement) 		{$x_{i,j}$};
%	\node[d-node] 	(d5) 	at (10cm,\basement)   		{$x_{i,J}$};
	%\node[d-node] 	(d6) 	at (9.75cm,\basement)   	{$x_{i,J}$};
	
	\path 
		(m0)	edge	node[left=1.5mm]	{$c_{2,0}$}	(r2)
	
		(m1)	edge	node[right=0.0mm]	{$c_{2,1}$}	(r2)

		(m2)	edge	node[right=0mm]	{$c_{2,2}$}	(r2)
		%(m3)	edge	node {}	(r4)
		(m3)	edge	node[right=0mm]	{$c_{2,k}$}		(r2)
		%(m3)	edge	node[right=1mm]	{$w_{j,k}$}	(r4)	
		(m4)	edge	node[right=2mm]	{$c_{2,K}$}	(r2);

		
\end{tikzpicture}
\end{center}
\caption{%The activity of each surface unit  is determined by a \emph{vote}, so to speak.  
%Each hidden unit casts a \emph{weighted vote}, so to speak, 
The activity of a given surface (or reconstruction) unit is determined by combining ``votes'' cast by the hidden units.}
%i.e., a value eThe votes must then be combined in some way to yield an activity.}
%$r_{i,j}$
% Each hidden unit $m_{i,k}$ casts a weighted vote, i.e., the product $m_{i,k}c_{j,k}$. 
% The votes must then be combined in some way to yield a single activity for $r_{i,j}$.}
\label{fig:voting}
\end{figure}

\section{Learning}
\label{sec:mcmm-learning}

%\subsection{General Optimization Process}
%\label{sec:num-opt}
In general terms, Multimorph learns through a \emph{gradient-based} 
search for the parameter values that minimize (or maximize) an objective function. The search consists of make many  incremental adjustments or updates to the parameter vector and noting the effect of each update on the objective function's output. Each update is related to the gradient 
of the objective function at the parameter component in question, the idea being to descend/ascend 
the slope of the objective function toward a minimum/maximum, depending 
on whether the object function is negatively or positively defined. 
%In the former case, it is generally called an \emph{error} function. 
That is, an objective function can be positive, in which case the goal is to maximize it, 
or negative, in which case it is
called an \emph{error function} and is to be minimized. Autoencoders typically 
employ an error function, although the two approaches are not necessarily at odds 
with each other. 
%\citet{saund:94} proposes a positive log-likelihood objective function 
%to accompany the Noisy-Or mixing function, but I found this function function to be 
%ineffective where the present study's sparse, high-dimensional data points were concerned. 
%It led to unstable, overly dramatic leaps in the learning process. 
Multimorph's objective function is an error function, specifically the \emph{sum-of-squared-error} (SSE)  function, normalized 
by the total number of features across
all data points (i.e., $I \times J$ features):
\begin{equation} \label{eq:sse}
E = \bigg(\frac{1}{2}\bigg) \frac{1}{I \times J}\sum_{i} \sum_{j} {(r_{i,j} - x_{i,j})}^2
\end{equation}
where $r_i,j$ is itself a function, namely the Noisy-OR fuction (see section~\ref{sec:mixing-function}). The reason for factor $\frac{1}{2}$ in equation \eqref{eq:sse} is that it cancels out the coefficient 2 in the first derivative.
% making Multimorph's objective function a composite function.  
%The task in the case of Multimorph is thus one of minimization. 
%In particular, it employs the method presented in
%\citet{cheng-and-li:2012}, which is an \emph{active set} method conducive to 
%\emph{bound-constrained} problems. It is also nonlinear method, i.e., conducive to \emph{nonlinear} problems. 
%These are important attributes for our purposes because \dots.

\subsection{Nonlinear Conjugate Gradient Optimization}
\label{sec:ncg-gen}
Because Multimorph's composite objective function is nonlinear, it requires a nonlinear optimization method.\footnote{Here, we mean \emph{nonlinear} in the standard mathematical sense, i.e., to describe functions whose graphs are not straight lines.} I chose a nonlinear conjugate-gradient method\footnote{In preliminary experiments conjugate-gradiant methods tended to outperform quasi-Newton methods. Moreover, \citet{saund:94} also uses a conjugate-gradient method.} 
%As mentioned above in section~\ref{sec:architecture} (and in subsequent sections), 
%an MCMM has two types of parameters:
%(1) the hidden-unit activities in $\textbf{M}$ and (2) the weights in $\textbf{C}$. (Recall that this 
%distinguishes MCMMs from other types of autoencoders, which do not treat hidden-unit activities as parameters.  
%The search for optimal $\textbf{M}$ and $\textbf{C}$ values is conducted
%via \emph{numerical optimization}, a term that embodies a family of 
%methods characterized by descending the gradient of an error function 
%(or, alternatively, by ascending the gradient of a positively defined objective function). 
%Nonlinear conjugate gradient methods are themselves a subcategory 
%of gradient-based optimization methods, which seek to minimize (or maximize)
%an objective function $f$ by incrementally descending (or ascending) the slope 
%of the objective function.
%, i.e., by incrementally decreasing (or increasing) the value 
%$f(\textbf{W})$, where is a parameter matrix. This descent (or ascent) 
%is achieved by first computing the G, the gradient, or first partial derivative, of 
%$f(\textbf{W})$ w.r.t. W, that is
%\begin{equation}
%
%\end{equation}
%and adding to each component of W a quantity proportional to the negative (or positive) 
%This is done by iteratively adjusting components of $f$'s parameter vector $\textbf{w}$ 
%so that the value $f(\textbf{w})$ is eventually minimized (or maximized).
%For the sake discussion, let us assume a single one type of parameter
%\subsection{The Nonlinear Conjugate Gradient Method}
%\label{sec:ncg-gen}
To facilitate a general description of the class of nonlinear conjugate gradient methods, 
we shall for moment consider a simple model with only one parameter 
%a hypothetical vector we shall call $\textbf{w}$.
vector $\textbf{w}$.\footnote{Sometimes a parameter is most naturally expressed as a matrix (with multiple rows and columns) rather a vector. For instance, if a system should have two layers of nodes, one of $K$ nodes and the other of $J$ nodes, with $K \times J$ weights connecting the two layers, the objective function's parameters (i.e., the weights) would most naturally expressed as a  $K \times J$ matrix. In such a case, however, the matrix can be vectorized---i.e., converted into vectors---via the \textsf{vec()} operation. See, e.g., \citet{fackler:2005} for an explanation of this operation.} % connecting two layers of nodes. 
Of course, as discussed in section~\ref{sec:architecture},
an MCMM actually has \emph{two} types of parameters, namely
the weights in $\textbf{C}$ and the hidden-unit activities in $\textbf{M}$. 
We shall fully examine the workings of this dual-parameter framework in \ref{sec:two-in-one} below.
%If there are $I$ nodes in one layer and $J$ nodes in the other, there are $I \times J$ 
%individual node-to-node weights.

The general process of nonlinear conjugate-gradient optimization 
(i.e., \emph{minimization} in our case)
is shown in algorithm~\ref{alg:gen-cg}. The process consists of a loop, 
which, in each iteration, updates the components of the parameter 
vector $\textbf{w}$ (line \ref{line:param-update-1} in algorithm~\ref{alg:gen-cg}).
Each update to $\textbf{w}$ can be thought of 
as a ``step" composed of both a direction $\textbf{d}$ and a magnitude (or step length), 
the latter being determined by a scaling factor $\alpha$. A step thus takes the form
\begin{equation}\label{eq:gen-update}
\textbf{w} = \textbf{w} + \alpha \, \textbf{d}
\end{equation}
The length of a step is conditioned upon its direction $\textbf{d}$; 
that is, one must decide on a direction $\textbf{d}$ before 
determining how far to go in that direction.
The scalar $alpha$ is computed by a \emph{line-search} function 
that takes the direction $\textbf{d}$
as input, as in line~\ref{line:alpha-update-1} in algorithm~\ref{alg:gen-cg}.
Multimorph's line-search function that makes sure that $\alpha$ satisfies the
Armijo condition \citep{armijo:1966}. %^, i.e., that 

The direction is initialized as the negative of the objective function's gradient
(line ~\ref{line:init-d} in algorithm~\ref{alg:gen-cg}). The gradient of an objective
function $f$ with respective to a parameter vector $\textbf{w}$ is the first partial derivative of $f$ with respect to
$\textbf{w}$, as in equation \eqref{eq:partial-vector}.  The gradient is of the same size (and same dimensions as) as the parameter vector $\textbf{w}$, and there is thus a one-to-one relationship between the parameter's components and the gradient's components.
\begin{equation}
\label{eq:partial-vector}
%\textbf{g} = $\frac{\partial f(\mathbf{w})}{\partial \mathbf{w}} =$
%\[ 
\textbf{g}(\mathbf{w}) = \frac{\partial f(\mathbf{w})}{\partial \mathbf{w}} =
  \begin{bmatrix}
    \frac{\partial f(\mathbf{w})}{\partial w_0}\\
    \frac{\partial f(\mathbf{w})}{\partial w_1} \\
    \frac{\partial f(\mathbf{w})}{\partial w_2}\\
    \vdots \\
	\frac{\partial f(\mathbf{w})}{\partial w_k}\\
	\vdots \\
	\frac{\partial f(\mathbf{w})}{\partial w_K}
  \end{bmatrix}
%\]
\end{equation}
In conjugate-gradient methods, if the goal is to decrease the objective function, the direction vector $\textbf{d}$  is initially set to the negative of the objective function's gradient. The negative of the gradient is simply the negative slope.  The negative slope is a descent direction. 
\begin{equation}
\textbf{d} = - \textbf{g}, \qquad \text{i.e., } \qquad
 \begin{bmatrix}
      d_0\\
    d_1 \\
   d_2\\
    \vdots \\
	d_k\\
	\vdots \\
	d_K
  \end{bmatrix} \quad =
\quad 
-\begin{bmatrix}
    g_0\\
    g_1 \\
   g_2\\
    \vdots \\
	g_k\\
	\vdots \\
	g_K
	\end{bmatrix}
  \end{equation}
Inside the loop, the direction vector $\textbf{d}$ is updated in each iteration, and each update to  $\textbf{d}$ 
takes into account information from the preceding iteration. Thus the current direction and gradient vectors in the in any given iteration are always copied and saved as $\textbf{g}_ {\text{old}}$ and $\textbf{d}_{\text{old}}$, respectively, before they are updated, so that they will be available later.
\begin{algorithm}[h]
 \KwData{parameter vector \textbf{w} along with ancillary data}
\KwResult{optimized parameter vector \textbf{w}}
 Choose $\epsilon_1, \epsilon_2$\;
 Compute gradient: $\textbf{g} := \frac{\partial f(\textbf{w})}{\partial \textbf{w}}$\;
Initialize direction: $\textbf{d} := -\textbf{g}$\; \label{line:init-d}
 \While{$||\textbf{g}|| > \epsilon_1$}{
   Compute $\alpha = \textsc{Line-Search}(\textbf{d})$\;  \label{line:alpha-update-1}
  %Compute $\alpha$ so that $f(\textbf{w}+\alpha \textbf{d}) \geq f(\textbf{w}) +  c \, \alpha \, \textbf{g}^{\textsf{T}}\textbf{d})$ (Armijo rule)\;
  Save parameter vector: \textbf{w}_{\text{old}} := \textbf{w}\;
  Update parameter vector: $\textbf{w} := \textbf{w} + \alpha \textbf{d}$\;  \label{line:param-update-1}
   Save gradient: $\textbf{g}_{\text{old}} := \textbf{g}$ \; \label{line:save-gradient-1}
   Save direction: $\textbf{d}_{\text{old}} := \textbf{d}$ \; \label{line:save-direction-1}
  Update $\textbf{g} := \frac{\partial f(\textbf{w})}{\partial \textbf{w}}$\;
  Compute $\beta$\;
  Update direction: $\textbf{d} := -\textbf{g} + \beta \textbf{d}_{\text{old}}$\;  \label{line:d-update-1}
  %Update $t := t + 1$\;
  \If{$f(\textbf{w}_{\text{old}}) - f(\textbf{w}) < \epsilon_2$}{
   Stop\;
   }
 }
\caption{General (nonlinear) conjugate-gradient algorithm. }
\label{alg:gen-cg}
\end{algorithm}

%The parameter vector $\textbf{w}$ is thus updated according to a rule that incorporates 
%both $\textbf{d}$ and $\alpha$, namely:

%Just before the update, the current parameter vector is saved in the variable $\textbf{w}_{\text{old}}$, as in line~\ref{line:param} in algorithm~\ref{alg:gen-cg}). 
%The scaling factor $\alpha$
%and the direction vector $\textbf{d}$ must be recomputed at each iteration. 
%Note that the vectors $\textbf{w}$, $\textbf{g}$, and $\textbf{d}$ all have the same number of components.
%Thus, there is for each parameter component a distinct gradient component and a distinct direction component. $d_{j,k}$ 
%for each parameter component $w_{j,k}$. The direction matrix $\textbf{D}$ 
%thus has the same dimensions as the parameter matrix. 

There are various ways to compute $\textbf{d}$, each corresponding to a 
particular type of optimization algorithm. The conjugate-gradient family, 
for instance, is the class of optimization algorithms wherein $\textbf{d}$ is a function 
of a special ratio called $\beta$, as in
\begin{equation}\label{eq:basic-d}
\textbf{d}= -\textbf{g}  + \beta \textbf{d}_{\text{old}} 
\end{equation}
where \textbf{g} is the gradient (first partial derivative) of the error function with respect to the parameter vector $\textbf{w}$ 
Conjugate-gradient methods are further subdivided according to how they calculate $\beta$.
 %\citet[For an overview of the available options, see][] 
\citet{hager:2006} provide an overview of the various existing recipes for $\beta$.
One example is the Polak--Ribi\'{e}re--Polyak (PRP) formula:
\begin{equation}\label{eq:PRP}
\beta^{PRP} = \frac{(\textbf{g})^{\textsf{T}}{\textbf{y}}}{{||(\textbf{g}_{\text{old}})||}^2}
\end{equation}
where
\begin{equation}\label{eq:y}
\textbf{y} = \textbf{g} - \textbf{g}_{\text{old}} 
\end{equation}

For Multimorph, I chose the optimization method of \citet{cheng-and-li:2012}, which is related to the Polak--Ribi\'{e}re--Polyak  method. In particular, it is based on the Polak--Ribi\'{e}re--Polyak variant proposed by \citet{zhang-et-al:2006}. This variant
%uses the Polak--Ribi\'{e}re--Polyak formula for $\beta$. But more precisely, conjugate-gradient
%\citet{cheng-and-li:2012} use a modified version of the original Polak--Ribi\'{e}re--Polyak method. The modified version, first proposed by \citet{zhang-et-al:2006}, 
introduces a third term, namely  $-\eta \textbf{y}$, to the direction formula \eqref{eq:basic-d}, as follows:
\begin{equation}\label{eq:mod-d-update}
\textbf{d} = -\textbf{g}  + \beta^{PRP} \textbf{d}_{\text{old}} - \eta \textbf{y}
\end{equation}
where 
\begin{equation}
\label{eq:eta}
\eta = \frac{\textbf{g}^{\textsf{T}}\textbf{d}_{\text{old}}}{{||\textbf{g}_{\text{old}})||}^2}
\end{equation}
The pseudocode for the \textsc{modified-PRP} method is shown in algorithm~\ref{alg:mod-prp-cg}. \citet{cheng-and-li:2012} essentially take \textsc{modified-PRP} and turn it into an \emph{active-set} method as described below in section~\ref{sec:active-set}.
%by introducing to it mechanisms for partitioning parameter components into \emph{active} and \emph{inactive} sets, as described below in section~\ref{sec:active-set}.
\begin{algorithm}[ht]
\KwData{parameter vector \textbf{w} to be optimized along with ancillary data}
\KwResult{optimized parameter vector \textbf{w}}
	 Choose $\epsilon_1, \epsilon_2$\;
 Compute gradient: $\textbf{g} = \frac{\partial f(\mathbf{w})}{\partial \mathbf{w}} $\;
 Initialize direction: $\mathbf{d} = -\textbf{g} $\; \label{line:init-d-2}
 \While{$||\textbf{g} || > \epsilon_1$}{
  Compute $\alpha = \textsc{Line-Search}(\textbf{d})$\;   \label{line:alpha-update-2}
   Save parameter vector: $\textbf{w}_{\text{old}} := \textbf{w}$\;
  Update parameter vector: $\textbf{w} := \textbf{w} + \alpha \textbf{d}$\;   \label{line:param-update-2}
      \If{$f(\textbf{w}_{\text{old}}) - f(\textbf{w}) < \epsilon_2$}{
   Stop\;
   }
   Save gradient: $\textbf{g}_{\text{old}} := \textbf{g}$\;
   Save direction: $\textbf{d}_{\text{old}} := \textbf{d}$ \;
  Update gradient $\textbf{g} := \nabla f(\textbf{w})$\;
  Update $\textbf{y} := \textbf{g} - \textbf{g}_{\text{old}}$\;
  Compute $\beta^{PRP} := \frac{\textbf{g}^{\textsf{T}}{\textbf{y}}}{{||\textbf{g}_{\text{old}}||}^2}$\;
  Update direction: $\textbf{d} := -\textbf{g} + \beta^{PRP} \textbf{d}_{\text{old}} - \eta \textbf{y}$\;   \label{line:d-update-2}
   %Update $t := t + 1$\;
 }
\caption{\textsc{Modified-PRP}. \textit{Pseudocode for the modified version of Polak--Ribi\'{e}re--Polyak conjugate-gradient algorithm proposed by \citet{zhang-et-al:2006}.}} %\citet{zhang-et-al:2006}}.} % of  Cheng and Li (2012)}
\label{alg:mod-prp-cg}
\end{algorithm}

%Initially, at $t = 0$, the search direction $\textbf{D}$ is set to the \emph{negative} gradient of the error function; that is,
%$\textbf{D} = - \textbf{G}$.  The negative gradient is simply the negative of the error curve's slope. It is thus  a descent direction.
%In the second iteration, updates to the direction matrix begin to take into account information from the immediately preceding iteration.
% ``A stationary point of a differentiable function of one variable is a 
% point on the graph of the function where the function's derivative is zero. 
% Informally, it is a point where the function stops increasing or decreasing 
% (hence the name).'' However, in a case of bound constrained optimization, 
% wherein each variable's value is constrained by a lower and upper 
% bound, the derivative may not be zero with respect to \emph{every} variable. 
% In such a case, therefore, a stationary point is more accurately defined as follows:
%What is a nonlinear conjugate gradient method?
%
%\begin{equation}
%p_k = -B_{k-1}\nabla f_k
%\end{equation}

\subsection{A \textit{Bound-Constrained} Variant \\ of Nonlinear Conjugate-Gradient Optimization}
\label{sec:active-set}
%The method of \citet{cheng-and-li:2012} satisfies two major criteria 
%concerning Multimorph's  learning rate. 
%\begin{enumerate} 
%\item 
Multimorph's learning task is a \emph{bound-constrained} problem. That is,
the problem of minimizing $E$ with respect to $\mathbf{M}$ and $\mathbf{C}$ 
(i.e., the problem of \emph{optimizing}  $\mathbf{M}$ and $\mathbf{C}$) is 
subject to constraints that take the form of 
lower and upper bounds (namely, 0 and 1) on each component in 
$\mathbf{M}$ and $\mathbf{C}$. That is,
\begin{align}
\label{eq:bounds-m}
0 \le m_{i,k} \le 1 \quad \text{$\forall i \in I$, $\forall k \in K$} \\
\label{eq:bounds-c}
0 \le c_{j,k} \le 1 \quad \text{$\forall j \in J$, $\forall k  \in K$}
\end{align}
These constraints are necessary to allow the reconstruction activities $r_{i,j}$ to approach 
and eventually match the component values of the original feature vectors, 0's and 1's in the case of Multimorph's data.

The constraints~\ref{eq:bounds-m} and \ref{eq:bounds-c} are necessary to ensure that Multimorph's mixing function, (the Noisy-OR function) always outputs reconstruction activities in $[0,1]$.
%feature vectors consist of 0's and 1's. Given Multimorph's mixing function, namely the Noisy-OR 
%function (see section~\ref{sec:mixing-function}), the above constraints are necessary to yield 
%reconstruction vectors of 0's and 1's.
%\item Multimorph's objective function (which contains the Noisy-Or mixing function) is nonlinear. Therefore, Multimorph requires a nonlinear optimization method.  The method of \citet{cheng-and-li:2012} is conducive to nonlinear optimization method.
%\end{enumerate}

I therefore chose a particular variant of the nonlinear conjugate gradient method that was designed to honor bound constraints like \eqref{eq:bounds-m} and \eqref{eq:bounds-c}, namely the \emph{active-set} method of\citet{cheng-and-li:2012}. 
%This algorithm is an \emph{active-set, bound-constrained} method, which means
An active-set methods classifies
%The algorithm of \citet{cheng-and-li:2012} 
%satisfies this requirement because it 
each parameter component according to the following:
\begin{enumerate}
\item its position relative to the constraint boundaries ($0$ and $1$ in the case of Multimorph)
\item if the component is at one the boundaries, the sign of the error gradient with respect to this component
\end{enumerate}
%If a parameter component is at one of boundaries, i.e., at $0$ or $1$, the sign of the gradient at this component 
%must also taken into count. 
For example, suppose $\textbf{m}$ is our parameter vector, and the task of optimizing $\textbf{m}$ is
subject to constraint~\ref{eq:bounds-m}. If the $k$th component $m_{i,k}$ is $0$, and the gradient of $E$ 
at $m_{i,k}$ is positive, then $m_{i,k}$ \emph{must not} be updated. This is
because the sign of an update in the descent direction is the negative (i.e., the opposite) of the gradient's sign. Therefore,
an update in these circumstances would be negative and thus push $m_{i,k}$ below $0$.
%and thus violate constraint \ref{eq:bounds-m}.
The constraint $0 \leq m_{i,k }\leq 1$ is thus said to be %\emph{active} for stay within $[0,1]$ would thus be 
\emph{active} 
for $m_{i,k}$, and the algorithm classifies $m_{i,k}$ as a member of the ``active set'' of 
$\textbf{m}$'s components (i.e., the set of components for which the constraint is active).
%the ``active'' set, i.e., the set of components at which the constraint is active.
However, suppose that the gradient with respect to $m_{i,k}$ were reversed, i.e., is positive, while everything else were kept the same, i.e., suppose 
$m_{i,k}$ were still $0$, but the gradient with respect to $m_{i,k}$ were \emph{negative} 
rather than positive. 
In this case, $m_{i,k}$ would be placed in an \emph{inactive} set (i.e., our constraint 
would not be active at $m_{i,k}$ ). This is because a negative
gradient at $m_{i,k}$ would be leading downward into the licit region $[0,1]$, and 
thus an update would move $m_{i,k}$ away from the constraint boundary $0$ in the 
positive direction. 
%\eqref{eq:bounds-m} would be 
%inactive at $m_{i,k}$, and $m_{i,k}$ would thus be placed in an inactive set. 
%This is because the gradient at $m_{i,k}$ would be leading downward into the licit region $[0,1]$, and thus an update would both move $m_{i,k}$ away 
%from $o$ (in the positive direction) \emph{and} further reduce $E$. 
By classifying 
parameter components in this way, active-set methods can tailor parameter updates 
so that constraints are honored. 

Active set methods typically take an 
additional step to guarantee that parameter components stay within the boundaries 
prescribed by the constraint(s): They ``clip" each parameter component that ends up being out 
of bounds after a round of updates. In the case of the constraint \eqref{eq:bounds-m}, i.e., $0 \leq m_{i,k} \leq 1$,
any component $m_{i,k}$ that is less than $0$ is clipped back to $0$, and any component that is greater 
than $1$ is clipped back to 1 \citep{ni:yuan:1997}.

%The method of \citet{cheng-and-li:2012} also satisfies the second criterion, 
%since it belongs to the nonlinear conjugate-gradient family of methods (see below).

\subsection{Two optimization problems in one}
\label{sec:two-in-one}
Until now, our discussion in this section has been for mostly framed in terms of a 
\emph{single} parameter vector, the generic vector $\textbf{w}$. 
This has been an expedient simplification, allowing us to focus on the parameter 
updates themselves while postponing
the discussion of how an MCMM optimizes two separate parameter matrices
jointly, particularly when the values of one depend on those of the other. 
It is to this question that we now turn.

An MCMM does not \emph{simultaneously} induce the 
$\textbf{M}$ and $\textbf{C}$ matrices, but rather 
alternates between two separate optimization states, \textsc{Optimize-M} and \textsc{Optimize-C}. 
In each state, one matrix is held fixed (i.e., treated as constant), 
while the other is optimized. The fixed matrix serves as the``known" set 
of values so that the other matrix can take its turn as the unknown 
parameter, i.e., the parameter to be optimized.
Thus, in \textsc{Optimize-M}, $\textbf{C}$ is treated as constant, i.e., 
as the known matrix, and $\textbf{M}$ is optimized to the extent it can be. 
Then, in \textsc{Optimize-C}, 
the newly updated $\textbf{M}$ takes its turn as the known matrix, 
while $\textbf{C}$ is optimized to the extent it can be. The alternation 
between these states is essentially the same strategy as that found in 
Expectation Maximization \citep{dempster-et-al:1977}.
%\textbf{M}$'s components are the variables, and the $\textbf{C}$'s components are constants. In the other state, the components of $\textbf{C}$ become the variables, while the components of $\textbf{M}$ are treated as constants. The MCMM alternates between these two states, holding one matrix fixed while optimizing its counterpart. At any given time, the fixed matrix serves as a``known" set of values so that the other matrix can be the unknown parameter, even though both are unknown at the beginning of the process. This is essentially the same strategy as that found in Expectation Maximization (EM).
\textsc{Optimize-M} and \textsc{Optimize-C} are implemented as two subroutines 
nested within a loop, as shown in algorithm~\ref{alg:m-and-c}. %beginning on line~\ref{line:alternation-loop}). 
In each iteration, \textsc{Optimize-M}  and \textsc{Optimize-M} are each run exactly once, 
with \textsc{Optimize-M} preceding \textsc{Optimize-C} (i.e.,\textsc{Optimize-M} 
terminates before \textsc{Optimize-C} is launched). Multiple iterations thus effectively 
give rise to an alternation between two states, hence the name \textsc{Alternation-Loop} in the caption of algorithm~\ref{alg:m-and-c}.
%This alternation loop iterates until $E$ stops decreasing significantly from iteration to iteration.
%\begin{algorithm}[h]
%Initialize cluster count $K = 1$\;
%Initialize $\textbf{M}$ and $\textbf{C}$\;
%\While{\textbf{True}}{
%Choose $\epsilon, \epsilon^{\prime}$\;
%Evaluate $E$\;\label{line:init-E}
% \While{\textbf{True}}{ \label{line:alternation-loop}
%Save current error: $E_{\text{old}} = E$\;
%\textsc{Optimize-M}\;
%\textsc{Optimize-C}\;
%Evaluate $E$\;
%      \If{$E_{\text{old}} - E) < \epsilon$}{
%   Stop\;
%   }
%   }
%   \If{$E < \epsilon^{\prime}$}{
%   Stop\;
%   }
%   Split worst cluster\;
%   Increment cluster count: $K := K + 1$\;
%   }
%\caption{\emph{Alternation Loop}: This loop effects an alternation between \textsc{Optimize-M} and \textsc{Optimize-C}.}
%\label{alg:m-and-c}
%\end{algorithm}

\begin{algorithm}[h]
\KwData{\textbf{M}, \textbf{C}, \textbf{X}, \textbf{R}}
\KwResult{Optimized \textbf{M} and \textbf{C} matrices}
 Evaluate Error: $E = \frac{1}{2} \big(\frac{1}{I\times J}\big) \sum_i \sum_j (r_{i,j} - x_{i,j})^2$\;\label{line:init-E} 
 \While{\textbf{True}}{   \label{line:init-alternation-loop}
Save current Error: $E_{\text{old}} = E$\;
\textsc{Optimize-M}(\textbf{M}, \textbf{C}, \textbf{X}, \textbf{R})\;
\textsc{Optimize-C}(\textbf{C}, \textbf{M}, \textbf{X}, \textbf{R})\;
Evaluate Error: $E = \frac{1}{2} \big( \frac{1}{I\times J} \big) \sum_i \sum_j (r_{i,j} - x_{i,j})^2$\;\label{line:update-E}
      \If{$E_{\text{old}} - E < \epsilon$}{
   Stop\;
   }
}
\caption{\textsc{Alternation-Loop}: \textit{This loop effects an alternation between \textsc{Optimize-M} and \textsc{Optimize-C}.}}
\label{alg:m-and-c}
\end{algorithm}

Both subroutines apply the 
optimization method of \citet{cheng-and-li:2012}, which if we have 
dubbed \textsc{Active-Set-Modified-PRP}, since it is an active-set version of the 
\textsc{Modified-PRP} method (algorithm~\ref{alg:mod-prp-cg}). 
\textsc{Optimize-M} contains a version of \textsc{Active-Set-Modified-PRP} tailored for
 the optimization of $\textbf{M}$ matrix, a version that we will call \textsc{Active-Set-Modified-PRP-for-M}.
  That is, 
that takes one cluster-membership vector $\textbf{m}_i$ at a time (for $i \in I$), 
since each cluster-membership vector must be optimized separately. 
Thus, \textsc{Optimize-M} is essentially a loop that iteratively calls 
\textsc{Active-Set-Modified-PRP-for-M} $I$ times, once for each row index $i$ in 
the matrix $\textbf{M}$, as shown in algorithm~\ref{alg:opt-M}.
\textsc{Optimize-C}, by contrast, calls \textsc{Active-Set-Modified-PRP-for-C} and does so just once; no loop is necessary. 
Rather, it feeds the \emph{entire} matrix $\textbf{C}$ to 
\textsc{Active-Set-Modified-PRP-for-C}. The matrix $\textbf{C}$ optimized 
as a whole rather than according to a row-by-row basis.
because it applies to all data points universally, 
whereas any given cluster-membership vector $\textbf{m}_i$ is relevant only to one 
data point (namely, the $i$th data point).

\begin{algorithm}[ht]
\KwData{\textbf{D}, \textbf{G}, \textbf{w}}
\KwResult{Constrained \textbf{D}}
	\For{$n \in N$}{
		% in the case of descent to toward minimum, is the Grad positive or negative?
		% --> We want gTd to be negative, and d = -Grad initially. That is, d and Grad
		% point in opposing directions. When this is no longer true, the model
		% is moving away from its target.
		% However, d is not always going to be negative; its sign just needs to oppose
		%the Gradient's sign.
		\If {$l < \textsf{vec}(\textbf{C})[n] < u$}{
			$\textsf{vec}(\textbf{D})[n] = -\textsf{vec}(\textbf{G})[n]$\;
		}
		\If {$\textsf{vec}(\textbf{C}[n]) = l$}{
			\eIf {$\textsf{vec}(\textbf{G})[n] \geq 0$}{
				$\textsf{vec}(\textbf{D})[n] = 0$\;
			}{
				$\textsf{vec}(\textbf{D})[n] = -\textsf{vec}(\textbf{G})[n]$\;
			}
		}
		\If {$\textsf{vec}(\textbf{C})[n] = u$} { 
			\eIf {$\textsf{vec}(\textbf{G})[n] \geq 0$}{
				 $\textsf{vec}(\textbf{D})[n] = 0$\;
			}{
				$\textsf{vec}(\textbf{D})[n] = -\textsf{vec}(\textbf{G})[n]$\;
			}
		}
	}
\caption{This procedure identifies and sets to zero any direction component that is going to cause its corresponding parameter component either to fall below the lower bound $l$ or exceed the upper bound $u$. It thus in effect cancels the update to this parameter component.}
\label{alg:constrained-d}
\end{algorithm}
%\textsc{Optimize-C} follows exactly the process outlined in section~\ref{sec:num-opt}, 
%except with  $\textbf{C}$ taking the place of the generic matrix $\textbf{W}$. 
%\textsc{Optimize-M} is a somewhat different case in that it is divided 
%into $I$ distinct optimization problems, one for each of its rows. 
%Recall from section~\ref{sec:architecture} that each row in $\textbf{M}$ 
%is the cluster-membership vector for a distinct data point. Each data point is 
%independent, and therefore the optimization of each data point's cluster membership 
%vector is an independent optimization problem. Thus, in the case of \textsc{Optimize-M}, 
%each \emph{vector} $\textbf{m}_i$ takes the place the generic parameter vector $\textbf{w}$.


%because each row in $\textbf{M}$ corresponds to a distinct data point, and all data points are treated as
%independent instances. (Specifically, each $\textbf{m}_i$
%is the cluster-membership vector for the $i$th data point, as described in section~\ref{sec:architecture}.) 
%Recall from section~\ref{sec:architecture}, and all data points is regarded as an independent instance.
%that each row in $\textbf{M}$ corresponds to a particular data point, and each datapecifically 
%each of which is optimized separately. 

% would be replaced with $\textbf{C}$; the entire $J \times K$ matrix is optimized at 


%the moment in section~\ref{sec:architecture}, MCMMs actually have two fundamentally distinct parameter matrices, namely $\textbf{M}$, the matrix of hidden-unit activities (or cluster-membership activities) and $\textbf{C}$, the matrix of weights connecting hidden units to surface units. We will now extend the single-parameter case to the dual-parameter setup found in 
%The MCMM's learning process is similar to Expectation Maximization
%(EM) in that at any given time it holds one set of variables. 
%fixed while optimizing the other set. We thus have two functions, \textsc{Optimize-M}
%and \textsc{Optimize-C}, which take turns optimizing their respective matrices $\mathbf{M}$ 
%\and $\mathbf{C}$:
%While In order to optimize $\mathbf{M}$, the weights $\mathbf{C}$ must be
%treated as constants. Then, the cluster-membership matrix $\mathbf{M}$ 
%is held fixed so that \textsc{Optimize-C} can optimize $\mathbf{C}$. 
%The particular optimization algorithm in both \textsc{Optimize-M} and \textsc{Optimize-C} was
%a variant of the Polak--Ribi\'{e}re--Polyak method devised by \citet{cheng-and-li:2012}. This is method is conducive to the present case for two reasons: (1) While it is similar to the conjugate-gradient method, in particular, the version of the conjugate-gradient that employs the Polak--Ribi\'{e}re--Polyak update rule for $\beta$,  it is designed for non-linear optimization problems. 
%\textit{non-linear bound-constrained} optimization.
%for Large-Scale Nonlinear Bound Constrained
%Optimization
%Bound-constrained optimization. 
% However, if the algorithm is to optimize $\mathbf{M}$, it must first
% know $\mathbf{C}$, and vice versa.  Therefore, the algorithm can only
% focus on matrix at time, optimizing the one while holding the other
% fixed.
% The learning process consists of two distinct optimization
% functions:
%, \textsc{Optimize-M} and \textsc{Optimize-C}.
%\begin{description}
%\item[
% \textsc{Optimize-M} holds $\mathbf{C}$ fixed in order to optimize the
% cluster activity vectors in $\mathbf{M}$, while \textsc{Optimize-C}
% holds $\mathbf{M}$ fixed in order to optimize the cluster centroids in
% $\mathbf{C}$.
%\end{description}

%\paragraph{\textsc{Optimize-M}.}
%The subroutine \textsc{Optimize-M}  loops over the $I$ cluster-activity vectors $\mathbf{m}_i$ in
%$\mathbf{M}$, optimizing each one separately.  The optimization algorithm itself is detailed in 
%\citet{cheng-and-li:2012}.
%%, one at a time.
%%in $\mathbf{M}$, 
%For each $\mathbf{m}_i$, \textsc{Optimize-M} enters an optimization 
%loop over its $K$ components, adjusting each 
%$m_{i,k}$ by a
%quantity proportional to the negative gradient of $E$ at $m_{i,k}$. 
%%\marginpar{How is this proportion determined?} 
%This loop repeats until $E$
%ceases to decrease significantly,
%whereupon \textsc{Optimize-M} proceeds to the next $\mathbf{m}_i$. 

%\begin{algorithm}[h]
%\KwData{Matrices: \textbf{X}, \textbf{R}, \textbf{M}, \textbf{C}; Scalars: $I$, $J$, $K$}
%\KwResult{Optimized \textbf{M} matrix}
%Choose $\epsilon_1,\epsilon_2$\;
% \For{$i \in \text{range}(0,I)$}{ \label{line:alternation-loop}
%Evaluate error: $e = \frac{1}{J}\frac{1}{2}\sum_j x_{i,j} - r_{i,j})^2$\;\label{line:init-e}
% Compute gradient: $\textbf{g} = \frac{\partial e}{\partial \textbf{m}_i}$\;
% Initialize direction: $\textbf{d} = -\textbf{g}$\;
%  \While{$||\textbf{g} || > \epsilon_1$}{
%  Compute $\alpha = \textsc{Line-Search}(\textbf{d})$\;   \label{line:alpha-update-2}
%%  Compute $\alpha$ so that $f(\textbf{w}+\alpha \textbf{d}) \geq f(\textbf{w}) +  c \, \alpha \, \textbf{g}^{\textsf{T}}\textbf{d})$ \;
%   %Save parameter vector: $\textbf{w}_{\text{old}} := \textbf{w}$\;
%  Update parameter vector: $\textbf{m}_i := \textbf{m}_i + \alpha \textbf{d}$\;   \label{line:param-update-2}
%Compute $\textbf{r}_i = \text{\textsc{predict}-r}(\textbf{m}_i, \textbf{c}_j,  J, K)$\;
% Save current error: $e_{\text{old}} = e$\;
%  Reevaluate error: $e = \frac{1}{J}\frac{1}{2}\sum_j (x_{i,j} - r_{i,j})^2$\;\label{line:eval-e}
%      \If{$e_{\text{old}} - e < \epsilon_2$}{
%   Stop\;
%   }
%   Save current gradient: $\textbf{g}_{\text{old}} := \textbf{g}$\;
%   Save current direction: $\textbf{d}_{\text{old}} := \textbf{d}$\;
%  Compute new gradient: $\textbf{g} = \frac{\partial e}{\partial \textbf{m}_i}$\;
%  Compute new $\textbf{y} := \textbf{g} - \textbf{g}_{\text{old}}$\;
%  Compute new $\beta^{PRP} := \frac{\textbf{g}^{\textsf{T}}{\textbf{y}}}{{||\textbf{g}_{\text{old}}||}^2}$\;
%  Compute new direction: $\textbf{d} := -\textbf{g} + \beta^{PRP} \textbf{d}_{\text{old}} - \eta \textbf{y}$\;   \label{line:d-update-2}
% }
% }
%\caption{\textsc{Optimize-M}}
%\label{alg:opt-M}
%\end{algorithm}

\begin{algorithm}[h]
\KwData{\textbf{M}, \textbf{C},\textbf{X}, \textbf{R}}
\KwResult{Optimized \textbf{M} matrix}
$I = \textsc{Number-of-Rows}(\textbf{M})$\;
 \For{$i \in I$}{ \label{line:init-loop}
\textsc{Active-Set-Modified-PRP-for-M}(\textbf{m}_i, \textbf{C}, \textbf{x}_i, \textbf{r}_i)\;
 }
\caption{\textsc{Optimize-M}}
\label{alg:opt-M}
\end{algorithm}

\begin{algorithm}[h]
\KwData{\textbf{C}, \textbf{M}, \textbf{X}, \textbf{R}}
\KwResult{Optimized \textbf{C} matrix}
%$I = \textsc{Number-of-Rows{\textbf{M}}$\;
%$J = \textsc{Number-of-Rows{\textbf{C}}$\;
\textsc{Active-Set-Modified-PRP-for-C}(\textbf{C}, \textbf{M}, \textbf{X}, \textbf{R})\;
\caption{\textsc{Optimize-C}}
\label{alg:opt-M}
\end{algorithm}
%\paragraph{\textsc{Optimize-C}.} 
%The function \textsc{Optimize-C} consists of a single optimization loop over the 
%entire matrix
%$\mathbf{C}$. Each $c_{j,k}$ is adjusted by a quantity
%proportional to the negative gradient of $E$ with respect to $c_{j,k}$.
%%, or $-\frac{\partial E}{\partial c_{j,k}}$.
%%(in the case of gradient descent).  
%%Since $E$ is simply
%%$\sum_i e_i$, i.e., the sum of the errors of the individual data-point
%%reconstructions, $-\frac{\partial E}{\partial c_{j,k}} = -\sum_i
%%\frac{\partial e_i}{\partial c_{j,k}}$.  Thus, the adjustment to each
%%$c_{j,k}$ takes into account the error of each reconstructed vector
%%$\mathbf{r}_i$ in $\mathbf{R}$. However,
%Unlike \textsc{Optimize-M}, which comprises $I$ separate optimization
%loops, \textsc{Optimize-C} consists of just one.
%%optimizing the $\mathbf{C}$ matrix as a whole.  
%When each of its $J \times K$
%components has been adjusted, one round of updates to $\mathbf{C}$ is
%complete.  $E$ is reevaluated only between completed rounds of
%updates. If the change in $E$ remains significant, another round begins.  

\subsection{Cluster Splitting}
\label{sec:cl-split}
%Both \textsc{Optimize-M} and \textsc{Optimize-C} are enclosed within 
%an ``alternation loop" 
%that alternates between the two functions, holding $\mathbf{C}$ fixed
%during \textsc{Optimize-M}, and vice versa.
The alternation loop (algorithm~{\ref{alg:m-and-c}) is nested within a still larger loop, which we shall call the ``outermost loop,'' which is responsible for increasing the number of clusters as necessary. That is, it launches the alternation loop (which alternates between \textsc{Optimize-M} and \textsc{Optimize-C}) and lets it run until it terminates (i.e., when $E$ stops decreasing significantly). Then, if the outermost loop's stopping condition has not yet been met, it adds a new cluster, and starts a new iteration (wherein the alternation loop is launched again). The stopping condition of the outermost loop is (in principle) that $E$ has fallen below a certain constant $\epsilon$.

 The outermost loop starts with a single cluster. This means that, initially, 
 $K = 1$, and thus the initial dimensions of the matrix $\textbf{M}$ are 
 $I \times 1$, and those of $\textbf{C}$ are $J \times 1$. Each cell in $\textbf{M}$'s 
 single initial column is initialized to $0.5$, while each cell in $\textbf{C}$'s 
 single initial row is initialized to a random number between $0$ and $1$.  

To the add a cluster, the system splits a cluster, specifically the worst cluster. To find the worst cluster, 
 the system iterates through all existing clusters, for $k \in K$, and at each $k$, 
it computes E^{\prime}_{k}, the MCMM's error \emph{without} the 
 $k$th cluster.  The worst cluster is the one whose \emph{absence} leads to the best model, 
 i.e., the one with the lowest $E^{\prime}$, and the index of the worst cluster is thus the 
 $k$ such that E^{\prime}_{k} is lowest. If this index is $h$, then cluster $h$ is the worst cluster and thus the cluster to split. To split cluster $h$, 
 the system would copy column $h$ in $\textbf{M}$ as pastes it as the new 
 rightmost column in $\textbf{M}$ (so that its index is $K$)  At this point, 
 the column indexed $h$ and the new rightmost columns have identical values.  
 Some noise is thus introduced to both to encourage the two clusters to diverge. 
 Similarly, column $h$ in $\textbf{C}$ is duplicated, resulting in a centroid vector 
 for both cluster $h$ and cluster $K$.
 
% initiall $K$, the number of clusters, by setting it to it. Thus, each iteration of the outermost loop makes the alternation loop iterate until the system's error $E$ stops decreasing significantly. within the larger loop shown in algorithm~\ref{alg:outer-loop}.  iterates until $E$ cannot be decreased further. At
%this point, an ``outer loop''
%splits the cluster which contributes the most to the error, adds one
%to the cluster count $K$, and restarts the alternation loop. The outer loop
%repeats until it reaches an overall stopping criterion, such as ``$E$ is lower than some small constant.''
%; otherwise, the function terminates.

\begin{algorithm}[h]
Initialize cluster count $K = 1$\;
Initialize $\textbf{M}$ and $\textbf{C}$\;
Choose $\epsilon$\;
Evaluate $E = \frac{1}{I\times J} \frac{1}{2} \sum_i \sum_j (r_{i,j} - x_{i,j})^2$\;\label{line:init-E}
\While{$E \geq \epsilon$}{
% \While{\textbf{True}}{\label{line:init-alternation-loop}
%Save current Error: $E_{\text{old}} = E$\;
%\textsc{Optimize-M}\;
%\textsc{Optimize-C}\;
%Evaluate $E = \frac{1}{I\times J} \frac{1}{2} \sum_i \sum_j (r_{i,j} - x_{i,j})^2$\;\label{line:Update-E}
%      \If{$E_{\text{old}} - E < \epsilon$}{
%   Stop\;
%   }
%   }
   \textsc{Alternation-Loop}(\textbf{M},\textbf{C},\textbf{X},\textbf{R}, $I$, $J$, $K$)\;
   Split worst cluster\;
   Increment cluster count: $K := K + 1$\;
   Evaluate $E = \frac{1}{2} \big(\frac{1}{I\times J}\big) \sum_i \sum_j (r_{i,j} - x_{i,j})^2$\;\label{line:update-E}
   }
\caption{\textsc{Outer-Loop}}
\label{alg:outer-loop}
\end{algorithm}

% \subsection{Bound Constrained Optimization}
%The optimization task is subject to the constraint %$0 le m_{i,k} ge 1$
%that no value in $\mathbf{M}$ or $\mathbf{C}$ may exceed 1 or fall below 0. 
%In other words,
%it is a task of bound constrained optimization. Thus, whenever a value in either 
%$\mathbf{M}$ or $\mathbf{C}$ is about
%to fall below 0, it is set to 0. Likewise, whenever a value is about to exceed 1, 
%it is set to 1 \citep{ni:yuan:1997}.
 %It repeatedly iterates over the $J \times K$ components of $\mathbf{C}$, stopping only when it finds a local minimum of $E$ is
  
%What is the inner loop and what is the outer loop?
%There are actually two inner loops: Opt-M is immediately followed by Opt-C. These two are encapsulated within a larger outer loop.
%What is the stopping criterion for the inner loop? What happens when this criterion is met?
%What is the stopping criterion for the outer loop?
%Cluster Splitting. Where does this enter the narrative?
%At this point, the algorithm then finds the worst cluster among the currently existing clusters and splits it, thereby increasing the cluster count $K$ by one.

\subsection{A Simple MCMM Example}
\label{subsec:example}

Figure~\ref{fig:example-1} shows a simple MCMM at two distinct stages. Subfigure~\ref{fig:example:subfig1-1} shows the MCMM before convergence, while learning is still in progress. The surface and hidden-unit activities at this stage are still greater than $0$ and less than $1$, though they are on their way to either $0$ or $1$. Subfigure~\ref{fig:example:subfig2-1}, on the other hand, shows the MCMM after it has converged, when the learning process is complete.

The input data matrix for the MCMM in figure~\ref{fig:example-1} consists of three data points (i.e., $I = 3$).
The hidden cluster activities $\mathbf{M}$, the weights $\mathbf{C}$, the surface-unit vectors $\mathbf{R}$,
and the mixing function together constitute a model that reproduces the
observed data points $\mathbf{X}$.
The nodes $m_{i,k}$ are the hidden units and represent cluster-membership activities. There are three cluster-membership vectors $m_{i}$, one for each $i \in I$, i.e., one for each data point. Because there are two clusters, each cluster-membership vector has two components. For instance, $\textbf{m}_{0} = [0, 1]$, which means the first data point $\textbf{x}_{0}$ does not belong to the first cluster, but \emph{does} belong to the second. By contrast, $\textbf{m}_{1} = [1,0]$ indicates that $\textbf{x}_{1}$ belongs to first cluster, but not to the second. Finally, $\textbf{m}_{2} = [1,1]$ indicates that  $\textbf{x}_{2}$ belongs to \emph{both} clusters. 

These cluster-membership vectors make more sense when we consider the this MCMM's $J \times K$ ($3 \times 2$) weight matrix $\textbf{C}$. The two column-vectors $\textbf{c}_0 = [0,1,0]$ and $\textbf{c}_1 = [1,0,1]$ are the \emph{centroids} of the first and second clusters, respectively. A cluster's centroid is a sort of average or composite of the cluster's member vectors; it represents the feature values that a cluster's members have in common and can thus serve as a label for a cluster. The first data point $\textbf{x}_0 = [0,1,0]$ is thus clearly a the cluster whose centroid  $[0,1,0]$. Similarly, the second data point $\textbf{x}_1 = [1,0,1]$ is clearly of the member of the cluster whose centroid is $[1,0,1]$. But just as clearly, the influence of both clusters is evident in the third data point $\textbf{x}_2 = [1,1,1]$.

Table~\ref{tab:cl-members} shows the members as well as the centroids of each of the two clusters: Notice that the active components in each cluster centroid correspond to the shared component values among the cluster's members.

\begin{table}[b]
\begin{mdframed}
\centering
\setlength{\extrarowheight}{4pt}
\begin{tabular}{ccc} 
Cluster index ($k$) & Member Data Points & Cluster Centroid \\ \hline\hline
 $0$ &  $[0,\mathbf{1},0]$, $[1,\mathbf{1},1]$ & $[0,1,0]$\\ \hline
 $1$  & $[\mathbf{1},0,\mathbf{1}]$, $[\mathbf{1},1,\mathbf{1}]$ & $[1,0,1]$\\
 \end{tabular}
\label{tab:cl-members}
\caption{\emph{Clusters from Figure~\ref{fig:example} and their Centroids.} The active features in each cluster's centroid are the \emph{shared} active features among the cluster's members.}
reflected in the centroids.}
\end{mdframed}
\end{table}	


% if $m_{1,2} = 1$,
%for instance, the second cluster is active for $\mathbf{d}_1$ (i.e.,
%$\mathbf{d}_1$ is a member of cluster 2).

%Figure~\ref{fig:example-1} shows an example of an MCMM for two data points (i.e., $I = 2$).
%The hidden cluster activities $\mathbf{M}$, the weights $\mathbf{C}$,
%and the mixing function $r$ constitute a model that reproduces the
%observed data points $\mathbf{D}$.
%The nodes $m_{i,k}$ (for $k \in K$) are the hidden units and represent cluster-membership activities.
% if $m_{1,2} = 1$,
%for instance, the second cluster is active for $\mathbf{d}_1$ (i.e.,
%$\mathbf{d}_1$ is a member of cluster 2).
%
%Note that the $J \times K$ weight matrix $\mathbf{C}$ is the same for
%all data points, and
%the $k{\text{th}}$ row in $\mathbf{C}$ can be seen as the $k{\text{th}}$
%cluster's centroid, i.e., its average or composite vector: the $j{\text{th}}$ component in
%$\mathbf{c}_k$ is 1 only if all data points in cluster $k$ have
%1 at feature $j$.

%\begin{figure}[htb!]
%\usetikzlibrary{positioning}
%%\begin{minipage}{.3\textwidth}
%\begin{center}
%%\subfigure[Learning in Progress]{
%\begin{tikzpicture}[shorten <=1pt,->,draw=black!100, scale=0.85]
%	\footnotesize
%%	\def \attic{5.95cm}
%%	\def \rowtwoht{5.4cm}
%%	\def \weightlevel{3.9cm}
%%	\def \rowoneht{2.4cm}
%%	\def \basement{1.8cm}
%%	\def \data{1cm}
%%	\def \china{0cm}
%
%	\def \attic{5.4cm}
%	\def \rowtwoht{4.8cm}
%	\def \weightlevel{3.6cm}
%	\def \rowoneht{2.4cm}
%	\def \basement{1.8cm}
%	\def \data{1cm}
%	\def \china{0cm}
%		
%	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
%	%\tikzstyle{dots}=[text width=5ex, text centered]
%	\tikzstyle{annot}=[text width=2.5em]
%	% labels
%	\tikzstyle{label}=[text width=2.5em, text centered]
%	\tikzstyle{formula}=[text width=30em, text centered]
%	
%	\scriptsize
%	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {$\mathbf{M}_{(i,k)}$};
%	\node[annot] (weights) at (0cm,\weightlevel) {$\mathbf{C}_{(j,k)}$};
%	\node[annot] (r-layer) at (0cm,\rowoneht) {$\mathbf{R}_{(i,j)}$};
%	
%	% hidden layer
%	\scriptsize
%	\node[m-node] 	(ma00)	at (1.45cm,\rowtwoht)		{$.2$};
%	\node[m-node] 	(ma01)	at (3.35cm,\rowtwoht)		{$.9$};
%	\node[m-node] 	(ma10)	at (5.55cm,\rowtwoht) 	{$.8$};
%	\node[m-node] 	(ma11)	at (7.45cm,\rowtwoht)	 	{$.1$};
%	% \node[m-node] 	(m20)	at (9.65cm,\rowtwoht) 	{$.8$};
%	% \node[m-node] 	(m21)	at (11.55cm,\rowtwoht)	 	{$.9$};
%	
%	%\footnotesize
%	\node[label]	(ml00) 	at (1.45cm,\attic)		{$m_{1,1}$}; %1.75 -> 1.45
%	\node[label]	(ml01) 	at (3.35cm,\attic)		{$m_{1,2}$}; %3.05 -> 3.35
%	\node[label] 	(ml10)	at (5.55cm,\attic) 	{$m_{2,1}$};     %5.85 -> 5.55
%	\node[label] 	(ml11)	at (7.45cm,\attic)	 	{$m_{2,2}$}; %7.15 -> 7.45
%	% \node[label] 	(ml20)	at (9.65cm,\attic) 	{$m_{3,1}$};     %9.95 -> 9.65
%	% \node[label] 	(ml21)	at (11.55cm,\attic)	 	{$m_{3,2}$}; %11.25 -> 11.55
%	
%	\scriptsize
%	\node[r-node] 	(ra00)	at (1.1cm,\rowoneht)		{$.24$};
%	\node[r-node] 	(ra01)	at (2.4cm,\rowoneht)		{$.81$};
%	\node[r-node] 	(ra02)	at (3.7cm,\rowoneht)	 	{$.23$};
%	
%	\node[r-node] 	(ra10)	at (5.2cm,\rowoneht) 		{$.68$};
%	\node[r-node] 	(ra11) 	at (6.5cm,\rowoneht)   	{$.16$};
%	\node[r-node] 	(ra12)	at (7.8cm,\rowoneht)		{$.76$};
%	
%	% \node[r-node] 	(r20) 	at (9.3cm,\rowoneht)  		{$.71$};
%	% \node[r-node] 	(r21)	at (10.6cm,\rowoneht) 		{$.83$};
%	% \node[r-node] 	(r22) 	at (11.9cm,\rowoneht)   	{$.77$};
%	
%	\node[label] 	(rl00)	at (1.1cm,\basement)		{$r_{1,1}$};
%	\node[label] 	(rl01)	at (2.4cm,\basement)		{$r_{1,2}$};
%	\node[label] 	(rl02)	at (3.7cm,\basement)	 	{$r_{1,3}$};
%	
%	\node[label] 	(rl10)	at (5.2cm,\basement) 		{$r_{2,1}$};
%	\node[label] 	(rl11) 	at (6.5cm,\basement)   	{$r_{2,2}$};
%	\node[label] 	(rl12)	at (7.8cm,\basement)		{$r_{2,3}$};
%	
%%	\node[label] 	(rl20) 	at (9.3cm,\basement)  		{$r_{3,1}$};
%%	\node[label] 	(rl21)	at (10.6cm,\basement) 		{$r_{3,2}$};
%%	\node[label] 	(rl22) 	at (11.9cm,\basement)   	{$r_{3,3}$};
%
%	\draw[-] (4.45cm, \attic+1.5mm) -- (4.45cm, \basement-1.5mm);
%%	\draw[-] (8.55cm, \attic+1.5mm) -- (8.55cm, \basement-1.5mm);
%
%	\scriptsize
%	\path
%		(ma00)	edge	node [left]	{$.85$} (ra00)
%		(ma00)	edge	node [left,xshift=-1mm,yshift=3mm]	{$.1$}	(ra01)
%		(ma00)	edge	node [left,xshift=-1mm,yshift=8mm]	{$.95$}	(ra02)
%
%		(ma01)	edge	node [right,xshift=3mm,yshift=8mm]	{$.1$}	(ra00)
%		(ma01)	edge	node [right,xshift=1mm,yshift=3mm]	{$.9$}	(ra01)
%		(ma01)	edge	node [right]	{$.05$} (ra02)
%		%
%		(ma10)	edge	node [left] {$.85$} (ra10)
%		(ma10)	edge	node [left,xshift=-1mm,yshift=3mm]	{$.1$}	(ra11)
%		(ma10)	edge	node [left,xshift=-1mm,yshift=8mm] {$.95$}	(ra12)
%		
%		(ma11)	edge	node [right,xshift=3mm,yshift=8mm]	{$.1$}	(ra10)
%		(ma11)	edge	node [right,xshift=1mm,yshift=3mm]	{$.9$}	(ra11)
%		(ma11)	edge	node [right]	{$.05$} (ra12);
%		%
		% (m20)	edge	node [left]	{$.85$}	(r20)
		% (m20)	edge	node [left,xshift=-1mm,yshift=4mm]	{$.1$}	(r21)
		% (m20)	edge	node [left,xshift=-1mm,yshift=10mm]	{$.95$} (r22)
		
		% (m21)	edge	node [right,xshift=3mm,yshift=10mm]		{$.1$}	(r20)
		% (m21)	edge	node [right,xshift=1mm,yshift=4mm]{$.9$}	(r21)
		% (m21)	edge	node [right]	{$.05$}	(r22);		
		
%\end{tikzpicture}
%\label{fig:example:fig1}
%\caption{A simple MCMM example} % showing learning in progress}
%\label{fig:example:fig1}
%\end{center}
%\end{figure}

%\begin{figure}[htb!]
%\usetikzlibrary{positioning}
%\begin{center}
%\subfigure[Observed Data]{
%\begin{tikzpicture}[shorten <=1pt,->,draw=black!100, scale=0.85]
%	\scriptsize
%	\tikzstyle{label}=[text width=3em, text centered]
%	\tikzstyle{annot}=[text width=2.5em]
%	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
%	
%	\def \china{0.6cm}
%	\def \data{0cm}
%	
%	\node[annot] (d-layer) at (0cm,\data) {$\mathbf{D}_{(i,j)}$};
%	\draw[-] (4.45cm, \china+1.5mm) -- (4.45cm, \data-4.5mm);
%%	\draw[-] (8.55cm, \china+1.5mm) -- (8.55cm, \data-4.5mm);
%	%\draw[-] (-1cm, \china+.5cm) -- (13cm, \china+.5cm);
%	%\draw[-] (-1cm, \data-.5cm) -- (13cm, \data-.5cm);
%	
%	\node[label] 	(dl00)	at (1.1cm,\china)		{$d_{1,1}$};
%	\node[label] 	(dl01)	at (2.4cm,\china)		{$d_{1,2}$};
%	\node[label] 	(dl02)	at (3.7cm,\china)	 	{$d_{1,3}$};
%	
%	\node[label] 	(dl10)	at (5.2cm,\china) 		{$d_{2,1}$};
%	\node[label] 	(dl11) 	at (6.5cm,\china)   	{$d_{2,2}$};
%	\node[label] 	(dl12)	at (7.8cm,\china)		{$d_{2,3}$};
%	
%	% \node[label] 	(dl20) 	at (9.3cm,\china)  		{$d_{3,1}$};
%	% \node[label] 	(dl21)	at (10.6cm,\china) 		{$d_{3,2}$};
%	% \node[label] 	(dl22) 	at (11.9cm,\china)   	{$d_{3,3}$};
%	
%	\node[d-node] 	(d00)	at (1.1cm,\data)		{$0$};
%	\node[d-node] 	(d01)	at (2.4cm,\data)		{$1$};
%	\node[d-node] 	(d02)	at (3.7cm,\data)		{$0$};
%
%	\node[d-node] 	(d10)	at (5.2cm,\data)		{$1$};
%	\node[d-node] 	(d11)	at (6.5cm,\data)		{$0$};
%	\node[d-node] 	(d12)	at (7.8cm,\data)		{$1$};
%	
%	% \node[d-node] 	(d20)	at (9.3cm,\data)		{$1$};
%	% \node[d-node] 	(d21)	at (10.6cm,\data)		{$1$};
%	% \node[d-node] 	(d22)	at (11.9cm,\data)		{$1$};
%\end{tikzpicture}
%\label{fig:example:subfig0}
%}
%\end{center}
%
%\usetikzlibrary{positioning}
%%\begin{minipage}{.3\textwidth}
%\begin{center}
%\subfigure[Learning in Progress]{
%\begin{tikzpicture}[shorten <=1pt,->,draw=black!100, scale=0.85]
%	\footnotesize
%%	\def \attic{5.95cm}
%%	\def \rowtwoht{5.4cm}
%%	\def \weightlevel{3.9cm}
%%	\def \rowoneht{2.4cm}
%%	\def \basement{1.8cm}
%%	\def \data{1cm}
%%	\def \china{0cm}
%
%	\def \attic{5cm}
%	\def \rowtwoht{4.4cm}
%	\def \weightlevel{3.4cm}
%	\def \rowoneht{2.4cm}
%	\def \basement{1.8cm}
%	\def \data{1cm}
%	\def \china{0cm}
%		
%	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
%	%\tikzstyle{dots}=[text width=5ex, text centered]
%	\tikzstyle{annot}=[text width=2.5em]
%	% labels
%	\tikzstyle{label}=[text width=2.5em, text centered]
%	\tikzstyle{formula}=[text width=30em, text centered]
%	
%	\scriptsize
%	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {$\mathbf{M}_{(i,k)}$};
%	\node[annot] (weights) at (0cm,\weightlevel) {$\mathbf{C}_{(j,k)}$};
%	\node[annot] (r-layer) at (0cm,\rowoneht) {$\mathbf{R}_{(i,j)}$};
%	
%	% hidden layer
%	\scriptsize
%	\node[m-node] 	(ma00)	at (1.3cm,\rowtwoht)		{$.2$};
%	\node[m-node] 	(ma01)	at (3.5cm,\rowtwoht)		{$.9$};
%	\node[m-node] 	(ma10)	at (5.4cm,\rowtwoht) 	{$.8$};
%	\node[m-node] 	(ma11)	at (7.6cm,\rowtwoht)	 	{$.1$};
%	% \node[m-node] 	(m20)	at (9.65cm,\rowtwoht) 	{$.8$};
%	% \node[m-node] 	(m21)	at (11.55cm,\rowtwoht)	 	{$.9$};
%	
%	%\footnotesize
%	\node[label]	(ml00) 	at (1.3cm,\attic)		{$m_{1,1}$}; %1.75 -> 1.45
%	\node[label]	(ml01) 	at (3.5cm,\attic)		{$m_{1,2}$}; %3.05 -> 3.35
%	\node[label] 	(ml10)	at (5.4cm,\attic) 	{$m_{2,1}$};     %5.85 -> 5.55
%	\node[label] 	(ml11)	at (7.6cm,\attic)	 	{$m_{2,2}$}; %7.15 -> 7.45
%	% \node[label] 	(ml20)	at (9.65cm,\attic) 	{$m_{3,1}$};     %9.95 -> 9.65
%	% \node[label] 	(ml21)	at (11.55cm,\attic)	 	{$m_{3,2}$}; %11.25 -> 11.55
%	
%	\scriptsize
%	\node[r-node] 	(ra00)	at (1.1cm,\rowoneht)		{$.24$};
%	\node[r-node] 	(ra01)	at (2.4cm,\rowoneht)		{$.81$};
%	\node[r-node] 	(ra02)	at (3.7cm,\rowoneht)	 	{$.23$};
%	
%	\node[r-node] 	(ra10)	at (5.2cm,\rowoneht) 		{$.68$};
%	\node[r-node] 	(ra11) 	at (6.5cm,\rowoneht)   	{$.16$};
%	\node[r-node] 	(ra12)	at (7.8cm,\rowoneht)		{$.76$};
%	
%	\node[label] 	(rl00)	at (1.1cm,\basement)		{$r_{1,1}$};
%	\node[label] 	(rl01)	at (2.4cm,\basement)		{$r_{1,2}$};
%	\node[label] 	(rl02)	at (3.7cm,\basement)	 	{$r_{1,3}$};
%	
%	\node[label] 	(rl10)	at (5.2cm,\basement) 		{$r_{2,1}$};
%	\node[label] 	(rl11) 	at (6.5cm,\basement)   	{$r_{2,2}$};
%	\node[label] 	(rl12)	at (7.8cm,\basement)		{$r_{2,3}$};
%
%	\draw[-] (4.45cm, \attic+1.5mm) -- (4.45cm, \basement-1.5mm);
%
%	\scriptsize
%	\path
%		(ma00)	edge	node [left]	{$.85$} (ra00)
%		(ma00)	edge	node [left,xshift=-1mm,yshift=2mm]	{$.1$}	(ra01)
%		(ma00)	edge	node [left,xshift=-1mm,yshift=6mm]	{$.95$}	(ra02)
%
%		(ma01)	edge	node [right,xshift=2.5mm,yshift=6mm]	{$.1$}	(ra00)
%		(ma01)	edge	node [right,xshift=1mm,yshift=2mm]	{$.9$}	(ra01)
%		(ma01)	edge	node [right]	{$.05$} (ra02)
%		%
%		(ma10)	edge	node [left] {$.85$} (ra10)
%		(ma10)	edge	node [left,xshift=-1mm,yshift=2mm]	{$.1$}	(ra11)
%		(ma10)	edge	node [left,xshift=-1mm,yshift=6mm] {$.95$}	(ra12)
%		
%		(ma11)	edge	node [right,xshift=2.5mm,yshift=6mm]	{$.1$}	(ra10)
%		(ma11)	edge	node [right,xshift=1mm,yshift=2mm]	{$.9$}	(ra11)
%		(ma11)	edge	node [right]	{$.05$} (ra12);
%		%
%		% (m20)	edge	node [left]	{$.85$}	(r20)
%		% (m20)	edge	node [left,xshift=-1mm,yshift=4mm]	{$.1$}	(r21)
%		% (m20)	edge	node [left,xshift=-1mm,yshift=10mm]	{$.95$} (r22)
%		
%		% (m21)	edge	node [right,xshift=3mm,yshift=10mm]		{$.1$}	(r20)
%		% (m21)	edge	node [right,xshift=1mm,yshift=4mm]{$.9$}	(r21)
%		% (m21)	edge	node [right]	{$.05$}	(r22);		
%		
%\end{tikzpicture}
%\label{fig:example:subfig1}
%}
%\subfigure[Convergence]{
%
%\begin{tikzpicture}[shorten <=1pt,->,draw=black!100, scale=0.85]
%	\small
%%	\def \attic{5.95cm}
%%	\def \rowtwoht{5.4cm}
%%	\def \weightlevel{3.9cm}
%%	\def \rowoneht{2.4cm}
%%	\def \basement{1.8cm}
%%	\def \data{1cm}
%%	\def \china{0cm}
%
%%	\def \attic{5.2cm}
%%	\def \rowtwoht{4.6cm}
%%	\def \weightlevel{3.5cm}
%%	\def \rowoneht{2.4cm}
%%	\def \basement{1.8cm}
%%	\def \data{1cm}
%%	\def \china{0cm}
%
%	\def \attic{5cm}
%	\def \rowtwoht{4.4cm}
%	\def \weightlevel{3.4cm}
%	\def \rowoneht{2.4cm}
%	\def \basement{1.8cm}
%	\def \data{1cm}
%	\def \china{0cm}
%		
%%	\def \attic{5.4cm}
%%	\def \rowtwoht{5cm}
%%	\def \weightlevel{3.9cm}
%%	\def \rowoneht{2.4cm}
%%	\def \basement{1.8cm}
%%	\def \data{1cm}
%%	\def \china{0cm}
%	
%	\small
%	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
%	\tikzstyle{annot}=[text width=2.5em]
%	% labels
%	\tikzstyle{label}=[text width=3em, text centered]
%	\tikzstyle{formula}=[text width=30em, text centered]
%	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {$\mathbf{M}_{(i,k)}$};
%	\node[annot] (weights) at (0cm,\weightlevel) {$\mathbf{C}_{(j,k)}$};
%	\node[annot] (r-layer) at (0cm,\rowoneht) {$\mathbf{R}_{(i,j)}$};
%	
%	\node[m-node] 	(m00)	at (1.3cm,\rowtwoht)		{$0$};
%	\node[m-node] 	(m01)	at (3.5cm,\rowtwoht)		{$1$};
%	\node[m-node] 	(m10)	at (5.4cm,\rowtwoht) 	{$1$};
%	\node[m-node] 	(m11)	at (7.6cm,\rowtwoht)	 	{$0$};
%	% \node[m-node] 	(m20)	at (9.65cm,\rowtwoht) 	{$1$};
%	% \node[m-node] 	(m21)	at (11.55cm,\rowtwoht)	 	{$1$};
%	
%	\node[label]	(ml00) 	at (1.3cm,\attic)		{$m_{1,1}$};
%	\node[label]	(ml01) 	at (3.5cm,\attic)		{$m_{1,2}$};
%	\node[label] 	(ml10)	at (5.4cm,\attic) 	{$m_{2,1}$};
%	\node[label] 	(ml11)	at (7.6cm,\attic)	 	{$m_{2,2}$};
%	% \node[label] 	(ml20)	at (9.65cm,\attic) 	{$m_{3,1}$};
%	% \node[label] 	(ml21)	at (11.55cm,\attic)	 	{$m_{3,2}$};
%	
%	%\node[m-node] 	(m20)	[right of=m11,xshift=2cm]	 	{$1.0$};
%	%\node[m-node] 	(m21)	[right of=m20,xshift=0.5cm]	 	{$1.0$};	
%	% reconstructed vector
%	\node[r-node] 	(r00)	at (1.1cm,\rowoneht)		{$0$};
%	\node[label] 	(rl00)	at (1.1cm,\basement)		{$r_{1,1}$};
%	\node[r-node] 	(r01)	at (2.4cm,\rowoneht)		{$1$};
%	\node[label] 	(rl01)	at (2.4cm,\basement)		{$r_{1,2}$};
%	\node[r-node] 	(r02)	at (3.7cm,\rowoneht)	 	{$0$};
%	\node[label] 	(rl02)	at (3.7cm,\basement)	 	{$r_{1,3}$};
%	
%	\node[r-node] 	(r10)	at (5.2cm,\rowoneht) 		{$1$};
%	\node[label] 	(rl10)	at (5.2cm,\basement) 		{$r_{2,1}$};
%	\node[r-node] 	(r11) 	at (6.5cm,\rowoneht)   		{$0$};
%	\node[label] 	(rl11) 	at (6.5cm,\basement)   		{$r_{2,2}$};
%	\node[r-node] 	(r12)	at (7.8cm,\rowoneht)		{$1$};
%	\node[label] 	(rl12)	at (7.8cm,\basement)		{$r_{2,3}$};
%	
%	% \node[r-node] 	(r20) 	at (9.3cm,\rowoneht)  		{$1$};
%	% \node[label] 	(rl20) 	at (9.3cm,\basement)  		{$r_{3,1}$};
%	% \node[r-node] 	(r21)	at (10.6cm,\rowoneht) 		{$1$};
%	% \node[label] 	(rl21)	at (10.6cm,\basement) 		{$r_{3,2}$};
%	% \node[r-node] 	(r22) 	at (11.9cm,\rowoneht)   		{$1$};
%	% \node[label] 	(rl22) 	at (11.9cm,\basement)   		{$r_{3,3}$};
%	
%	\draw[-] (4.45cm, \attic+1.5mm) -- (4.45cm, \basement-1.5mm);
%%	\draw[-] (8.55cm, \attic+1.5mm) -- (8.55cm, \basement-1.5mm);
%
%	\path
%		(m00)	edge	node [left] 	{$1$}	(r00)
%		(m00)	edge	node [left,xshift=-1mm,yshift=2mm]	{$0$}	(r01)
%		(m00)	edge	node [left,xshift=-3mm,yshift=6mm]	{$1$}	(r02)
%
%		(m01)	edge	node [right,xshift=3mm,yshift=6mm]	{$0$}	(r00)
%		(m01)	edge	node [right,xshift=1mm,yshift=2mm]	{$1$}	(r01)
%		(m01)	edge	node [right]	{$0$}	(r02)
%		%
%		(m10)	edge	node [left] 	{$1$}	(r10)
%		(m10)	edge	node [left,xshift=-1mm,yshift=2mm]	{$0$}	(r11)
%		(m10)	edge	node [left,xshift=-3mm,yshift=6mm] {$1$}	(r12)
%		
%		(m11)	edge	node [right,xshift=3mm,yshift=6mm]	{$0$}	(r10)
%		(m11)	edge	node [right,xshift=1mm,yshift=2mm]	{$1$}	(r11)
%		(m11)	edge	node [right]	{$0$}	(r12);
%		%
%		% (m20)	edge	node [left]	{$1$}	(r20)
%		% (m20)	edge	node [left,xshift=-1mm,yshift=4mm]	{$0$}	(r21)
%		% (m20)	edge	node [left,xshift=-3mm,yshift=10mm]	{$1$} (r22)
%		
%		% (m21)	edge	node [right,xshift=3mm,yshift=10mm]		{$0$}	(r20)
%		% (m21)	edge	node [right,xshift=1mm,yshift=4mm]{$1$}	(r21)
%		% (m21)	edge	node [right]	{$0$}	(r22);		
%\end{tikzpicture}
%\label{fig:example:subfig2}
%}
%
%\begin{framed}
%	\centering
%	\small
%	where
%	$\begin{aligned}
%	   r_{i,j} = 1 - \Pi_{k=1}^{K} (1 - m_{i,k}c_{j,k}) 
%	\end{aligned}$
%        \hspace{2em}
%        [\textsc{noisy-or} function]
%\end{framed}
%
%\end{center}
%\caption{A simple MCMM example} % showing learning in progress}
%\label{fig:example}
%\end{figure}

We can see that while learning is in progress, the cluster activities
$m_{i,k}$ and the cluster centers $c_{j,k}$ are in flux, as the
error rate is being reduced, but that they converge to values of 0 and
1.  At convergence, a reconstruction node $r_{i,j}$ is 1 if at least one
$m_{i,k}c_{j,k} = 1$ (and $0$ otherwise).

%the activities and the centers are 1 and are 0 otherwise.

 \begin{figure}[htb!]
 %\usetikzlibrary{positioning}
 \begin{center}
 \subfigure[Observed Data]{
 \begin{tikzpicture}[shorten <=1pt,->,draw=black!100]
 	\scriptsize
 	\tikzstyle{label}=[text width=3em, text centered]
 	\tikzstyle{annot}=[text width=2.5em]
 	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
	
 	\def \china{0.6cm}
 	\def \data{0cm}
	
 	\node[annot] (d-layer) at (0cm,\data) {$\mathbf{X}_{(i,j)}$};
 	\draw[-] (4.45cm, \china+1.5mm) -- (4.45cm, \data-4.5mm);
 	\draw[-] (8.55cm, \china+1.5mm) -- (8.55cm, \data-4.5mm);
 	%\draw[-] (-1cm, \china+.5cm) -- (13cm, \china+.5cm);
 	%\draw[-] (-1cm, \data-.5cm) -- (13cm, \data-.5cm);
	
 	\node[label] 	(dl00)	at (1.1cm,\china)		{$x_{0,0}$};
 	\node[label] 	(dl01)	at (2.4cm,\china)		{$x_{0,1}$};
 	\node[label] 	(dl02)	at (3.7cm,\china)	 	{$x_{0,2}$};
	
 	\node[label] 	(dl10)	at (5.2cm,\china) 		{$x_{1,0}$};
 	\node[label] 	(dl11) 	at (6.5cm,\china)   	{$x_{1,1}$};
 	\node[label] 	(dl12)	at (7.8cm,\china)		{$x_{1,2}$};
	
 	\node[label] 	(dl20) 	at (9.3cm,\china)  		{$x_{2,0}$};
 	\node[label] 	(dl21)	at (10.6cm,\china) 		{$x_{2,1}$};
 	\node[label] 	(dl22) 	at (11.9cm,\china)   	{$x_{2,2}$};
	
 	\node[d-node] 	(d00)	at (1.1cm,\data)		{$0$};
 	\node[d-node] 	(d01)	at (2.4cm,\data)		{$1$};
 	\node[d-node] 	(d02)	at (3.7cm,\data)		{$0$};

 	\node[d-node] 	(d10)	at (5.2cm,\data)		{$1$};
 	\node[d-node] 	(d11)	at (6.5cm,\data)		{$0$};
 	\node[d-node] 	(d12)	at (7.8cm,\data)		{$1$};
	
 	\node[d-node] 	(d20)	at (9.3cm,\data)		{$1$};
 	\node[d-node] 	(d21)	at (10.6cm,\data)		{$1$};
 	\node[d-node] 	(d22)	at (11.9cm,\data)		{$1$};
 \end{tikzpicture}
 \label{fig:example:subfig0-1}
 }
 \end{center}

 \usetikzlibrary{positioning}
 %\begin{minipage}{.3\textwidth}
 \begin{center}
 \subfigure[Learning in Progress]{
 \begin{tikzpicture}[shorten <=1pt,->,draw=black!100]
 	\footnotesize
 	\def \attic{5.95cm}
 	\def \rowtwoht{5.4cm}
 	\def \weightlevel{3.9cm}
 	\def \rowoneht{2.4cm}
 	\def \basement{1.8cm}
% 	\def \data{1cm}
% 	\def \china{0cm}
	
 	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
 	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
 	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
 	%\tikzstyle{dots}=[text width=5ex, text centered]
 	\tikzstyle{annot}=[text width=3em]
 	% labels
 	\tikzstyle{label}=[text width=3em, text centered]
 	\tikzstyle{formula}=[text width=30em, text centered]
	
 	\scriptsize
 	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {$\mathbf{M}_{(i,k)}$};
 	\node[annot] (weights) at (0cm,\weightlevel) {$\mathbf{C}_{(j,k)}$};
 	\node[annot] (r-layer) at (0cm,\rowoneht) {$\mathbf{R}_{(i,j)}$};
	
 	% hidden layer
 	\scriptsize
 	\node[m-node] 	(m00)	at (1.45cm,\rowtwoht)		{$.2$};
 	\node[m-node] 	(m01)	at (3.35cm,\rowtwoht)		{$.9$};
 	\node[m-node] 	(m10)	at (5.55cm,\rowtwoht) 	{$.8$};
 	\node[m-node] 	(m11)	at (7.45cm,\rowtwoht)	 	{$.1$};
 	\node[m-node] 	(m20)	at (9.65cm,\rowtwoht) 	{$.8$};
 	\node[m-node] 	(m21)	at (11.55cm,\rowtwoht)	 	{$.9$};
	
 	%\footnotesize
 	\node[label]	(ml00) 	at (1.45cm,\attic)		{$m_{0,0}$}; %1.75 -> 1.45
 	\node[label]	(ml01) 	at (3.35cm,\attic)		{$m_{0,1}$}; %3.05 -> 3.35
 	\node[label] 	(ml10)	at (5.55cm,\attic) 	{$m_{1,0}$};     %5.85 -> 5.55
 	\node[label] 	(ml11)	at (7.45cm,\attic)	 	{$m_{1,1}$}; %7.15 -> 7.45
 	\node[label] 	(ml20)	at (9.65cm,\attic) 	{$m_{2,0}$};     %9.95 -> 9.65
 	\node[label] 	(ml21)	at (11.55cm,\attic)	 	{$m_{2,1}$}; %11.25 -> 11.55
	
 	\scriptsize
 	\node[r-node] 	(r00)	at (1.1cm,\rowoneht)		{$.24$};
 	\node[r-node] 	(r01)	at (2.4cm,\rowoneht)		{$.81$};
 	\node[r-node] 	(r02)	at (3.7cm,\rowoneht)	 	{$.23$};
	
 	\node[r-node] 	(r10)	at (5.2cm,\rowoneht) 		{$.68$};
 	\node[r-node] 	(r11) 	at (6.5cm,\rowoneht)   	{$.16$};
 	\node[r-node] 	(r12)	at (7.8cm,\rowoneht)		{$.76$};
	
 	\node[r-node] 	(r20) 	at (9.3cm,\rowoneht)  		{$.71$};
 	\node[r-node] 	(r21)	at (10.6cm,\rowoneht) 		{$.83$};
 	\node[r-node] 	(r22) 	at (11.9cm,\rowoneht)   	{$.77$};
	
 	\node[label] 	(rl00)	at (1.1cm,\basement)		{$r_{0,0}$};
 	\node[label] 	(rl01)	at (2.4cm,\basement)		{$r_{0,1}$};
 	\node[label] 	(rl02)	at (3.7cm,\basement)	 	{$r_{0,2}$};
	
 	\node[label] 	(rl10)	at (5.2cm,\basement) 		{$r_{1,0}$};
 	\node[label] 	(rl11) 	at (6.5cm,\basement)   	{$r_{1,1}$};
 	\node[label] 	(rl12)	at (7.8cm,\basement)		{$r_{1,2}$};
	
 	\node[label] 	(rl20) 	at (9.3cm,\basement)  		{$r_{2,0}$};
 	\node[label] 	(rl21)	at (10.6cm,\basement) 		{$r_{2,1}$};
 	\node[label] 	(rl22) 	at (11.9cm,\basement)   	{$r_{2,2}$};

 	\draw[-] (4.45cm, \attic+1.5mm) -- (4.45cm, \basement-1.5mm);
 	\draw[-] (8.55cm, \attic+1.5mm) -- (8.55cm, \basement-1.5mm);

 	\scriptsize
 	\path
 		(m00)	edge	node [left] 	{$.85$}	(r00)
 		(m00)	edge	node [left,xshift=-1mm,yshift=4mm]	{$.1$}	(r01)
 		(m00)	edge	node [left,xshift=-1mm,yshift=10mm]	{$.95$}	(r02)

 		(m01)	edge	node [right,xshift=3mm,yshift=10mm]	{$.1$}	(r00)
 		(m01)	edge	node [right,xshift=1mm,yshift=4mm]	{$.9$}	(r01)
 		(m01)	edge	node [right]	{$.05$}	(r02)
 		%
 		(m10)	edge	node [left] 	{$.85$}	(r10)
 		(m10)	edge	node [left,xshift=-1mm,yshift=4mm]	{$.1$}	(r11)
 		(m10)	edge	node [left,xshift=-1mm,yshift=10mm] {$.95$}	(r12)
		
 		(m11)	edge	node [right,xshift=3mm,yshift=10mm]	{$.1$}	(r10)
 		(m11)	edge	node [right,xshift=1mm,yshift=4mm]	{$.9$}	(r11)
 		(m11)	edge	node [right]	{$.05$}	(r12)
 		%
 		(m20)	edge	node [left]	{$.85$}	(r20)
 		(m20)	edge	node [left,xshift=-1mm,yshift=4mm]	{$.1$}	(r21)
 		(m20)	edge	node [left,xshift=-1mm,yshift=10mm]	{$.95$} (r22)
		
 		(m21)	edge	node [right,xshift=3mm,yshift=10mm]		{$.1$}	(r20)
 		(m21)	edge	node [right,xshift=1mm,yshift=4mm]{$.9$}	(r21)
 		(m21)	edge	node [right]	{$.05$}	(r22);		
		
 \end{tikzpicture}
 \label{fig:example:subfig1-1}
 }
 \subfigure[Convergence]{

 \begin{tikzpicture}[shorten <=1pt,->,draw=black!100]
 	\footnotesize
 	\def \attic{5.95cm}
 	\def \rowtwoht{5.4cm}
 	\def \weightlevel{3.9cm}
 	\def \rowoneht{2.4cm}
 	\def \basement{1.8cm}
% 	\def \data{1cm}
% 	\def \china{0cm}
	
 	\scriptsize
 	\tikzstyle{m-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
 	\tikzstyle{r-node}=[circle,draw=black!100,thick,inner sep=2pt,minimum size=6mm]
 	\tikzstyle{d-node}=[circle,draw=black!100,fill=gray!45,thick,inner sep=2pt,minimum size=6mm]
 	\tikzstyle{annot}=[text width=2.5em]
 	% labels
 	\tikzstyle{label}=[text width=3em, text centered]
 	\tikzstyle{formula}=[text width=30em, text centered]
 	\node[annot] (hidden-layer) at (0cm,\rowtwoht) {$\mathbf{M}_{(i,k)}$};
 	\node[annot] (weights) at (0cm,\weightlevel) {$\mathbf{C}_{(j,k)}$};
 	\node[annot] (r-layer) at (0cm,\rowoneht) {$\mathbf{R}_{(i,j)}$};
	
 	\node[m-node] 	(m00)	at (1.45cm,\rowtwoht)		{$0$};
 	\node[m-node] 	(m01)	at (3.35cm,\rowtwoht)		{$1$};
 	\node[m-node] 	(m10)	at (5.55cm,\rowtwoht) 	{$1$};
 	\node[m-node] 	(m11)	at (7.45cm,\rowtwoht)	 	{$0$};
 	\node[m-node] 	(m20)	at (9.65cm,\rowtwoht) 	{$1$};
 	\node[m-node] 	(m21)	at (11.55cm,\rowtwoht)	 	{$1$};
	
 	\node[label]	(ml00) 	at (1.45cm,\attic)		{$m_{0,0}$};
 	\node[label]	(ml01) 	at (3.35cm,\attic)		{$m_{0,1}$};
 	\node[label] 	(ml10)	at (5.55cm,\attic) 	{$m_{1,0}$};
 	\node[label] 	(ml11)	at (7.45cm,\attic)	 	{$m_{1,1}$};
 	\node[label] 	(ml20)	at (9.65cm,\attic) 	{$m_{2,0}$};
 	\node[label] 	(ml21)	at (11.55cm,\attic)	 	{$m_{2,1}$};
	
 	%\node[m-node] 	(m20)	[right of=m11,xshift=2cm]	 	{$1.0$};
 	%\node[m-node] 	(m21)	[right of=m20,xshift=0.5cm]	 	{$1.0$};	
 	% reconstructed vector
 	\node[r-node] 	(r00)	at (1.1cm,\rowoneht)		{$0$};
 	\node[label] 	(rl00)	at (1.1cm,\basement)		{$r_{0,0}$};
 	\node[r-node] 	(r01)	at (2.4cm,\rowoneht)		{$1$};
 	\node[label] 	(rl01)	at (2.4cm,\basement)		{$r_{0,1}$};
 	\node[r-node] 	(r02)	at (3.7cm,\rowoneht)	 	{$0$};
 	\node[label] 	(rl02)	at (3.7cm,\basement)	 	{$r_{0,2}$};
	
 	\node[r-node] 	(r10)	at (5.2cm,\rowoneht) 		{$1$};
 	\node[label] 	(rl10)	at (5.2cm,\basement) 		{$r_{1,0}$};
 	\node[r-node] 	(r11) 	at (6.5cm,\rowoneht)   		{$0$};
 	\node[label] 	(rl11) 	at (6.5cm,\basement)   		{$r_{1,1}$};
 	\node[r-node] 	(r12)	at (7.8cm,\rowoneht)		{$1$};
 	\node[label] 	(rl12)	at (7.8cm,\basement)		{$r_{1,2}$};
	
 	\node[r-node] 	(r20) 	at (9.3cm,\rowoneht)  		{$1$};
 	\node[label] 	(rl20) 	at (9.3cm,\basement)  		{$r_{2,0}$};
 	\node[r-node] 	(r21)	at (10.6cm,\rowoneht) 		{$1$};
 	\node[label] 	(rl21)	at (10.6cm,\basement) 		{$r_{2,1}$};
 	\node[r-node] 	(r22) 	at (11.9cm,\rowoneht)   		{$1$};
 	\node[label] 	(rl22) 	at (11.9cm,\basement)   		{$r_{2,2}$};
	
 	\draw[-] (4.45cm, \attic+1.5mm) -- (4.45cm, \basement-1.5mm);
 	\draw[-] (8.55cm, \attic+1.5mm) -- (8.55cm, \basement-1.5mm);

 	\path
 		(m00)	edge	node [left] 	{$1$}	(r00)
 		(m00)	edge	node [left,xshift=-1mm,yshift=4mm]	{$0$}	(r01)
 		(m00)	edge	node [left,xshift=-3mm,yshift=10mm]	{$1$}	(r02)

 		(m01)	edge	node [right,xshift=3mm,yshift=10mm]	{$0$}	(r00)
 		(m01)	edge	node [right,xshift=1mm,yshift=4mm]	{$1$}	(r01)
 		(m01)	edge	node [right]	{$0$}	(r02)
 		%
 		(m10)	edge	node [left] 	{$1$}	(r10)
 		(m10)	edge	node [left,xshift=-1mm,yshift=4mm]	{$0$}	(r11)
 		(m10)	edge	node [left,xshift=-3mm,yshift=10mm] {$1$}	(r12)
		
 		(m11)	edge	node [right,xshift=3mm,yshift=10mm]	{$0$}	(r10)
 		(m11)	edge	node [right,xshift=1mm,yshift=4mm]	{$1$}	(r11)
 		(m11)	edge	node [right]	{$0$}	(r12)
 		%
 		(m20)	edge	node [left]	{$1$}	(r20)
 		(m20)	edge	node [left,xshift=-1mm,yshift=4mm]	{$0$}	(r21)
 		(m20)	edge	node [left,xshift=-3mm,yshift=10mm]	{$1$} (r22)
		
 		(m21)	edge	node [right,xshift=3mm,yshift=10mm]		{$0$}	(r20)
 		(m21)	edge	node [right,xshift=1mm,yshift=4mm]{$1$}	(r21)
 		(m21)	edge	node [right]	{$0$}	(r22);		
 \end{tikzpicture}
 \label{fig:example:subfig2-1}
 }

 \begin{framed}
 	\centering
 	\small
 	where
 	$\begin{aligned}
 	   r_{i,j} = 1 - \Pi_{k=1}^{K} (1 - m_{i,k}c_{j,k}) 
 	\end{aligned}$
 \end{framed}

 \label{fig:example-1}
 \caption{MCMM example showing learning in progress}
 \end{center}
 \end{figure}

