\chapter{Results}
\label{ch:results}

\section{Introduction}

%In the preceding chapter, %~\ref{ch:eval}, 
%we motivated and outlined a multi-faceted evaluation program, an approach comprising both
%a qualitative component and a \emph{dual-paradigm} quantitative component, that is, a quantitative component with 
%both \emph{intrinsic} and \emph{extrinsic} sub-components. 

We saw in chapter~\ref{ch:MCMM} that Multimorph's output, in its raw form,
consists of two matrices, $\mathbf{M}$ and $\mathbf{C}$.\footnote{Note that these 
matrices are actually by-products of the MCMM's attempt to reconstruct its original 
data points (see chapter~\ref{ch:MCMM}).} While these are numerical matrices,
they together encode a \emph{clustering} of words, and they can be converted (or decoded) 
into the the form of word clustering through algorithm~\ref{alg:members}, which we presented in section~\ref{sec:intrinsic} 
of the preceding chapter. 
Each experiment described in
chapter~\ref{ch:experi}---that is, each combination of $\delta$, $s$, and 
data-representation type---yielded a distinct pair of $\mathbf{M}$ and $\mathbf{C}$ 
matrices. Each such pair was in turn converted into a clustering of words through 
algorithm~\ref{alg:members}, and each clustering was then evaluated according to the
multi-faceted evaluation program described
%methods described 
in chapter~\ref{ch:eval}. 
%In this chapter, I present the results of these multi-faceted evaluations. 
This chapter now presents the results of the different methods composing this multi-faceted evaluation.

%As a preliminary matter, it is worthwhile to review briefly what we are evaluating.
%The 
%As discussed in chapter~\ref{ch:MCMM}, Multimorph's MCMM
%in effect groups its input words into morphological clusters. 
%During the course of learning, a particular vector of hidden-unit values, 
%namely, the vector $\mathbf{m}_{i}$, is induced for each word $w_i$ (or, rather, its feature-vector representation  $\mathbf{x}_i$). 
%The vector $\mathbf{m}_{i}$ consists of $K$ hidden-unit \emph{activities}, each of which is 0, 1, or a number between 0 and 1 (but never less than 0 or greater than 1). Each of these activities
%%than real number in the interval $[0,1]$.
%corresponds to a particular cluster and, in particular, indicates the extent to which word $w_i$
%is a member of this cluster. We thus sometimes call the $\mathbf{m}_{i}$ the \textit{cluster-membership}
%vector of word $w_i$. % with a particular index $k$, an integer in the range $[0, K)$,
%%where $K$ is the total number of clusters. % greater than or equal to 0 and less than $K$, the total number of clusters.
%%Each hidden unit, therefore, indicates the extent to which word $i$ is a member its corresponding cluster.
%The threshold for cluster membership is $\theta_{\text{mc}}$. That is, if hidden unit 
%$\mathbf{m}_{i,k}$ has a value equal to or greater than 
%$\theta_{\text{mc}}$, then word $w_i$ is counted as a member of the cluster indexed $k$, but not otherwise.
%% Otherwise, it is not a member of cluster $k$. 

%For example, suppose that word $w_i$ had the hidden-unit vector 
%\begin{center}
%$\mathbf{m}_{i} = [0.2, 0.0,0.9,0.1,0.8]$
%\end{center}
%wherein the activities of the third and fifth clusters, 0.9 and 0.8, respectively, 
%exceed the cluster-membership 
%threshold, while the other three values
%are well below it. Thus, of the five clusters in this hypothetical model, 
%word $w_i$ is a member of the third and fifth clusters.
%
%In this way, the $I \times K$ matrix $\mathbf{M}$, that is, the collection 
%of all $I$ hidden-unit vectors $\mathbf{m}_i$,
%defines a \emph{disjunctive clustering}, i.e., a clustering in which a single item can 
%%\emph{fully} (not just \emph{partially} or \emph{possibly}) 
%belong to multiple clusters at once \citep{manning-and-schutze:1999}. 
%Disjunctive clusterings are thus to be distinguished from soft clusterings,
%%that a standard mixture model would produce as well as the 
%and mixed-membership clusterings 
%(see the discussion of mixture and mixed-membership models 
%in chapter~\ref{ch:MCMM}). 
%Each of the $I$ input words can belong to up to $K$ clusters. 
%Each of the $K$ columns contains a particular cluster's 
%membership indicators, wherein each cell indicates the membership status of a data point (or word). 
%%data point as one moves down the rows of  $\textbf{M}$. 
%Multimorph produced such a clustering for 
%each of the experimental parameter combinations described in 
%chapter~\ref{ch:experi}. 

As a final preliminary matter for this chapter, recall from 
section~\ref{sec:mcmm-learning}
that Multimorph's MCMM begins its learning process with a single cluster, so that initially $K = 1$.
Then, whenever the  
global reconstruction error can no longer be decreased 
significantly, it adds a cluster (by splitting the worst cluster) and thus initiates a new cycle of error minimization.
%i.e., when a plateau is reached.  with the current
%array of clusters. 
In principle, this incremental addition 
of clusters
should continue until the global reconstruction falls below a
threshold $\epsilon$, a predisignated number close to $0$.
% (e.g., 0.0001).  
%In this study, $\epsilon$ was
% set at 
% ($0.0001$ in this study).
%falls below a threshold $\epsilon$ close to 0, such as $\epsilon = 0.0001$).
In theory, therefore, Multimorph should have created just enough clusters to decrease its error to just below $\epsilon$.\footnote{In this study, $\epsilon$ was set to $0.0001$, a valuation that turned out to be moot, however, as Multimorph's error was never that small.}
%to $\epsilon$ or below it.
In practice, however,
% in the course of this dissertation's experiments,
 Multimorph's error never reached this threshold.
 Rather, in every experimental trial reported in this thesis, Multimorph's 
 error decreased steadily toward $\epsilon$ for a time, but then stopped at a point greater that $\epsilon$, 
 where it reversed direction and started to increase.  
It continued to increase until the experiment in question was stopped at 3000 or 4000 clusters.  
The error minimum generally occurred when the cluster count $K$ was between 400 and 1200. 
For the transcriptional data with stress marking, the average error-minimizing $K$ was 589.4.
It was
833.0 for the transcriptional data without stress marking,
and 742.9 for the orthographic data. Because the error-minimizing $K$ was between 500 and 1000
for all three datasets, I am reporting results both at $K = 500$ and $K = 1000$.

\section{Qualitative Analysis}
\label{sec:qual}

%\begin{figure}[t]
%\begin{mdframed}
%\begin{tabbing}
%\hspace*{13ex}\= \hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex} \kill
%mecalc\a'{e}let \> mehamad\a'{a}f \> mehaw\a'{i}de\textipa{P}o \> bemaq\a'{e}l \> mehamirp\a'{e}set \> meqar\textsubdot{t}\a'{o}n \\
%wemecaxc\a'{e}ax \> meqa\v{s}q\a'{e}\v{s} \> \v{s}eme\textsubdot{k}as\a'{e} \> mexaq\a'{a} \> milem\a'{a}\textsubdot{t}a \> me\textsubdot{t}ap\a'{e}l \\
%le\textipa{Q}acme\textsubdot{k}\a'{a} \> me\textsubdot{k}ab\a'{e}y \> hamefaq\a'{e}d \> mehaqlip\a'{o}t \> wema\v{s}\a'{a}\textsubdot{k} \> weme\textipa{P}ax\a'{o}ra \\
%\v{s}emexak\a'{i}m \> meha\v{s}uq \> mefah\a'{e}qet \> meha\v{s}ulx\a'{a}n \> mehami\textsubdot{t}b\a'{a}x \> \v{s}emaxz\a'{i}q \\
%megar\a'{e}det \> mehadq\a'{i}m \> mehab\a'{e}rez \> meqa\v{s}\a'{e}ret \> meharof\a'{e}\textipa{P} \> mehab\a'{e}\textsubdot{t}en \\
%\end{tabbing}
%\label{fig:cluster-0-0-3}
%\caption{Thirty words randomly selected from the 680 members of cluster 0 from the settings $s=0,\delta =3$, data representation: TS}
%\end{mdframed}
%\end{figure}

The value of quantitative methods lies in their systematicity and objectivity, but
they are by no means guaranteed to capture every salient fact regarding a system's output,
especially when the system in question is an unsupervised. 
 This dissertation thus incorporated a qualitative analysis of Multimorph's output to supplement the  
quantitative results presented later in this chapter.
This qualitative analysis consisted in manually inspecting the component words of the clusters 
output by Multimorph.\footnote{Recall that
algorithm~\ref{alg:members} (section~\ref{sec:intrinsic}) converted the MCMM's $\mathbf{M}$ and $\mathbf{C}$
matrices into a list of wordlists, where each wordlist was (or represented) a cluster.} 
When inspecting a given cluster, I took into account its centroid vector, particularly the centroid's \emph{most active
features}, since these features help determine which shared bits of form among a cluster's words can truly be attributed to the MCMM. 
%That is, sometimes 
%directly and which are by-products or correlates of the 
%ten most active features in the centroid vector of the cluster in question. 
%
%\item A manual inspection of \emph{ten most active features} in each cluster's centroid vector (see chapter~\ref{sec:mcmm}, particularly sections~\ref{sec:mixing-function} and \ref{subsec:example})
%\item A 
%assembled via algorithm~\ref{alg:members} (section~\ref{sec:instrinsic})

The manual inspection revealed many clusters corresponding to morphological categories, including 
both concatenative and nonconcatenative morphs. In general, the most prominent morphs, e.g., the plural
suffixes \textit{-im}
and \textit{-wt}, corresponded to clusters with low indices ($k$ values), as the MCMM tended to discover these clusters first. The most 
prominent morphs were generally concatenative affixes and nonconcatenative vowel patterns. Thus, in the 
$\langle{s}=4,\delta=3\rangle$ clustering, for instance, the first cluster (i.e., the cluster indexed $k = 0$) corresponded to the morph \textit{-\'{e}.et},
%\footnote{The \_ here represents a single slot occupied by a consonant of a \emph{different} morph, namely the root. The \_  thus indicates discontiguity.}
a common feminine ending for participles.

%\begin{center}
%\textit{-\'{e}\_et} figure here?
%\end{center}
There were some clusters that corresponded to roots, although root clusters were less common in MCMM clusterings
than clusters corresponding to vowel patterns and affixes. 
%There are a many more unique roots in Hebrew
%than unique vowel patterns and affixal morphs, since roots are lexical, but individual roots tend to occur at far lower frequencies than 
%morphs that serve morphosyntactic purposes.
Roots also tended to occur at higher cluster indices (i.e., $k$ values); that is, the 
MCMM tended to ``find'' them later during the learning process. 
These facts are likely due to the lower frequencies of roots: Individual roots do not occur in as many distinct words
as concatenative morphs and vowel patterns, since roots are entirely lexical in nature. 
Root clusters therefore tended to be among the smallest of clusters.

\begin{figure}[!t]
\begin{mdframed}
\vspace{2pt}
{ \textbf{\textipa{P}.k.l}} \,{\text{\,(a.k.l)}} \hfill {$s = \mathbf{1}$, $\delta = \mathbf{3}$} \hfill { cluster 182}%: \textbf{4}, \textbf{3}} % \rangle$} {$s,
\vspace{3pt}
\begin{normalsize}
\begin{tabbing}
\hspace*{15ex}\= \hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{13ex} \kill
e\textbf{a}\textbf{k}\textbf{l}t \> \textbf{a}w\textbf{k}\textbf{l}t \> blklwkim \> \textbf{a}w\textbf{k}\textbf{l} \> awtkm \> e\textbf{a}w\textbf{k}\textbf{l}t \\
e\textbf{a}\textbf{k}\textbf{l}tm \> eklblbim \> h\textbf{a}w\textbf{k}\textbf{l} \> elh\textbf{a}\textbf{k}i\textbf{l} \> b\textbf{a}w\textbf{k}\textbf{l} \> enw\textbf{a}\textbf{k}\textbf{l} \\
hkwl \> hkil \> \textbf{a}\textbf{k}\textbf{l}tm \> blklkh \> hmkxwl \> blklkw \\
hklim \> cvrk \> ekwlm \> bekl \> \textbf{a}\textbf{k}\textbf{l}ti \> \textbf{a}\textbf{k}\textbf{l} \\
hmikl \> enstkl \> blklk \> htlklkh \> \textbf{a}st\textbf{k}\textbf{l} \> h\textbf{a}\textbf{k}\textbf{l}t 
\end{tabbing}
\end{normalsize}
\vspace{3pt}
\begin{mdframed}
\begin{small}
\textit{Ten most active centroid features:}
\vspace{-5pt}
\begin{tabbing}
\hspace*{6ex}\= \hspace*{12ex}\= \hspace*{6ex}\= \hspace*{12ex}\= \hspace*{6ex} \= \hspace*{12ex} \= \hspace*{6ex}\= \hspace*{12ex} \= \hspace*{6ex} \= \hspace*{10ex}\kill
%k<l \> (1.0000) \> a<k  \> (0.8271) \> t<k  \> (0.1012) \> w<k  \>  (0.0616) \> s<l  \> (0.0406) \\
 %k<k \> (0.0388) \> k<m  \> (0.0305) \> s<k  \> (0.0293)\> k<x  \> (0.0213) \> l<k  \> (0.0170) \\
 \texttt{k<l} \> (1.0000) \> \texttt{a<k} \> (0.8271) \> \texttt{t<k} \> (0.1012) \> \texttt{w<k} \> (0.0616) \> \texttt{s<l} \> (0.0406)\\
\texttt{k<k} \> (0.0388) \> \texttt{k<m} \> (0.0305) \> \texttt{s<k} \> (0.0293) \> \texttt{k<x} \> (0.0213) \> \texttt{l<k} \> (0.0170)
\end{tabbing}
\end{small}
\end{mdframed}
\vspace{-4pt}
\caption{Thirty words randomly selected from the 97 members of cluster 182 from the settings $s=1,\delta =3$, data representation: O}
\label{fig:cluster-182-1-3-O}
\end{mdframed}
\end{figure}

\begin{figure}[!t]
\begin{mdframed}
\vspace{2pt}
{ \textbf{\textipa{P}.k.l}}\, {\text{(a.k.l)}} \hfill {$s = \mathbf{3}$, $\delta = \mathbf{2}$} \hfill { cluster 91}
\vspace{3pt}
\begin{normalsize}
\begin{tabbing}
\hspace*{15ex}\= \hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{13ex} \kill
h\textbf{a}\textbf{k}\textbf{l}ti \> etwkli \> e\textbf{a}w\textbf{k}\textbf{l}im \> e\textbf{a}\textbf{k}\textbf{l}tm \> \textbf{a}\textbf{k}\textbf{l}i \> elh\textbf{a}\textbf{k}i\textbf{l} \\
blklwkim \> \textbf{a}\textbf{k}\textbf{l}h \> aekib \> e\textbf{a}\textbf{k}\textbf{l}t \> etw\textbf{a}\textbf{k}\textbf{l}i \> e\textbf{a}\textbf{k}\textbf{l} \\
arkib \> \textbf{a}\textbf{k}\textbf{l}tm \> \textbf{a}\textbf{k}\textbf{l} \> \textbf{a}\textbf{k}\textbf{l}ti \> enw\textbf{a}\textbf{k}\textbf{l} \> \textbf{a}\textbf{k}\textbf{l}t \\
\textbf{a}w\textbf{k}\textbf{l} \> \textbf{a}\textbf{k}i\textbf{l}h \> ehklb \> e\textbf{a}w\textbf{k}\textbf{l}t \> \textbf{a}e\textbf{k}w\textbf{l} \> \textbf{a}\textbf{k}\textbf{l}w \\
h\textbf{a}w\textbf{k}\textbf{l} \> ehkwl \> \textbf{a}w\textbf{k}\textbf{l}t \> etw\textbf{a}\textbf{k}\textbf{l} \> b\textbf{a}w\textbf{k}\textbf{l} \> e\textbf{a}w\textbf{k}\textbf{l}
%haklti \> etwkli \> eawklim \> eakltm \> akli \> elhakil \\
%blklwkim \> aklh \> aekib \> eaklt \> etwakli \> eakl \\
%arkib \> akltm \> akl \> aklti \> enwakl \> aklt \\
%awkl \> akilh \> ehklb \> eawklt \> aekwl \> aklw \\
%hawkl \> ehkwl \> awklt \> etwakl \> bawkl \> eawkl \\
\end{tabbing}
\end{normalsize}
\vspace{-3pt}
\begin{small}
\begin{mdframed}
\textit{Ten most active centroid features}
\vspace{-4pt}
\begin{tabbing}
\hspace*{6ex}\= \hspace*{12ex}\= \hspace*{7ex}\= \hspace*{12ex}\= \hspace*{6ex} \= \hspace*{13ex}\= \hspace*{8ex}\= \hspace*{12ex}\= \hspace*{8ex} \= \hspace*{10ex}\kill
%##91akl
\texttt{k<l} \> (1.0000) \> \texttt{a<k} \> (1.0000) \> \texttt{a<l} \> (0.4196) \> \texttt{k}@\texttt{[-1]} \> (0.4082) \> \texttt{y}@\texttt{[-1]} \> (0.2929)\\
\texttt{w<k} \> (0.2262) \> \texttt{k<i} \> (0.1747) \> \texttt{l<t} \> (0.1599) \> \texttt{k<t} \> (0.1414) \> \texttt{g}@\texttt{[1]} \> (0.0730)
\end{tabbing}
\end{mdframed}
\end{small}
\vspace{-5pt}
\caption{Thirty words randomly selected from the 42 members of cluster 91 from the settings $s=3,\delta =2$, data representation: O}
\label{fig:cluster-91-3-2-O}
\end{mdframed}
\end{figure}

%There were root clusters
% in the clusterings 
 All of three data types produced root clusters, but the
orthographic clusterings contained the largest number. This is perhaps to some degree a result
of the prevalence of consonants in Hebrew orthography (see chapter~\ref{ch:experi}).
Standard Hebrew orthography does not represent many vowels, and thus the consonants tend to denser; indeed, they are often adjacent. Thus, precedence features in the orthographic data can generally 
span more consonants than they can in the transcriptional datasets. 

The present section exemplifies some of the MCMM's clusters in a series of figures, each figure presenting a random sample of the members of a particular cluster. Each of these figures takes the form of a box that contains the following: 
\begin{itemize}
\item 30 words randomly selected from a particular cluster
\item The 10 most active features in this cluster's centroid. 
\end{itemize}
The first four figures all give word samples from clusters corresponding to the root \textit{\textipa{P}.k.l}, which is associated with the concept of eating, as in, for instance, \textit{\textipa{P}a\textsubdot{k}\'al} (`he/it ate') and \textit{\textipa{P}a\textsubdot{k}il\'a} (`eating').
Figure~\ref{fig:cluster-182-1-3-O} samples the words from cluster $k = 182$   
in the clustering $\langle s=1, \delta=3 \rangle$.
Its data source was the orthographic dataset, and so its words are expressed in the (transliterated) orthographic alphabet (see the discussion in chapter~\ref{ch:experi}). Thus, \textit{\textipa{P}.k.l} becomes \textit{a.k.l}.
In the words that actually contain the root \textit{a.k.l}, the letters \textbf{a}, \textbf{k}, and \textbf{l} appear in boldface.
Notice that not all of the words in figure~\ref{fig:cluster-182-1-3-O} have this root.

\begin{figure}[!t]
\begin{mdframed}
\vspace{2pt}
{ \textbf{\textipa{P}.k.l}}\, {\text{\, (a.k.l)}}  \hfill {$s = \mathbf{2}$, $\delta = \mathbf{2}$} \hfill { cluster 81}
%\vspace{-3pt}%\hfill {\scriptsize Size: 17}\\
%{\small \emph{Purity: 1.0}}\\
\vspace{3pt}
\begin{normalsize}
\begin{tabbing}
\hspace*{15ex}\= \hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{13ex} \kill
b\textbf{a}w\textbf{k}\textbf{l} \> \textbf{a}w\textbf{k}\textbf{l}t \> \textbf{a}e\textbf{k}w\textbf{l} \> etw\textbf{a}\textbf{k}\textbf{l}i \> etw\textbf{a}\textbf{k}\textbf{l} \> el\textbf{a}\textbf{k}w\textbf{l} \\
h\textbf{a}w\textbf{k}\textbf{l} \> \textbf{a}\textbf{k}\textbf{l}nw \> e\textbf{a}w\textbf{k}\textbf{l} \> \textbf{a}\textbf{k}\textbf{l}h \> \textbf{a}e\textbf{k}w\textbf{l}im \> \textbf{a}w\textbf{k}\textbf{l} \\
h\textbf{a}\textbf{k}\textbf{l}ti \> \textbf{a}\textbf{k}\textbf{l}i \> enw\textbf{a}\textbf{k}\textbf{l} \> e\textbf{a}w\textbf{k}\textbf{l}im \> \textbf{a}\textbf{k}\textbf{l}ti \> \textbf{a}\textbf{k}\textbf{l}w \\
\textbf{a}\textbf{k}\textbf{l}tm \> alk \> \textbf{a}w\textbf{k}\textbf{l}im \> \textbf{a}\textbf{k}\textbf{l} \> e\textbf{a}\textbf{k}\textbf{l}tm \> a\textbf{a}\textbf{k}i\textbf{l} \\
\textbf{a}\textbf{k}\textbf{l}t \> \textbf{a}\textbf{k}i\textbf{l}h \> eiw\textbf{a}\textbf{k}\textbf{l} \> b\textbf{a}\textbf{k}i\textbf{l}h \> e\textbf{a}\textbf{k}\textbf{l} \> h\textbf{a}\textbf{k}\textbf{l}t
%bawkl \> awklt \> aekwl \> etwakli \> etwakl \> elakwl \\
%hawkl \> aklnw \> eawkl \> aklh \> aekwlim \> awkl \\
%haklti \> akli \> enwakl \> eawklim \> aklti \> aklw \\
%akltm \> alk \> awklim \> akl \> eakltm \> aakil \\
%aklt \> akilh \> eiwakl \> bakilh \> eakl \> haklt
\end{tabbing}
\end{normalsize}
%\label{fig:cluster-81-2-2-O}
%\caption{Thirty words randomly selected from the 35 members of cluster 81 from the settings $s=2,\delta =2$, data representation: O}
\vspace{-5pt}
\begin{small}
\begin{mdframed}
\textit{Ten most active centroid features}
\vspace{-5pt}
\begin{tabbing}
\hspace*{6ex}\= \hspace*{12ex}\= \hspace*{6ex}\= \hspace*{12ex}\= \hspace*{6ex} \= \hspace*{12ex}\= \hspace*{6ex}\= \hspace*{12ex}\= \hspace*{6ex} \= \hspace*{10ex}\kill
\texttt{k<l} \> (1.000) \> \texttt{a<l} \> (1.000) \> \texttt{a<k} \> (1.000) \> \texttt{w<k} \> (0.181) \> \texttt{k<t} \> (0.160)\\
\texttt{k<i} \> (0.139) \> \texttt{i}@\texttt{[0]} \> (0.112) \> \texttt{w<a} \> (0.068) \> \texttt{h<k} \> (0.058) \> \texttt{l<h} \> (0.044)
\end{tabbing}
\end{mdframed}
\end{small}
\vspace{-5pt}
\caption{Thirty words randomly selected from the 35 members of cluster 81 from the settings $s=2,\delta =2$, data representation: O}
\label{fig:cluster-81-2-2-O}
\end{mdframed}
\end{figure}

Figures~\ref{fig:cluster-91-3-2-O} and \ref{fig:cluster-81-2-2-O} represent \textit{a.k.l}
(\textit{\textipa{P}.k.l}) clusters from the clusterings generated at
$\langle{s}=3,\delta=2 \rangle$ and $\langle s=2, \delta=2 \rangle$, respectively. 
%The cluster in figure~\ref{fig:cluster-91-3-2-O} is better than the one in figure~\ref}fig:cluster-81-2-2-O}, previous one in that each is 
These clusters are both better than the first one in that they are
\emph{purer} representations of the root \textit{a.k.l} (\textit{\textipa{P}.k.l}); that is,
each has a greater proportion of words that actually contain the root \textit{a.k.l}.
Moreover, for both of these clusters, the two \emph{most-active} features are 
%for both of these clusters are
\begin{center}
 { \texttt{a<k}} \quad  and \quad { \texttt{k<l}}
\end{center}
This makes sense. Clearly, no other feature, except perhaps \texttt{a<l}, would be as helpful in 
encoding the sequence of  position-variant (or ``floating'') consonants \textit{a.k.l}. In figure~\ref{fig:cluster-182-1-3-O}, the activities of \texttt{k<l} and \texttt{a<k} are 1.0 and 0.8271, respectively.
Figure~\ref{fig:cluster-91-3-2-O}, showing a purer cluster, has these features both at 1.0 activities. It also has \texttt{a<l} among its top ten features, 
at an activity of 0.4196, a low activity to be sure, but this feature does not appear at all among the top ten features of the preceding figure's
cluster. 
Finally, figure~\ref{fig:cluster-81-2-2-O}, the purest of these three clusters, has all three of the features \texttt{k<l}, \texttt{a<k}, and \texttt{a<l} among
its ten most active,
and, moreover, it has each of these features at a perfect activity of 1.0.

\begin{figure}[!t]
\begin{mdframed}
\vspace{2pt}
{ \textbf{\textipa{P}.k.l}} \hfill {$s = \mathbf{0}$, $\delta = \mathbf{2}$} \hfill { cluster 33}
 \vspace{3pt}
 \begin{normalsize}
\begin{tabbing}
\hspace*{17.5ex}\= \hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{13ex} \kill
%\v{s}e\textipa{P}o\textsubdot{k}l\a'{i}m \> hat\textsubdot{k}\a'{e}let \> \textipa{Q}a\textsubdot{k}b\a'{a}r \> \v{s}e\textipa{P}a\textsubdot{k}\a'{a}ltem \> webi\textsubdot{k}l\a'{a}l \> ma\textsubdot{k}\a'{i}n \\
%ta\textipa{P}a\textsubdot{k}\a'{i}li \> hamelu\textsubdot{k}l\a'{a}\textsubdot{k} \> wehal\textsubdot{k}\a'{a} \> hithap\textsubdot{k}\a'{a} \> midra\textsubdot{k}\a'{a} \> xa\textsubdot{k}\a'{a}m \\
%wehamelu\textsubdot{k}la\textsubdot{k}\a'{i}m \> weme\v{s}u\textsubdot{k}l\a'{a}l \> mitla\textsubdot{k}le\textsubdot{k}\a'{i}m \> \textipa{P}a\textsubdot{k}\a'{a}l \> hamidra\textsubdot{k}\a'{a} \> we\textipa{P}a\textsubdot{k}\a'{a}lt \\
%la\textipa{Q}a\textsubdot{k}\v{s}\a'{a}yw \> he\textipa{P}e\textsubdot{k}\a'{a}lt \> yela\textsubdot{k}l\a'{e}\textsubdot{k} \> ya\textsubdot{k}\a'{o}lnu \> we\textipa{P}a\textsubdot{k}l\a'{u} \> ya\textsubdot{k}\a'{o}l \\
%we\textipa{P}a\textsubdot{k}\a'{a}l \> leyad\textsubdot{k}\a'{a} \> na\textsubdot{k}\a'{i}n \> we\textipa{P}a\textsubdot{k}\a'{a}lti \> \v{s}e\textipa{P}a\textsubdot{k}n\a'{i}s \> wena\textsubdot{k}n\a'{i}s \\
\v{s}e\textbf{\textipa{P}}o\textbf{\textsubdot{k}}\textbf{l}\a'{i}m \> hat\textsubdot{k}\a'{e}let \> \textipa{Q}a\textsubdot{k}b\a'{a}r \> \v{s}e\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l}tem \> webi\textsubdot{k}l\a'{a}l \> ma\textsubdot{k}\a'{i}n \\
ta\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{i}\textbf{l}i \> hamelu\textsubdot{k}l\a'{a}\textsubdot{k} \> wehal\textsubdot{k}\a'{a} \> hithap\textsubdot{k}\a'{a} \> midra\textsubdot{k}\a'{a} \> xa\textsubdot{k}\a'{a}m \\
wehamelu\textsubdot{k}la\textsubdot{k}\a'{i}m \> weme\v{s}u\textsubdot{k}l\a'{a}l \> mitla\textsubdot{k}le\textsubdot{k}\a'{i}m \> \textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l} \> hamidra\textsubdot{k}\a'{a} \> we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l}t \\
la\textipa{Q}a\textsubdot{k}\v{s}\a'{a}yw \> he\textbf{\textipa{P}}e\textbf{\textsubdot{k}}\a'{a}\textbf{l}t \> yela\textsubdot{k}l\a'{e}\textsubdot{k} \> ya\textsubdot{k}\a'{o}lnu \> we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\textbf{l}\a'{u} \> ya\textsubdot{k}\a'{o}l \\
we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l} \> leyad\textsubdot{k}\a'{a} \> na\textsubdot{k}\a'{i}n \> we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l}ti \> \v{s}e\textipa{P}a\textsubdot{k}n\a'{i}s \> wena\textsubdot{k}n\a'{i}s 
\end{tabbing}
\end{normalsize}
\vspace{-3pt}
\begin{mdframed}
\begin{small}
\textit{Ten most active centroid features}
\vspace{-3pt}
\begin{tabbing}
\hspace*{6ex}\= \hspace*{12ex}\= \hspace*{6ex}\= \hspace*{12ex} \= \hspace*{6ex} \= \hspace*{12ex} \= \hspace*{7ex}\= \hspace*{13ex} \= \hspace*{7ex} \= \hspace*{9.5ex}\kill
\texttt{a<\textsubdot{k}} \> (1.0000) \> \texttt{\textsubdot{k}<\a'{a}} \> (0.8585) \> \texttt{\textsubdot{k}<l} \> (0.7588) \> \texttt{\textipa{P}<\textsubdot{k}} \> (0.4984) \> \texttt{\textsubdot{k}<\a'{i}} \> (0.4439)\\
\texttt{e<\textsubdot{k}} \> (0.3341) \> \texttt{\a'{a}<l} \> (0.3159) \> \texttt{\textsubdot{k}<n} \> (0.2960) \> \texttt{o<\textsubdot{k}} \> (0.2271) \> \texttt{l<\textsubdot{k}} \> (0.2206)
\end{tabbing}
\end{small}
\end{mdframed}
\caption{Thirty words randomly selected from the 163 members of cluster 33 from the settings $s=0,\delta =2$, data representation: TS}
\label{fig:cluster-33-0-2-TS}
\end{mdframed}
\end{figure}

\begin{figure}[!t]
\begin{mdframed}
\vspace{2pt}
{ \textbf{d.l.q}} \hfill {$s = \mathbf{4}$, $\delta = \mathbf{3}$} \hfill { cluster 339}\\ 
\vspace{-3pt}
 \begin{normalsize}
\begin{tabbing}
\hspace*{15ex}\= \hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{13ex} \kill
hi\textbf{d}\textbf{l}\a'{i}\textbf{q} \> weleha\textbf{d}\textbf{l}\a'{i}\textbf{q} \> ha\v{s}ehi\textbf{d}\textbf{l}\a'{i}\textbf{q} \> ya\textbf{d}\textbf{l}\a'{i}\textbf{q} \> lehadb\a'{i}q \> wehi\textbf{d}\textbf{l}\a'{i}\textbf{q} \\
tadb\a'{i}qi \> hi\textbf{d}\textbf{l}\a'{a}\textbf{q}nu \> ta\textbf{d}\textbf{l}\a'{i}\textbf{q} \> welehadb\a'{i}q \> wenadb\a'{i}q \> we\v{s}eyadb\a'{i}q \\
lagdol\a'{i}m \> wema\textbf{d}\textbf{l}\a'{i}\textbf{q} \> mavdil\a'{i}m \> \textipa{P}a\textbf{d}\textbf{l}\a'{i}\textbf{q} \> hi\textbf{d}\textbf{l}\a'{i}\textbf{q}u \> ta\textbf{d}\textbf{l}\a'{i}\textbf{q}i \\
basandal\a'{i}m \> ma\textbf{d}\textbf{l}i\textbf{q}\a'{a} \> hi\textbf{d}\textbf{l}\a'{a}\textbf{q}t \> \v{s}etadb\a'{i}qi \> hi\textbf{d}\textbf{l}\a'{a}\textbf{q}ti \> leha\textbf{d}\textbf{l}\a'{i}\textbf{q} \\
\textbf{d}o\textbf{l}\textbf{q}\a'{i}m \> gdel\a'{i}m \> gdol\a'{i}m \> hasandal\a'{i}m \> ni\textbf{d}\textbf{l}\a'{a}\textbf{q} \> \textbf{d}\textbf{l}u\textbf{q}\a'{a}
%\hspace*{13ex}\= \hspace*{13ex}\=\hspace*{15ex}\=\hspace*{15ex}\=\hspace*{15ex}\=\hspace*{15ex} \kill
%hidl\a'{i}q \> welehadl\a'{i}q \> ha\v{s}ehidl\a'{i}q \> yadl\a'{i}q \> lehadb\a'{i}q \> wehidl\a'{i}q \\
%tadb\a'{i}qi \> hidl\a'{a}qnu \> tadl\a'{i}q \> welehadb\a'{i}q \> wenadb\a'{i}q \> we\v{s}eyadb\a'{i}q \\
%lagdol\a'{i}m \> wemadl\a'{i}q \> mavdil\a'{i}m \> \textipa{P}adl\a'{i}q \> hidl\a'{i}qu \> tadl\a'{i}qi \\
%basandal\a'{i}m \> madliq\a'{a} \> hidl\a'{a}qt \> \v{s}etadb\a'{i}qi \> hidl\a'{a}qti \> lehadl\a'{i}q \\
%dolq\a'{i}m \> gdel\a'{i}m \> gdol\a'{i}m \> hasandal\a'{i}m \> nidl\a'{a}q \> dluq\a'{a} \\
\end{tabbing}
\end{normalsize}
\vspace{-5pt}
\begin{mdframed}
\begin{small}
\textit{Ten most active centroid features}
\vspace{-3pt}
\begin{tabbing}
\hspace*{6ex}\= \hspace*{11ex}\= \hspace*{6ex}\= \hspace*{11ex}\= \hspace*{7ex} \= \hspace*{11ex} \= \hspace*{8ex}\= \hspace*{11ex} \= \hspace*{8ex} \= \hspace*{9.5ex}\kill
\texttt{d<\a'{i}} \> (1.000) \> \texttt{d<l} \> (1.000) \> \texttt{l<q} \> (0.853) \> \texttt{d<q} \> (0.798) \> \texttt{\a'{i}<q} \> (0.501)\\
\texttt{h<d} \> (0.374) \> \texttt{l<\a'{i}} \> (0.162) \> \texttt{g<d} \> (0.157) \> \texttt{\a'{e}}@\texttt{[-1]} \> (0.135) \> \texttt{r}@\texttt{[-2]} \> (0.135)
\end{tabbing}
\end{small}
\end{mdframed}
\vspace{-5pt}
\caption{Thirty words randomly selected from the 59 members of cluster 339 from the settings $s=4,\delta =3$, data representation: TS}
\label{fig:cluster-339-4-3-TS}
\end{mdframed}
\end{figure}

\begin{figure}[htb]
\begin{mdframed}
\vspace{2pt}
{\textbf{a.\'{e}}} \hfill {$s = \mathbf{2}$, $\delta = \mathbf{3}$} \hfill { cluster 19}
\vspace{3pt}
\begin{normalsize}
\begin{tabbing}
\hspace*{15ex}\= \hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{16ex}\=\hspace*{13ex} \kill
%lexam\a'{e}\v{s} \> taxac\a'{e} \> lexap\a'{e}\textsubdot{s} \> \textipa{P}a\textsubdot{k}as\a'{e} \> \v{s}e\textipa{P}axr\a'{e}y \> mitla\textsubdot{k}l\a'{e}\textsubdot{k} \\
%k\v{s}e\textipa{P}at\a'{e}m \> \textipa{Q}acm\a'{e}nu \> sifr\a'{e}y \> wenexap\a'{e}\textsubdot{s} \> weyot\a'{e}r \> metaqt\a'{e}q \\
%mera\v{s}r\a'{e}\v{s} \> tefaz\a'{e}r \> meyac\a'{e}v \> texab\a'{e}q \> lehizah\a'{e}r \> ne\textsubdot{s}ax\a'{e}q \\
%cam\a'{e}\textipa{P} \> we\textipa{P}ax\a'{e}r \> lehi\v{s}tat\a'{e}f \> teqal\a'{e}f \> \textipa{P}asad\a'{e}r \> mitxal\a'{e}f \\
%haqos\a'{e}m \> \textipa{P}axam\a'{e}m \> meqalq\a'{e}l \> \v{s}emi\v{s}tal\v{s}\a'{e}l \> mitrax\a'{e}c \> teqab\a'{e}l
lex\textbf{a}m\textbf{\a'{e}}\v{s} \> taxac\a'{e} \> lex\textbf{a}p\textbf{\a'{e}}\textsubdot{s} \> \textipa{P}a\textsubdot{k}as\a'{e} \> \v{s}e\textipa{P}axr\a'{e}y \> mitl\textbf{a}\textsubdot{k}l\textbf{\a'{e}}\textsubdot{k} \\
k\v{s}e\textipa{P}at\a'{e}m \> \textipa{Q}acm\a'{e}nu \> sifr\a'{e}y \> wenex\textbf{a}p\textbf{\a'{e}}\textsubdot{s} \> weyot\a'{e}r \> met\textbf{a}qt\textbf{\a'{e}}q \\
mer\textbf{a}\v{s}r\textbf{\a'{e}}\v{s} \> tef\textbf{a}z\textbf{\a'{e}}r \> mey\textbf{a}c\textbf{\a'{e}}v \> tex\textbf{a}b\textbf{\a'{e}}q \> lehiz\textbf{a}h\textbf{\a'{e}}r \> ne\textsubdot{s}\textbf{a}x\textbf{\a'{e}}q \\
c\textbf{a}m\textbf{\a'{e}}\textipa{P} \> we\textipa{P}\textbf{a}x\textbf{\a'{e}}r \> lehi\v{s}t\textbf{a}t\textbf{\a'{e}}f \> teq\textbf{a}l\textbf{\a'{e}}f \> \textipa{P}as\textbf{a}d\textbf{\a'{e}}r \> mitx\textbf{a}l\textbf{\a'{e}}f \\
haqos\a'{e}m \> \textipa{P}ax\textbf{a}m\textbf{\a'{e}}m \> meq\textbf{a}lq\textbf{\a'{e}}l \> \v{s}emi\v{s}t\textbf{a}l\v{s}\textbf{\a'{e}}l \> mitr\textbf{a}x\textbf{\a'{e}}c \> teq\textbf{a}b\textbf{\a'{e}}l
\end{tabbing}
\end{normalsize}
\vspace{-3pt}
\begin{mdframed}
\begin{small}
\textit{Ten most active centroid features}
\vspace{-3pt}
\begin{tabbing}
\hspace*{7ex}\= \hspace*{12ex}\= \hspace*{7ex}\= \hspace*{12ex} \= \hspace*{7ex} \= \hspace*{12ex} \= \hspace*{6ex}\= \hspace*{12ex} \= \hspace*{6ex} \= \hspace*{10ex}\kill
%2_3_cluster_19
\texttt{a<\a'{e}} \> (1.0000) \> \texttt{\a'{u}}@\texttt{[1]} \> (0.9931) \> \texttt{r<\a'{e}} \> (0.1233) \> \texttt{\a'{e}<r} \> (0.1112) \> \texttt{x<\a'{e}} \> (0.1097)\\
\texttt{n<\a'{e}} \> (0.0827) \> \texttt{q<\a'{e}} \> (0.0820) \> \texttt{b<\a'{e}} \> (0.0805) \> \texttt{t<\a'{e}} \> (0.0751) \> \texttt{d<\a'{e}} \> (0.0662)
\end{tabbing}
\end{small}
\end{mdframed}
\vspace{-3pt}
\caption{Thirty words randomly selected from the 1052 members of cluster 19 from the settings $s=2,\delta =3$, data representation: TS. Actual instances of the \textit{Pi`el/Hitpa`el} vowel pattern \textit{a.\'{e}} are in boldface.}
\label{fig:cluster-19-2-3-TS}
\end{mdframed}
\end{figure}
Figure~\ref{fig:cluster-33-0-2-TS} shows yet another cluster corresponding to the root \textit{a.k.l/}\textit{\textipa{P}.k.l}, this one
having come from the transcriptional dataset with stress marking. This cluster was generated under the parameter settings $\langle s=0, \delta=2 \rangle$. An $s$ value of 0 means that positional features were completely absent in this clustering (see chapter~\ref{ch:experi}).
%However, this lack would not have hindered the MCMM in finding roots, since precedence features are the better feature type for capturing roots.
The cluster-member sample in figure~\ref{fig:cluster-33-0-2-TS} shows partial success in finding the root 
\textit{\textipa{P}.k.l}, but certainly less success than the three preceding clusters show
(figures~\ref{fig:cluster-182-1-3-O}, \ref{fig:cluster-91-3-2-O},
and \ref{fig:cluster-81-2-2-O}). 

One might ask whether this cluster's relative lack of coherence might be due to the absence of positional features. This is unlikely, however,
since the positions of a root's consonants are not fixed, but rather vary considerably, depending on affixation and the vowel pattern with which the root combines.
% Rather,  consonants can vary widely from word to word
%This is 
 %probably not due to the absence of positional features, however, since positional 
 %features do not seem conducive to representing a sequence of characters whose positions may vary.
 %The boldface letters in figure~\ref{fig:cluster-33-0-2-TS}.
 %demonstrate the positional variability of root consonants from word to word. 
 The simple attachment of an affix moves a root away from the edges of a word; for example,
 the positional feature  \texttt{{\textipa{P}}}@\texttt{[0]} is true in \textit{\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l}}, but the attachment of \textit{we-} (`and') renders this same feature false in \textit{we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l}}.
The positions of root consonants can also vary \textit{relative to each other}.  
For example, the position of \textit{l} relative to \textit{x} is different in the words \textit{we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\a'{a}\textbf{l}} and \textit{we\textbf{\textipa{P}}a\textbf{\textsubdot{k}}\textbf{l}\a'{u}}.
Therefore, it seems unlikely that the absence of positional features is the reason for the relative lack of purity in figure~\ref{fig:cluster-33-0-2-TS}. This is supported by the scarcity of positional features among the top ten features of the other clusters in this section.\footnote{Suppose there existed a root $C_1$.$C_2$.$C_3$ that was \emph{always} discontinuous, with exactly $n$ non-root characters always separating $C_1$ from $C_2$, and exactly $m$ non-root characters always separating $C_2$ from $C_3$. Suppose further that each root consonant always (i.e., in each word) occupied the \emph{same} position relative to either the beginning or end of the word. As long as the positions of its root consonants remained fixed, \emph{this nonconcatenative root could be represented entirely by positional features.} 
\footnote The bipartite architecture of the MCMM should be able to form the necessary clusters based solely on non-adjacent positional features, since it is the bipartiteness of the MCMM that allows it to represent discontinuous units of structure, not particular feature types \textit{per se}. However, if the root consonants were permitted to change their positional indices in response to prefix attachment, etc., then positional features would no longer be adequate.}
% t seems that positional features would not have been much help anyway, and are  
%The fact that positional features are largely absent from the top ten features of the other clusters in this section supports this notion. `
%It is not clear exactly why this cluster is not as good as the preceding clusters. What is clear, however, is that this cluster's poorer 
%Let us take a closer look at the features of this bad boy.
%Do we see anything curious?

Let us now take a closer look of top ten features of this cluster (figure~\ref{fig:cluster-33-0-2-TS}) with an eye to explaining its relative incoherence. 
Of course, all of this cluster's features are precedence features. But even so, the top ten features in this cluster's centroid are not ideal for representing 
a %position-variant 
sequence of position-variant consonants.
% closer look at this cluster's top ten centroid features, one can see that they are not ideal for capturing the root \textit{\textipa{P}.k.l}. 
The activities of the important features 
\texttt{\textipa{P}<\textsubdot{k}} (= orthographic \texttt{a<k}) and \texttt{\textsubdot{k}<l} (= orthographic \texttt{k<l}) are only 0.4984 and
 0.7588, respectively. Both of these should probably be closer to 1.0 if this cluster is to represent the
 root \textit{{\textipa{P}.k.l}}. Moreover, six of these ten features reference vowels. 
%at in figure~\ref{fig:cluster-33-0-2-TS} particularly well with
% If we could choose features and their activities
%for representing \textit{\textipa{P}.k.l}, we would certainly not choose  an activity of 0.4984 for {\textipa{P}<\textsubdot{k}},  \ (0.4984) for {\textipa{P}<\textsubdot{k}}
In fact, the two most active features, namely \texttt{a<\textsubdot{k}} (activity 1.0) and \texttt{\textsubdot{k}<\'{a}} (activity 0.8585), both reference a vowel.\footnote{In the transcriptional datasets, \textsf{a} is a vowel.} References to specific vowels are problematic for roots because different vowel patterns have different vowels, and thus the particular sequence of vowels that interleaves with a root's consonants can vary from word to word.
%  as different vowel patterns have different vowels.

Another cluster of transcribed words (with stress-marking) is represented in figure~\ref{fig:cluster-339-4-3-TS}. 
This one corresponds to the root \textit{d.l.q} and is considerably purer than the 
cluster in figure~\ref{fig:cluster-33-0-2-TS}. The activities of the two key root features \texttt{d<l} and \texttt{l<q} are considerably higher than those of their counterpart (namely \texttt{\textipa{P}<\textsubdot{k}} and \texttt{\textsubdot{k}<l}) in figure~\ref{fig:cluster-33-0-2-TS}. We also have the ``third'' root feature \texttt{d<q} in this cluster. Such ``third'' root features, i.e., those spanning the first root consonant to the third (and generally last) root consonant, tend to be rarer and/or have weaker activities in transcriptional root clusters,
since the distance between the first and last root consonants often exceeds $\delta$ when vowels are 
involved. Moreover, the activity of this particular third root feature--namely, 0.798---is fairly high. (And yet it is still well below the perfect 1.0 of its analogue \texttt{a<l} in the orthographic cluster in figure~\ref{fig:cluster-81-2-2-O}.)
%While the activity of this particular third root feature fairly high, namely, 0.798,  compare it to the 1.0 activity of its 
% had an activity of 1.0.
We must once again note the problematic influence of vowel-referencing features, however, particularly features that
reference \textit{\'{i}}, such as \texttt{d<\'{i}} and \texttt{\'{i}<q}.
The former is especially strong, with an activity of 1.0.  %especially strong, having an activity of 1.0, 
%The feature \texttt{\'{i}<q} activity  
% %a modest activity of 
% 0.501. 
 The \textit{\'{i}} in these two features is reflected in frequency of \textit{\'{i}} in 
this cluster's words. It would thus be more accurate to say that this cluster represents a subset of the 
occurrences of root \textit{d.l.q}, namely its co-occurrences with \textit{\'{i}}.

Our last cluster example in this section is the one in figure~\ref{fig:cluster-19-2-3-TS}. It demonstrates 
Multimorph's capacity to learn vowel patterns. The pattern in this case is \textit{a.\'{e}}.
 It is a morphological component of the \textit{Hitpa`el} binyan, 
as in \textit{mitrax\'{e}c}, as well as the 
the prefix stem of the \textit{Pi`el} binyan,
e.g., \textit{teqal\'{e}f}.
As in the case of roots, precedence features seem to be better suited to capturing vowel patterns than positional features.
%In general, the precedent feature type seems to be essential for representing
%nonconcatenative phenomena of any kind. 
%Positional features, by contrast, do not appear to be essential for representing nonconcatenative morphs, and they could even be 
%detrimental.

% \texttt{d<q} \> (0.798) \> \texttt{\a'{i}<q} \> (0.501)\\
%\texttt{h<d} \> (0.374) \> \texttt{l<\a'{i}} \> (0.162) \> \texttt{g<d} \> (0.157) \> \texttt{\a'{e}}@\texttt{[-1]} \> (0.135) \> \texttt{r}@\texttt{[-2]} \> (0.135)
%\texttt{d<\a'{i}} \> (1.000) \>

%are the better feature type for capturing  better for capturing roots anyone 
%these clusters' top-ten feature lists that the two most import features for capturing
%the root \textit{a.k.l} are 
%Furthermore, notice that 

% figures~\ref{fig:cluster-182-1-3-O}, \ref{fig:cluster-91-3-2-O}, and \ref{fig:cluster-81-2-2-O} are samples from
%The root \textit{\textipa{P}.k.l} occurs both in the orthographic and transcriptional clusters. This section provides random subsets from some of these 
%\textit{\textipa{P}.k.l} clusters. Figures~\ref{fig:cluster-182-1-3-O}, \ref{fig:cluster-91-3-2-O}, and \ref{fig:cluster-81-2-2-O} are samples from
%orthographic  \textit{\textipa{P}.k.l}  
%present a sample of thirty words
% 
% the following information:
%\begin{enumerate}
%\item Along the top of the box, from left to right: 
%	\begin{enumerate}
%	\item the cluster's equivalent morph in large, boldface type
%	\item the parameter settings ($s$ and $\delta$ values) under which the cluster was formed
%	\item the cluster's index within its clustering
%	\end{enumerate}
%\item Thirty words randomly selected from a particular cluster. 
%\item The ten most active features in the cluster's centroid.
%\end{enumerate}
%the cluster's equivalent morph in large boldface type, the parameter settings ($s$ and $\delta$ values) of its larger
%clustering, and the index of the cluster within its cluster

%First, each figure presents thirty words randomly selected from a particular cluster. 
%Second, these words is another box 
%containing the ten most active features in the it's
%centroid vector, along with their activities. In the 
%upper left corner, the cluster's corresponding morph is represented as string in larger, 
%boldface type. In the upper right corner,
%the cluster's index is displayed.  



%three data types. In this section, we take a look a  \ref{fig:cluster-182-1-3-O}, there is some interesting shit from cluster indexed 182---
%i.e.,  It it
%The occurrence of \textit{\textipa{P}.k.l} clusters in different clusterings provides a common ground for examining the effects of 
%
%Precedence feats are more useful than pos feats; they can capture all manner of morphs—any morph consisting of at least 2 chars, that is.
%
%Positional feats, especially at large s values, seem to tend to introduce noisy features—features that cannot correspond to any morph.
%
%There are certain ideal ways to form apples. There are certain ideal shapes, ideal combinations of atomics such as the precedent feature-transitive chain template for roots. Is a third feature important? Hey d 
%
%Why are there feats that seemingly don’t do anything?
%Possible reasons:
%Noise due to Nonconvergence
%Very low m vals overall, which might cause high centroid feature values (but so what?)
%
%It seems that only a single precedence feature is necessary to capture vowel patterns, as vowel patterns generally consist of no more than two vowels.
%
%In general, positional features do not seem to be very useful in capturing nonconcatenative phenomena. But what is the evidence? Well, I haven’t observed an instance of a positional feature making a positive contribution toward capturing a single, coherent nonconcatenative morph. Consider what it would look like if a positional feature were involved in capturing a root—say, d.l.q, for example. 


%Among the clusterings generated from the transcriptional datasets, the TS clusterings contained a 10 root clusters with 10 or more members 


%It is thus not
%within a clustering, i.e., wi
%a wide variety of morphs were represented in the clusterings---both 
%concatenative and noncocatenative morphs. 
%Moreover, both concatenative and nonconcatenative morphs were  prenumerous morphological categories including, %-interpretable clusters, including 
%%clusters corresponding to the morphological categories
%for example \textit{-{o}t} \> feminine plural   \textit{-\v{s}e} \> (prefixal particle `that/which')
%\begin{tabular}{ll}
%
%\end{tabular}
%\hspace{1in} \= \hspace{5.5in} \kill
%\textit{-\a'{o}t} \> feminine plural  \\ %\textit{-ot}, \textit{-wt})
%\textit{-\a'{i}m} \> masculine plural \\
%%morph: \textit{-\'nu} (first-person plural marker in past-tense verbs, etc.)
%\textit{ha-} \> a feminine singular ending, see section~\ref{sec:heb-example} \\
%\textit{we-} \> conjunctive prefixal particle \\
%\textit{-\v{s}e} \> prefixal particle 'that/which' complementizer and relativizer
%\end{tabbing}
%For example, figure~\ref{fig:cl-fem} displays 
%a random sample of 60 words from a 1632-word cluster produced by Multimorph’s MCMM. This group 
%is intended as an abridgment of its superset, which is too large to display here. This cluster was produced by the 
%MCMM at the experimental settings $\delta = 2$, $s = 2$, and $K = 1000$, i.e. it was one of 
%%other 
%1000 clusters that the MCMM produced during this experimental run.
%
%
%\begin{figure}[t]
%\begin{mdframed}
%\begin{tabbing}
%\hspace*{1ex}\= \hspace*{13ex}\= \hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex} \kill
%\> nafalt \> pizart \> webarakevet \> werevi\textipa{P}it \> we\textipa{P}omeret \> \textipa{P}emet\\
%\> bubot \> hakapot \> mexaberet \> wela\textipa{P}alot \> wemakot \>\v{s}ehizmant\\
%\> bamesilot \> bami\v{s}qefet \> fizit \> mistateret \> ye\textsubdot{k}olot \> \textipa{P}o\textsubdot{k}elet\\
%\> be\textipa{P}emet \> kazo\textipa{P}t \> ktumot \> laxalonot \> mat\textipa{P}imot \> \textipa{P}axeret\\
%\> hatmunot \> hawilonot \> ha\textipa{P}otiyot \> li\v{s}tot \> moxeqet \>\v{s}era\textipa{P}it\\
%\> dri\v{s}at \> lanequdot \> maxliqot \> mitxape\textsubdot{s}et \> wexalonot \>\v{s}e\textglotstop{P}amart\\
%\> daqot \> habdiqot \> megare\v{s}et \> ni\textsubdot{k}nast \> pi\textsubdot{t}riyot \>\v{s}ehaxanuyot\\
%\> ha\v{s}amenet \> hiclaxt \> laxalalit \> meha\textsubdot{s}aqit \> nimce\textipa{P}t \>\v{s}lulonet\\
%\> hahit\textipa{P}amlut \> labanot \> mela\textsubdot{k}le\textsubdot{k}et \> safart \> xada\v{s}ot \> \textipa{P}acuvot\\
%\> bakisa\textipa{P}ot \> madregot \> melu\textsubdot{k}la\textsubdot{k}ot \> melu\textsubdot{k}le\textsubdot{k}et \> mit\textipa{P}aqe\v{s}et \> \textipa{P}orot
%\end{tabbing}
%\caption{Sixty words randomly selected from a 1632-word cluster generated by Multimorph's MCMM (at $s = 2$, and $\delta = 2$). The endings on these words are the feminine endings discussed in section~\ref{sec:heb-example} of chapter~\ref{autonomous}.} 
%\label{fig:cl-fem}
%\end{mdframed}
%\end{figure}
%\begin{figure}[tb!]
%\begin{mdframed}
%\textit{Ten most active centroid features:}
%\small
%\begin{tabbing}
%\hspace*{1ex}\= \hspace*{17ex}\= \hspace*{17ex}\=\hspace*{17ex}\=\hspace*{17ex}\=\hspace*{17ex} \kill
%\> \texttt{w@[1]} (1.0000) \> \texttt{r<t} \, (0.0663) \> \texttt{l<t} \, (0.0455) \> \texttt{q<t} \, (0.0392) \> \texttt{v<t} \, (0.0258) \\
%\> \texttt{\textsubdot{k}<t} \, (0.0257) \> \texttt{u<t} \, (0.0240) \> \texttt{d<t} \, (0.0176) \> \texttt{o<t} \, (0.0153) \> \texttt{c<t} \, (0.0143)
%\end{tabbing}
%\label{fig:fem-features}
%\caption{The ten most active features in the cluster represented in figure~\ref{fig:cl-fem}. The feature (surface-unit) activities are included in parentheses.}
%\end{mdframed}
%\end{figure}
%
%This cluster appears to correspond almost exactly to the feminine endings 
%discussed in section~\ref{sec:heb-example}---namely, the set of suffixes that 
%involve combinations of \textit{-u}, \textit{-i}, and \textit{-t}. 
%Notice that all of the endings in figure~\ref{fig:cl-fem} at least share the \textit{t}.
% 
%%[Here, I intend to consult another of Multimorph's output documents, one that lists
%%the top ten most active features for each cluster. Presumably, these lists would more clearly and
%%accurately characterize each cluster.
%%Perhaps, for example, 
%%the feature \texttt{t@[-1]} is among the most active features underlying the cluster associated with figure~\ref{fig:cl-fem}, but perhaps others 
%%contribute significantly as well.] 
%
%Another cluster sample, this one consisting of 60 words randomly selected from a 426-word cluster, is displayed 
%in figure~\ref{fig:cl-hit}. This particular cluster represents a \emph{binyan}, which, in Hebrew and other Semitic 
%languages is a class of verbs whose stems share the same vowel pattern and combine with the same  
%of affixes. Vowel patterns are the complements of consonantal roots; that is, both the root and the pattern must be present in order
%to realize a phonologically viable stem. 
%%interleaved (or interdigitated) to form a stem. 
%Nearly every verb in this cluster is of the \textit{Hitpa`el} binyan, which is distinguished by the pattern
%\begin{center}
%$\diamond$\,i\,t\,C\,a\,C\,e\,C 
%\end{center}
%where $\diamond$ is a place holder that can be filled by 
%\textit{h}, \textit{m}, \textit{t}, \textit{y}, \textit{n}, or \textit{\textipa{P}}, 
%depending on tense, person, and number. 
%Some of the words in % figure~\ref{subfig:cl-hit-sample} 
%figure~\ref{fig:cl-hit} are nouns derived from the \textit{Hitpa`el} binyan. For instance, \textit{ha-hitna\v{s}m-uy-\a'{o}t } is such a noun, where \textit{ha-} is the definite-article prefix, \textit{hitna\v{s}em}\footnote{the \textit{e} in the stem \textit{hitna\v{s}em} is deleted when the suffixes \textit{-uy-\a'{o}t} are added.}
%is the \textit{hitpa\textipa{`}el} stem, and \textit{-uy} is the nominalizing suffix \textit{-ut}, whose \textit{t} becomes become \textit{y} when it immediately precedes a stressed vowel, and \textit{-ot} is the feminine plural suffix. 
%%which bears the nominal suffix \textit{-ut} ($\to$ \textit{-uy} before the\textit{o}) and the fem.pl suffix \textit{-ot} 
%
%%words randomly selected from a 426-word cluster generated by Multimorph's MCMM 
%%at $s = 2$, and $\delta = 2$. These words are almost entirely of the \textit{Hitpa`el} binyan
%
%%\subfigure[60]{
%%\begin{tabbing}
%%\hspace*{13ex}\= \hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex} \kill
%%hamitnag\v{s}\a'{o}t \> hitgalgel\a'{a} \> hitnadn\a'{e}d \> lehistap\a'{e}r \> welehitra\textipa{P}\a'{o}t \> wemistak\a'{e}l\\
%%hithap\textsubdot{k}\a'{a} \> mitra\v{s}\a'{e}met \> titnagv\a'{i} \> titxat\a'{e}n \> wemistak\a'{e}let \> yitqalq\a'{e}l\\
%%hahitna\v{s}muy\a'{o}t \> histak\a'{a}lt \> hit\textipa{P}amc\a'{u} \> hi\v{s}tat\a'{a}ta \> lehit\textipa{Q}as\a'{e}q \> titqa\v{s}r\a'{i}\\
%%hitgalg\a'{a}lti \> hitragz\a'{u} \> hitwakx\a'{a} \> mitlah\a'{e}vet \> mitmac\a'{e}\textipa{P}t \> mitrag\a'{e}z\\
%%behitxa\v{s}\a'{e}v \> hit\textipa{Q}orer\a'{a} \> mamtaq\a'{i}m \> mitgal\a'{e}\c{c}et \> mit\textipa{P}am\a'{e}cet \> \v{s}emistarq\a'{i}m\\
%%hitnah\a'{e}g \> hitpazr\a'{u} \> hitpoc\a'{e}c \> hi\v{s}tan\a'{a} \> lehitxab\a'{e}\textipa{P} \> \v{s}ehitpoc\a'{e}c\\
%%histar\a'{a}qt \> hitpocec\a'{u} \> hitrag\a'{e}z \> hitya\v{s}\a'{e}v \> lehithap\a'{e}\textsubdot{k} \> titqalx\a'{i}\\
%%hit\textipa{Q}anyen\a'{a} \> lehit\textipa{Q}acb\a'{e}n \> mitpan\a'{e}qet \> mitqa\v{s}\a'{e}r \> nitgalg\a'{e}l \> tistarq\a'{i}\\
%%mistak\a'{e}l \> mitno\textipa{Q}\a'{e}a\textipa{Q} \> mitpa\textipa{Q}\a'{e}l \> nitlab\a'{e}\v{s} \> titgalgel\a'{i} \> \v{s}emitkad\a'{e}r\\
%%hitparq\a'{a} \> lehitqa\v{s}\a'{e}r \> mitpar\a'{e}q \> mit\textipa{Q}aq\a'{e}\v{s}et \> titxal\a'{e}q \> titya\v{s}v\a'{i} \\
%%\end{tabbing}
%%\label{subfig:cl-hit-sample}
%%}
%
%\begin{figure}[t]
%\begin{mdframed}
%\centering
%\begin{tabbing}
%\hspace*{13ex}\= \hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex}\=\hspace*{13ex} \kill
%hamitnag\v{s}\a'{o}t \> hitgalgel\a'{a} \> hitnadn\a'{e}d \> lehistap\a'{e}r \> welehitra\textipa{P}\a'{o}t \> wemistak\a'{e}l\\
%hithap\textsubdot{k}\a'{a} \> mitra\v{s}\a'{e}met \> titnagv\a'{i} \> titxat\a'{e}n \> wemistak\a'{e}let \> yitqalq\a'{e}l\\
%hahitna\v{s}muy\a'{o}t \> histak\a'{a}lt \> hit\textipa{P}amc\a'{u} \> hi\v{s}tat\a'{a}ta \> lehit\textipa{Q}as\a'{e}q \> titqa\v{s}r\a'{i}\\
%hitgalg\a'{a}lti \> hitragz\a'{u} \> hitwakx\a'{a} \> mitlah\a'{e}vet \> mitmac\a'{e}\textipa{P}t \> mitrag\a'{e}z\\
%behitxa\v{s}\a'{e}v \> hit\textipa{Q}orer\a'{a} \> mamtaq\a'{i}m \> mitgal\a'{e}\c{c}et \> mit\textipa{P}am\a'{e}cet \> \v{s}emistarq\a'{i}m\\
%hitnah\a'{e}g \> hitpazr\a'{u} \> hitpoc\a'{e}c \> hi\v{s}tan\a'{a} \> lehitxab\a'{e}\textipa{P} \> \v{s}ehitpoc\a'{e}c\\
%histar\a'{a}qt \> hitpocec\a'{u} \> hitrag\a'{e}z \> hitya\v{s}\a'{e}v \> lehithap\a'{e}\textsubdot{k} \> titqalx\a'{i}\\
%hit\textipa{Q}anyen\a'{a} \> lehit\textipa{Q}acb\a'{e}n \> mitpan\a'{e}qet \> mitqa\v{s}\a'{e}r \> nitgalg\a'{e}l \> tistarq\a'{i}\\
%mistak\a'{e}l \> mitno\textipa{Q}\a'{e}a\textipa{Q} \> mitpa\textipa{Q}\a'{e}l \> nitlab\a'{e}\v{s} \> titgalgel\a'{i} \> \v{s}emitkad\a'{e}r\\
%hitparq\a'{a} \> lehitqa\v{s}\a'{e}r \> mitpar\a'{e}q \> mit\textipa{Q}aq\a'{e}\v{s}et \> titxal\a'{e}q \> titya\v{s}v\a'{i}
%\end{tabbing}
%\label{fig:cl-hit}
%\caption{60 words randomly selected from a 426-word cluster generated by Multimorph's MCMM 
%at $s = 2$, and $\delta = 2$. These words are almost entirely of the \textit{Hitpa`el} binyan.}
%\end{mdframed}
%\end{figure}
%
%\begin{figure}[tb!]
%\begin{mdframed}
%\begin{tabbing}
%\centering
%\hspace*{17ex}\= \hspace*{17ex}\=\hspace*{17ex}\=\hspace*{17ex}\=\hspace*{17ex} \kill
%\texttt{a<e} \, (1.0000) \> \texttt{e<a} \, (0.0280) \>  \texttt{b<e} \, (0.0202) \> \texttt{y<e} \, (0.0174) \> \texttt{\v{s}<e} \, (0.0091) \\ 
%\texttt{m<e} \, (0.0080) \> \texttt{p<e} \, (0.0079) \> \texttt{d<t} \, (0.0065) \>\texttt{e<f} \, (0.0064) \> \texttt{e<h} \, (0.0055) 
%\end{tabbing}
%\label{fig:cl-hit-features}
%\caption{The ten most active features in the cluster represented figure~\ref{fig:cl-hit}. The feature (surface-unit) activities are included in parentheses.}
%\end{mdframed}
%\end{figure}
%
%This \textit{Hitpa\textipa{`}el} example demonstrates that Multimorph 
%is capable of learning nonconcatenative morphology. 
%The \textipa{/a/} and the \textipa{/e/} in CaCeC are separated by a consonant. Among the words in \ref{fig:cl-hit}, 
%the C that occurs \emph{between} the \textipa{/a/} and \textipa{/e/} varies. It follows that Multimorph is not merely recognizing 
%continuous substrings that contain both \textipa{/a/} and \textipa{/e/}.

\section{Quantitative Results}
The quantitative results stem from the dual-paradigm evaluation method outlined in chapter~\ref{ch:eval}. 
There are thus two distinct bodies of quantitative results, one from the \emph{intrinsic} component, and 
the other from the \emph{extrinsic}. We turn first to the intrinsic results.
Whereas the qualitative analysis discussed in section~\ref{sec:qual} was performed 
manually, the quantitative evaluation was performed computationally.
While human eyes can be beneficial to evaluating the results of unsupervised learning, 
a human may be less effective at judging overall consistency. 

\subsection{Intrinsic Results}
\label{sec:intr-results}
The intrinsic results are displayed in tables~\ref{tab:intr-500} and \ref{tab:intr-1000}. 
These are the ``master" tables, so to speak, for the intrinsic results, wherein each row 
represents the intrinsic evaluation of a \emph{model}. 
Each \emph{model} is distinguished by its input data type (TS, TR, or O) and its feature set, which depends 
on the values of $s$ and $\delta$.  %and the number of clusters it was permitted to accumulate, i.e., the cutoff point---either $K = 500$ or $K = 1000$.
The evaluation metrics, discussed in chapter~\ref{ch:eval}, are \textbf{average cluster-wise purity} (Purity), 
\textbf{BCubed precision} (BP), \textbf{BCubed Recall} (BR), 
and the \textbf{F1-score} (F), that is, the harmonic mean of BP and BR.  The tables also state the \emph{coverage} 
(Cov.) of each clustering, which is the number of words belonging to at least one cluster (see section~\ref{sec:intrinsic}), 
and $K^{\prime}$, 
which the set of \emph{active} clusters, that is, the clusters with at least one member. 
Each table  is associated with a $K$ cutoff value, either 500 or 1000, and each divided into three subtables, 
one for each of the three types of input data representation (DR).

\begin{table}[!ht]
\centering
\subtable[Transcriptions with Stress Marking (TS)\label{subtab:intr-TS-500}]{
%\small
\centering
\fontsize{11pt}{12pt}\selectfont
%\caption{ Transcriptions with Stress Marking (TS)}
\setlength{\extrarowheight}{3pt}
\begin{tabular}{cc|ccccrr}
\toprule
$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline%\hline
0 & 1 & 0.450 & 0.483 & 0.371 & 0.419 & 11585 & 88 \\%500 %TS %0_1_K1000_N12272_basic_181104_14-26_k-500
0 & 2 & 0.479 & 0.329 & 0.504 & 0.386 & 11984 & 104 \\
0 & 3 & 0.480 & 0.341 & 0.424 & 0.378 & 11348 & 75 \\ \hline %500 %TS %0_3_K1000_N12272_basic_181104_15-15_k-500
2 & 1 & 0.880 & 0.429 & 0.440 & 0.434 & 12103 & 346 \\%500 %TS %2_1_K6000_N12272_basic_180621_23-57_k-500
2 & 2 & 0.505 & 0.360 & 0.497 & 0.418 & 12076 & 74 \\%500 %TS %2_2_K6000_N12272_basic_180621_21-24_k-500
2 & 3 & 0.503 & 0.317 & 0.513 & 0.392 & 12084 & 96 \\ \hline %500 %TS %2_3_K1000_N12272_basic_181014_23-53_k-500
4 & 1 & 0.465 & 0.302 & 0.549 & 0.390 & 12164 & 81 \\%500 %TS %4_1_K6000_N12271_basic_180616_12-50_k-500
4 & 2 & 0.487 & 0.328 & 0.473 & 0.388 & 12214 & 153 \\%500 %TS %4_2_K6000_N12271_basic_180616_13-06_k-500
4 & 3 & 0.488 & 0.308 & 0.494 & 0.378 & 12121 & 165 \\ \hline 
6 & 1 & 0.455 & 0.323 & 0.514 & 0.397 & 12101 & 82 \\%500 %TS %6_1_K6000_N12271_basic_180616_13-06_k-500
6 & 2 & 0.494 & 0.336 & 0.510 & 0.405 & 11958 & 64 \\%500 %TS %6_2_K6000_N12272_basic_180620_02-37_k-500
6 & 3 & 0.529 & 0.345 & 0.474 & 0.399 & 11858 & 76 \\ \hline %\hline%500 %TS %6_3_K6000_N12271_basic_180621_07-59_k-500
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.518 & 0.350 & 0.480 & 0.399 & 11966 & 117 \\
 \bottomrule
\end{tabular}
}\\
%\subtable[Transcriptions, no stress marking (TR), $K=500$ \label{subtab:intr-TR-500}]{
%\tiny
%\setlength{\extrarowheight}{3pt}
%\frame{
%\frametitle{Intrinsic Evaluation, $K = 500$}
%\begin{table}
\subtable[Transcriptions, No Stress Marking (TR)\label{subtab:intr-TR-500}]{
%\footnotesize
%\small
\centering
\fontsize{11pt}{12pt}\selectfont
%\centering
\setlength{\extrarowheight}{3pt}
\begin{tabular}{cc|ccccrr}
\toprule
$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline %\hline
0 & 1 & 0.460 & 0.366 & 0.366 & 0.365 & 11994 & 108 \\
0 & 2 & 0.517 & 0.310 & 0.398 & 0.340 & 11748 & 74 \\
0 & 3 & 0.416 & 0.243 & 0.571 & 0.341 & 12079 & 110 \\ \hline %500 %TR %0_3_K6000_N12222_basic_180626_18-27_k-500
2 & 1 & 0.462 & 0.311 & 0.466 & 0.370 & 12182 & 116 \\
2 & 2 & 0.456 & 0.269 & 0.583 & 0.368 & 12187 & 111 \\%500 %TR %2_2_K6000_N12222_basic_180626_18-27_k-500
2 & 3 & 0.433 & 0.236 & 0.629 & 0.343 & 12194 & 106 \\ \hline %500 %TR %2_3_K1000_N12222_basic_181015_00-00_k-500
4 & 1 & 0.426 & 0.259 & 0.599 & 0.361 & 12198 & 134 \\  %500 %TR %4_1_K6000_N12222_basic_180619_21-27_k-500
4 & 2 & 0.441 & 0.279 & 0.573 & 0.374 & 12164 & 106 \\
4 & 3 & 0.431 & 0.272 & 0.565 & 0.367 & 12149 & 102 \\ \hline %500 %TR %4_3_K6000_N12222_basic_180621_01-45_k-500
6 & 1 & 0.429 & 0.236 & 0.639 & 0.345 & 12191 & 130 \\  %500 %TR %6_1_K6000_N12221_basic_180616_12-58_k-500
6 & 2 & 0.445 & 0.263 & 0.594 & 0.364 & 12153 & 102 \\
6 & 3 & 0.432 & 0.268 & 0.576 & 0.366 & 12078 & 66 \\ \hline %\hline %500 %TR %6_3_K6000_N12222_basic_180621_01-59_k-500
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.446 & 0.276 & 0.547 & 0.359 & 12110 & 105 \\
 \bottomrule
\end{tabular}
%\end{table} 
}\\
%\caption{
%Intrinsic evaluation results at $K = 500$. 
%In the table headers, $s$ and $\delta$ are the parameters for positional and precedence features, respectively. \textit{BP} and \textit{BR} are BCubed Precision and BCubed Recall, respectively, and \textit{Cov.} (coverage) is the number of words that are active members of at least one cluster.
%\label{tab:intr-500}
%\end{table}
%
%\clearpage
%%\subtable[Orthographic data (O), $K=500$ \label{subtab:intr-O-500}]{
%%\setlength{\extrarowheight}{6pt}
%%\frame{
%%\frametitle{Intrinsic Evaluation, $K=500$}
%%\begin{table}
%\begin{table}
%\ContinuedFloat
\subtable[Orthographic (O), $K=500$]{
\centering
\fontsize{11pt}{12pt}\selectfont
%\footnotesize
%\small
%\centering
\setlength{\extrarowheight}{3pt}
\begin{tabular}{cc|ccccrr}
\toprule
$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline%\hline
0 & 1 & 0.431 & 0.348 & 0.152 & 0.212 & 10962 & 229 \\%500 %O %0_1_K6000_N11166_basic_180727_16-17_k-500
0 & 2 & 0.422 & 0.295 & 0.247 & 0.269 & 11107 & 293 \\%500 %O %0_2_K6000_N11166_basic_180727_16-17_k-500
0 & 3 & 0.435 & 0.283 & 0.325 & 0.303 & 11051 & 247 \\ \hline %500 %O %0_3_K6000_N11166_basic_180727_16-17_k-500
1 & 1 & 0.559 & 0.447 & 0.553 & 0.494 & 4488 & 340 \\%500 %O %1_1_K6000_N11166_basic_180723_13-16_k-500
1 & 2 & 0.462 & 0.392 & 0.567 & 0.463 & 4493 & 122 \\%500 %O %1_2_K6000_N11166_basic_180723_13-15_k-500
1 & 3 & 0.488 & 0.344 & 0.583 & 0.433 & 4485 & 156 \\ \hline %500 %O %1_3_K1000_N11166_basic_181104_15-26_k-500
2 & 1 & 0.405 & 0.362 & 0.524 & 0.428 & 3447 & 66 \\%500 %O %2_1_K6000_N11166_basic_180723_13-11_k-500
2 & 2 & 0.422 & 0.329 & 0.602 & 0.426 & 3441 & 96 \\%500 %O %2_2_K6000_N11166_basic_180723_13-11_k-500
2 & 3 & 0.441 & 0.298 & 0.622 & 0.403 & 3445 & 132 \\ \hline %500 %O %2_3_K1000_N11166_basic_181015_00-18_k-500
3 & 1 & 0.371 & 0.330 & 0.575 & 0.419 & 3447 & 78 \\%500 %O %3_1_K6000_N11166_basic_180727_03-42_k-500
3 & 2 & 0.396 & 0.303 & 0.582 & 0.399 & 3444 & 113 \\%500 %O %3_2_K6000_N11166_basic_180723_13-19_k-500
3 & 3 & 0.436 & 0.280 & 0.603 & 0.382 & 3444 & 142 \\ \hline %\hline %500 %O %3_3_K6000_N11166_basic_180727_16-15_k-500
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.439 & 0.334 & 0.495 & 0.386 & 5605 & 168 \\ \bottomrule
\end{tabular}
\label{subtab:intr-O-500}
}
\caption{
Intrinsic evaluation results at $K = 500$. 
%In the table headers, $s$ and $\delta$ are the parameters for positional and precedence features, respectively. \textit{BP} and \textit{BR} are BCubed Precision and BCubed Recall, respectively, and \textit{Cov.} (coverage) is the number of words that are active members of at least one cluster.
}
\label{tab:intr-500}
\end{table}

\begin{table}[!ht]
\centering
\subtable[Transcriptions with Stress Markings (TS)\label{subtab:intr-TS-1000}]{ %(TS), $K=1000$ 
\centering
\fontsize{11pt}{12pt}\selectfont
%\label{subtab:intr-TS-1000}]{
%\small
\setlength{\extrarowheight}{3pt}
\begin{tabular}{cc|ccccrr}
\toprule
$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline%\hline
0 & 1 & 0.448 & 0.481 & 0.370 & 0.418 & 11613 & 90 \\%1000 %TS %0_1_K1000_N12272_basic_181104_14-26_k-1000
0 & 2 & 0.486 & 0.397 & 0.438 & 0.417 & 11759 & 106 \\%1000 %TS %0_2_K1000_N12272_basic_181104_14-25_k-1000
0 & 3 & 0.376 & 0.340 & 0.426 & 0.379 & 11691 & 999 \\ \hline %1000 %TS %0_3_K1000_N12272_basic_181104_15-15_k-1000
2 & 1 & 0.650 & 0.421 & 0.442 & 0.431 & 12144 & 986 \\%1000 %TS %2_1_K6000_N12272_basic_180621_23-57_k-1000
2 & 2 & 0.464 & 0.356 & 0.504 & 0.417 & 12103 & 989 \\%1000 %TS %2_2_K6000_N12272_basic_180621_21-24_k-1000
2 & 3 & 0.488 & 0.315 & 0.513 & 0.391 & 12111 & 107 \\ \hline %1000 %TS %2_3_K1000_N12272_basic_181014_23-53_k-1000
4 & 1 & 0.457 & 0.299 & 0.536 & 0.384 & 12193 & 103 \\%1000 %TS %4_1_K1000_N12272_basic_181104_06-06_k-1000
4 & 2 & 0.577 & 0.367 & 0.459 & 0.408 & 12011 & 591 \\%1000 %TS %4_2_K1000_N12272_basic_181104_04-36_k-1000
4 & 3 & 0.473 & 0.273 & 0.519 & 0.358 & 12242 & 297 \\ \hline %1000 %TS %4_3_K1000_N12272_basic_181015_00-21_k-1000
6 & 1 & 0.648 & 0.295 & 0.552 & 0.385 & 12139 & 995 \\%1000 %TS %6_1_K6000_N12271_basic_180616_13-06_k-1000
6 & 2 & 0.655 & 0.333 & 0.499 & 0.400 & 11908 & 1000 \\%1000 %TS %6_2_K6000_N12272_basic_180620_02-37_k-1000
6 & 3 & 0.703 & 0.333 & 0.497 & 0.399 & 11877 & 999 \\ \hline %\hline%1000 %TS %6_3_K1000_N12272_basic_181104_07-21_k-1000
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.535 & 0.351 & 0.480 & 0.399 & 11983 & 605 \\
 \bottomrule
\end{tabular}
}\\
%\end{table}
%%\frame {
%\frametitle{Intrinsic Evaluation, $K=1000$}
%\begin{table}
\subtable[Transcriptions, No Stress Marking (TR)\label{subtab:intr-TR-1000}]{
%\small
\centering
\fontsize{11pt}{12pt}\selectfont
\setlength{\extrarowheight}{3pt}
\begin{tabular}{cc|ccccrr}
\toprule
$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline%\hline
0 & 1 & 0.553 & 0.356 & 0.346 & 0.350 & 11936 & 427 \\
0 & 2 & 0.615 & 0.302 & 0.516 & 0.381 & 12041 & 994 \\%1000 %TR %0_2_K6000_N12222_basic_180623_01-57_k-1000
0 & 3 & 0.547 & 0.230 & 0.593 & 0.331 & 12168 & 860 \\ \hline %1000 %TR %0_3_K6000_N12222_basic_180626_18-27_k-1000
2 & 1 & 0.786 & 0.326 & 0.522 & 0.401 & 12205 & 966 \\%1000 %TR %2_1_K6000_N12222_basic_180628_05-27_k-1000
2 & 2 & 0.445 & 0.264 & 0.588 & 0.365 & 12202 & 133 \\%1000 %TR %2_2_K6000_N12222_basic_180626_18-27_k-1000
2 & 3 & 0.551 & 0.252 & 0.323 & 0.283 & 12111 & 75 \\ \hline %1000 %TR %2_3_K1000_N12222_basic_181014_23-53_k-1000
4 & 1 & 0.812 & 0.257 & 0.601 & 0.360 & 12203 & 432 \\%1000 %TR %4_1_K6000_N12222_basic_180619_21-27_k-1000
4 & 2 & 0.436 & 0.258 & 0.612 & 0.363 & 12177 & 109 \\%1000 %TR %4_2_K6000_N12222_basic_180626_18-27_k-1000
4 & 3 & 0.426 & 0.235 & 0.637 & 0.344 & 12136 & 113 \\ \hline %1000 %TR %4_3_K6000_N12222_basic_180621_01-45_k-1000
6 & 1 & 0.705 & 0.234 & 0.643 & 0.343 & 12189 & 886 \\%1000 %TR %6_1_K6000_N12221_basic_180616_12-58_k-1000
6 & 2 & 0.431 & 0.253 & 0.597 & 0.356 & 12152 & 114 \\%1000 %TR %6_2_K6000_N12222_basic_180626_17-48_k-1000
6 & 3 & 0.446 & 0.271 & 0.569 & 0.367 & 12082 & 72 \\ \hline %\hline%1000 %TR %6_3_K6000_N12222_basic_180621_01-59_k-1000
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.563 & 0.270 & 0.546 & 0.354 & 12133 & 432 \\
 \bottomrule
\end{tabular}
}\\
\subtable[Orthographic (O)]{
\centering
\small
\setlength{\extrarowheight}{3pt}
\begin{tabular}{cc|ccccrr}
\toprule
$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline%\hline
0 & 1 & 0.565 & 0.348 & 0.152 & 0.212 & 11000 & 808 \\%1000 %O %0_1_K6000_N11166_basic_180727_16-17_k-1000
0 & 2 & 0.427 & 0.295 & 0.248 & 0.269 & 11113 & 303 \\%1000 %O %0_2_K6000_N11166_basic_180727_16-17_k-1000
0 & 3 & 0.439 & 0.280 & 0.326 & 0.302 & 11098 & 1000 \\ \hline %1000 %O %0_3_K6000_N11166_basic_180727_16-17_k-1000
1 & 1 & 0.401 & 0.440 & 0.557 & 0.492 & 4490 & 1000 \\%1000 %O %1_1_K6000_N11166_basic_180727_03-24_k-1000
1 & 2 & 0.385 & 0.384 & 0.571 & 0.459 & 4493 & 1000 \\%1000 %O %1_2_K6000_N11166_basic_180723_13-15_k-1000
1 & 3 & 0.442 & 0.338 & 0.590 & 0.430 & 4489 & 385 \\ \hline %1000 %O %1_3_K1000_N11166_basic_181104_15-26_k-1000
2 & 1 & 0.402 & 0.355 & 0.534 & 0.426 & 3447 & 1000 \\%1000 %O %2_1_K6000_N11166_basic_180727_03-16_k-1000
2 & 2 & 0.596 & 0.328 & 0.602 & 0.424 & 3444 & 942 \\%1000 %O %2_2_K6000_N11166_basic_180723_13-11_k-1000
2 & 3 & 0.443 & 0.295 & 0.623 & 0.400 & 3446 & 137 \\ \hline %1000 %O %2_3_K1000_N11166_basic_181015_00-18_k-1000
3 & 1 & 0.361 & 0.325 & 0.596 & 0.421 & 3446 & 385 \\
3 & 2 & 0.389 & 0.317 & 0.617 & 0.418 & 3444 & 101 \\%1000 %O %3_2_K6000_N11166_basic_180727_16-16_k-1000
3 & 3 & 0.418 & 0.271 & 0.609 & 0.375 & 3443 & 155 \\ \hline %\hline%1000 %O %3_3_K6000_N11166_basic_180727_16-15_k-1000
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.439 & 0.331 & 0.502 & 0.386 & 5613 & 601 \\
 \bottomrule
\end{tabular}
\label{subtab:intr-O-1000}
}
%\end{table}
\caption{Intrinsic evaluation results at $K = 1000$. 
%The headers $s$ and $\delta$ are the parameters for positional and precedence features, respectively. \textit{BP} and \textit{BR} stand for BCubed Precision and Recall, and \textit{Cov.} (coverage) is the number of words that are active members of at least one cluster.
}
\label{tab:intr-1000}
\end{table}

One of the most important observations regarding tables~\ref{tab:intr-500} and \ref{tab:intr-1000}
 is that the models trained on O, the orthographic dataset, performed just as well as as those trained on TS, the transcriptional dataset
  with stress marking.
  %, if not \emph{slightly better}. 
  The average F-score for all O 
 models at $K=1000$ (that is, the average F-score of \emph{all} valuations of $s$
   and $\delta$) was 0.386, whereas the corresponding averages for TS and TR at $K=1000$ were 0.399 and 0.354, respectively.
 % (Note that this is the average F-score computed over \emph{all} valuations of $s$
   %and $\delta$, i.e., all rows in table~\ref{subtab:intr-O-1000}.) 
%   The average F-score of the TS models %(the transcriptions with stress markings) 
%   at $K=1000$ was 0.365. 
%   These were substantially higher that average F-score of the TR models---the models trained on transcriptions without stress marking---at $K=1000$, which was 0.308.
  Two important points emerge from these observations: 
  
\emph{First, the orthographic models performed surprisingly well.} 
Their F-scores were as good those of the TS models and substantially better than those of the
  TR models. It must be said that orthographic coverages were generally much lower than the transcriptional coverages, especially where the orthographic F-scores were the highest.
But even so, this is still a surprising result because, as discussed in chapter~\ref{ch:experi}, 
 the orthography of Hebrew is basically consonantal; that is, the alphabet lacks vowel symbols. 
It would not be unreasonable to expect the orthographic dataset to be the least efficacious of the three because it
lacks the information contributed by vowels, which would seem to be important information, as vowel patterns seem to be crucial to Hebrew's 
root-and-pattern morphology. However, the intrinsic results in both tables~\ref{tab:intr-500} and \ref{tab:intr-1000} 
show no real difference in performance between the O and TS models.
%(Their results do differ qualitatively, however, as discussed above in section~\ref{sec:qual}.) 
This suggests that vowel symbols \emph{per se} are not necessarily 
helpful to the task of inducing a model of Hebrew morphology. Indeed, 
they appear to have been more harmful than helpful in this study,
particularly where finding roots was concerned. As discussed in section~\ref{sec:qual}, the presence of vowels in precedence features made these features less generalizable. 
%They seem to have added considerable noise and complexity to the task of finding consonantal 
%roots, 
%It should be 
%noted, however, that there are many possible ways to encode vowels in features. Perhaps there is a way to
%encode vowels that is flexible---a feature encoding that can be g more general in some cases and more specific in others. 
%way to encode vowels to better effect, a way perhaps that makes 
%there could be feature-set formats that problems encountered in the present study
%to better effect than the present study's.

\emph{Second, the TS models outperformed the TR models by a substantial margin.} From this observation, we can infer that
the transcriptions with stress marking  engendered better features than the
transcriptions that lacked stress information. We can further infer that the distinction between \emph{stressed syllable} and \emph{unstressed syllable} is salient to the task of inducing Hebrew morphological structure. 
%vowels whose stress is not specified. 
It is important to note, however, that the 
alphabets used in this study consisted solely of atomic unicode characters. 
That is, the stressed \textsf{\'a}, 
for example, was not encoded as \textsf{a} (\texttt{U+0061}) followed 
by the acute-accent combining character \texttt{U+0301}, but 
rather as the single \emph{precomposed} character{\texttt{U+00E1}. 
The stressed (or accented) vowels \texttt{\'e}, \textsf{\'o}, and \textsf{\'u} were similarly each represented as a single, precomposed unicode character. 
Such atomic symbols are incapable of expressing inter-symbol relationships. 
For example, \textsf{\'a} (\texttt{U+00E1}) and \textsf{a} (\texttt{U+0061}) 
are completely distinct, as unrelated as the characters \textit{a} and \textit{t} as far as the unicode is concerned.
There is likewise
no relationship between the characters \texttt{U+00E1} (\textsf{\'a}) and  
\texttt{U+00E9} (\textsf{\'e}) or any other combination of stressed-vowel 
precomposed characters. There is nothing in these characters that represents 
categorical traits such as ``stressed'' or ``unstressed'' or ``vowel.''  

In effect, therefore, to include stress marking was to increase the size of the 
alphabet from 34 to 39 atomic symbols, which
in turn resulted in a considerable increase in the number of features, as the number features depended 
directly on the size of the alphabet, as discussed in chapter~\ref{ch:experi}. 
It seems noteworthy 
that this particular increase apparently did not have a detrimental effect, %result in an
%information overload---that is, that it did not, it would seem, 
%produce more noise than useful information, 
as the models trained on stress-marked transcriptions
performed better than those trained on stressless transcriptions.  

We also must note the precipitous drop in coverage in the results of the orthographic models. In table~\ref{subtab:intr-O-500}, the coverage drops from 11051 to 4488 precisely at the transition between $s=0$ and $s=1$. It thus coincides exactly with the introduction of positional features. Table~\ref{subtab:intr-O-1000} shows essentially the same drop and at precisely the same $s$ value (i.e., affix length).  In both of these tables, the coverage decreases even further as $s$ increases to $s=3$.  %low at $s=2$ and $s=3$, even decreasing further, by another 1000 words, in fact. 
We do not see this decrease in the transcriptional models. %Apparently the introduction of positional features  
At the same time, $K^{\prime}$, the number of clusters with at least one active member, is maximized in the orthographic models at $s = 1$, precisely when the coverage is lowest, indicating a greater degree overlap among the clusters. 
%Perhaps the positional features encourage the MCMM to pay more attention to words containing positional regularities, as in cases of frequent affixes.  %highly frequent suffix or beginning with a highly frequent prefix. %($K=500$)

Perhaps the constriction in word coverage was a consequence of heightened specificity in the feature sets---new specificity introduced by positional features.
Positional features, each specifying both a character \emph{and} a position, are intrinsically more specific than precedence features. A precedence feature merely specifies a pair of characters, not the absolute positions of these characters within the word. The distance between the characters is not even specified for most values of $\delta$ (see section~\ref{sec:features}).
% be range of values, with the size of this range and thus the amount of vagueness varying in direct proportion to $\delta$. The size of the span is fully specified only when $\delta=1$, in which case there is no span, i.e. the two characters must be adjacent (see section~\ref{}).
%  even specific they can allow a range of different span lengthsPrecedence as 
%Perhaps this constriction is a consequence of the greater degree of specificity in positional features as compared to precedence features.

Because of their specificity, positional features perhaps made it generally harder for a word to gain membership in a cluster, hence the abrupt decrease in coverage when positional features were introduced. The reason we see this dramatic decrease only in the orthographic data could be due to the sheer number of precedence features in the transcriptional feature sets, especially the TS feature sets. 
%The transcriptional feature sets contained many more precedence features than the orthographic feature set. 
The number of precedence features was a function of the size of the alphabet from which they are drawn. 
%
In particular, it was the size of the alphabet \emph{squared}. For example, the size of the orthographic alphabet was 22; thus, the number of orthographic precedence features was $22^2 = 484$. By contrast, there were 39 characters in the TS alphabet, which means that there were $39^2 = 1521$ in the TS data.\footnote{Features that never occurred in the dataset in question dataset were deleted, so the actual numbers were somewhat lower than these.}
%%The size of the precedence feature set thus increases \emph{exponentially} with the size of the alphabet. By contrast, 

The number of positional features was also a function of the size of the alphabet, but a \emph{linear} one, namely 
\begin{equation*}
\text{size(}{PositionalFeatures}\text{)} = s \times 2 \times \text{size(}{Alphabet}\text{)}
\end{equation*}
where $s$ was the affix-length parameter, i.e., the number of positions considered at \emph{one} of the two ends, and $s \times 2$ was the total number positions (taking into account both ends; see section~\ref{sec:features}). The size of the positional feature set thus also increased with the size of the alphabet, but at a much slower rate than the size of the precedence feature sets.

Moreover, in every feature set, the precedence features substantially outnumbered the positional, but this size disparity was greater for larger alphabets.
For example, in the TS feature sets (whose alphabet size was 39), at $s =2$, the ratio of the number of precedence features to the number of positional features was 9.7 to one. By contrast, in the orthographic feature sets (alphabet size 22), this ratio was considerably smaller, namely 5.5 to one.
Perhaps the precedence features were numerous enough in the transcriptional feature sets (especially TS) to drown out the influence of the relatively scarce positional features in these feature sets. This would explain the lack of a coverage drop in the transcriptional results.

%The highest F-scores also tended to occur when $s = 1$, but this was true not only of the orthographic models, but of the transcriptional models, as well.
The highest F-scores tended to occur at $s=1$ for the O models and $s=2$ for the TR and TS models. In other words, the highest F-scores tended to occur at the lowest non-zero $s$ value for each of the three data types, suggesting that the benefit of positional features is maximized at the edges of words (see section~\ref{sec:concl-features} in the next chapter).
%At the edges of words, the increased specificity seems to have a positive effect, since $s=1$ places all positional features exactly at the edges of words.

%By contrast, the number positional features is
%\begin{equation*}
%s \times 2 \times \text{size(}{Alphabet}\text{)}
%\end{equation*}
%as explained in section~\ref{sec:features}
%Thus, if $s=2$, and the alphabet is the transcriptional alphabet with stress marking, we have
%\begin{equation*}
%s \times 2 \times 22 =  88 \text{orthographic positional features}
%\end{equation*}
%as explained in section~\ref{sec:features}
%And if alphabet is the transcriptional one with stress marking, we have
%\begin{equation*}
%s \times 2 \times 39 = 156 \text{transcriptional positional features}
%\end{equation*}

%The size of the positional feature set increases linearly with the size of the alphabet, while that of the precedence increases exponentially 
%with the size of the alphabet. 
%than precedence features.

%Such an effect would make sense, as concatenative affixes do play a prominent role in Hebrew's morphology, 
%Perhaps the constriction in the number words covered in the orthographic clusters at $s=1$

%was the result of the interacting factors:
%Regularities become more specific, and thus clusters would become more specific, more definite and distinguished if the system is given small number of highly useful features (i.e., positional features corresponding to the extreme edges of words.
%Precedence features have a built-in degree of vagueness.
%Because of their specificity, positional features might make it harder for a word to gain membership in a cluster. At the edges of words, this increased specificity seems to have had a positive effect.
% F-scores tended to be lower where $s = 2$ and $s=3$. Moreover, the coverage in the orthographic clusters decreased even further at $s$
%values greater than 1. 
%noncatenative regularities would be less frequent in orthographically-represnted, as would concatenative regularities, in fact, since the introduction of vowels increases the number of detectible categories. 
%However, %the  tables~\ref{tab:intr-500} and \ref{tab:intr-100}, 
%it appears positional are only useful at the edges of words (see section \ref{sec:concl-features} in the next chapter).

%suggest that this ostensible positional focusing effect has a positive effect, as long as it is not overdone: 
%and table~\ref{tab:intr-1000} ($K=1000$). At $K=500$ decreases decrease of around 1200 to around 400 in both tables occurs precisely at the transition from $s=0$ to $s=1$. The coverage $K=500$ and $K=1000$.
%This drop from occurs precisely at the transition from $s=0$ to $s=1$. 
%It also seems remarkable that the coverage (see section~\ref{sec:intrinsic}) of the orthographic models drops dramatically precisely at the transition from $s=0$ to $s=1$.
%The coverage column itself shows We introduced the concept of \emph{coverage} in the preceding chapter (see section~\ref{sec:intrinsic}). 
 
%precedence for every one positional feature.
%In the orthographic results in both tables~\ref{tab:tab:intr-500-1000}
%\subsubsection{Regarding the Purity Metric}
%As mentioned in chapter~\ref{ch:eval}, the purity metric is not designed to evaluate overlapping clusters.
%Thus, the accuracy of this metric is inversely proportional to the degree to which clusters overlap. If there is no overlap between
%%The ideal situation where purity is concerned is one such that if two words are different in any way, even if they differ in one feature, they will share no gold-std classes.
%Cluster 2;  size: 2040;  Purity: 0.2632;   pre:we (537), pre:ha (441), pre:mi (405)
%Cluster 0011;  size: 1105;  Purity: 0.2262;   piel\%prefix_stem (250), part\%m\%sg (205), ptn:piel (175)
%Cluster 0043;  size:  191;  Purity: 0.6649;   past%1%Pl (127), qal%suffix_stem (81), pre:sh (34)
%
%\begin{table}[t]
%\centering
%\setlength{\extrarowheight}{10pt}
%\begin{tabular}{cccc}
%\toprule
%$k$ & Size & Purity & Most Frequent Gold-Std Classes (with frequencies) \\ 
%\midrule
%12 & 632 & 0.5665 & \texttt{qal\%participle} (358), \, \texttt{f\%sg} (165), \, \texttt{part\%m\%sg} (144) \\\hline
%13 & 1178 & 0.3744 & \texttt{qal\%suffix\_stem} (441),  \, \texttt{f\%sg} (257), \, \texttt{pre:ha} (208) \\\hline
%20 & 536 & 0.3881 & \texttt{fut\%1\%sg }(208), \, \texttt{qal\%prefix\_stem} (86), \, \texttt{m\%sg} (78) \\\hline
%24 & 188 & 0.6755 & \texttt{past\%1\%pl} (127), \, \texttt{qal\%suffix\_stem} (76), \, \texttt{pre:sh} (32) \\\hline
%25 & 492 & 0.6301 & \makecell{\texttt{fut\%(2\%M)|(23\%f)} (310), \, \texttt{fut\%2\%f\%sg} (288), \\ \texttt{qal\%prefix\_stem} (176)} \\
%\bottomrule
%\end{tabular}
%\label{tab:purity-pitfall}
%\caption{Selected cluster purities from a clustering generated at $\langle{s} = 2,\delta = 2\rangle$ from the TS dataset.}
%\end{table}
%On the other hand, the presence of all of the transcriptional vowels in transcriptional was not always helpful. The 
%transcriptional vowels seems to have added considerable noise and complexity to the task of finding consonantal 
%roots, as explained in section \ref{sec:qual}. 
%to models that were substantially 
%better than the models trained on the stressless transcriptions. 
%It thus seems to have counteracted the decrease in performance that apparently 
%resulted from the inclusion of non-stressed vowels in the stressless transcriptional data. 


%\begin{table}
%\small
%\centering
%\subtable[Transcriptions with stress (TS), $K=500$ \label{subtab:intr-TS-500}]{
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccccrr}
%$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline\hline
%0 & 1 & 0.450 & 0.483 & 0.371 & 0.419 & 11585 & 88 \\%500 %TS %0_1_K1000_N12272_basic_181104_14-26_k-500
%0 & 2 & 0.479 & 0.329 & 0.504 & 0.386 & 11984 & 104 \\
%0 & 3 & 0.480 & 0.341 & 0.424 & 0.378 & 11348 & 75 \\ \hline %500 %TS %0_3_K1000_N12272_basic_181104_15-15_k-500
%2 & 1 & 0.880 & 0.429 & 0.440 & 0.434 & 12103 & 346 \\%500 %TS %2_1_K6000_N12272_basic_180621_23-57_k-500
%2 & 2 & 0.505 & 0.360 & 0.497 & 0.418 & 12076 & 74 \\%500 %TS %2_2_K6000_N12272_basic_180621_21-24_k-500
%2 & 3 & 0.503 & 0.317 & 0.513 & 0.392 & 12084 & 96 \\ \hline %500 %TS %2_3_K1000_N12272_basic_181014_23-53_k-500
%4 & 1 & 0.465 & 0.302 & 0.549 & 0.390 & 12164 & 81 \\%500 %TS %4_1_K6000_N12271_basic_180616_12-50_k-500
%4 & 2 & 0.487 & 0.328 & 0.473 & 0.388 & 12214 & 153 \\%500 %TS %4_2_K6000_N12271_basic_180616_13-06_k-500
%4 & 3 & 0.488 & 0.308 & 0.494 & 0.378 & 12121 & 165 \\ \hline 
%6 & 1 & 0.455 & 0.323 & 0.514 & 0.397 & 12101 & 82 \\%500 %TS %6_1_K6000_N12271_basic_180616_13-06_k-500
%6 & 2 & 0.494 & 0.336 & 0.510 & 0.405 & 11958 & 64 \\%500 %TS %6_2_K6000_N12272_basic_180620_02-37_k-500
%6 & 3 & 0.529 & 0.345 & 0.474 & 0.399 & 11858 & 76 \\ \hline \hline%500 %TS %6_3_K6000_N12271_basic_180621_07-59_k-500
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.518 & 0.350 & 0.480 & 0.399 & 11966 & 117 \\
%\end{tabular}
%}
%\subtable[Transcriptions, no stress marking (TR), $K=500$ \label{subtab:intr-TR-500}]{
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccccrr}
%$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline\hline
%0 & 1 & 0.460 & 0.366 & 0.366 & 0.365 & 11994 & 108 \\
%0 & 2 & 0.517 & 0.310 & 0.398 & 0.340 & 11748 & 74 \\
%0 & 3 & 0.416 & 0.243 & 0.571 & 0.341 & 12079 & 110 \\ \hline %500 %TR %0_3_K6000_N12222_basic_180626_18-27_k-500
%2 & 1 & 0.462 & 0.311 & 0.466 & 0.370 & 12182 & 116 \\
%2 & 2 & 0.456 & 0.269 & 0.583 & 0.368 & 12187 & 111 \\%500 %TR %2_2_K6000_N12222_basic_180626_18-27_k-500
%2 & 3 & 0.433 & 0.236 & 0.629 & 0.343 & 12194 & 106 \\ \hline %500 %TR %2_3_K1000_N12222_basic_181015_00-00_k-500
%4 & 1 & 0.426 & 0.259 & 0.599 & 0.361 & 12198 & 134 \\  %500 %TR %4_1_K6000_N12222_basic_180619_21-27_k-500
%4 & 2 & 0.441 & 0.279 & 0.573 & 0.374 & 12164 & 106 \\
%4 & 3 & 0.431 & 0.272 & 0.565 & 0.367 & 12149 & 102 \\ \hline %500 %TR %4_3_K6000_N12222_basic_180621_01-45_k-500
%6 & 1 & 0.429 & 0.236 & 0.639 & 0.345 & 12191 & 130 \\  %500 %TR %6_1_K6000_N12221_basic_180616_12-58_k-500
%6 & 2 & 0.445 & 0.263 & 0.594 & 0.364 & 12153 & 102 \\
%6 & 3 & 0.432 & 0.268 & 0.576 & 0.366 & 12078 & 66 \\ \hline \hline %500 %TR %6_3_K6000_N12222_basic_180621_01-59_k-500
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.446 & 0.276 & 0.547 & 0.359 & 12110 & 105 \\
%\end{tabular}
%}
%\subtable[Orthographic data (O), $K=500$ \label{subtab:intr-O-500}]{
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccccrr}
%$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline\hline
%0 & 1 & 0.431 & 0.348 & 0.152 & 0.212 & 10962 & 229 \\%500 %O %0_1_K6000_N11166_basic_180727_16-17_k-500
%0 & 2 & 0.422 & 0.295 & 0.247 & 0.269 & 11107 & 293 \\%500 %O %0_2_K6000_N11166_basic_180727_16-17_k-500
%0 & 3 & 0.435 & 0.283 & 0.325 & 0.303 & 11051 & 247 \\ \hline %500 %O %0_3_K6000_N11166_basic_180727_16-17_k-500
%1 & 1 & 0.559 & 0.447 & 0.553 & 0.494 & 4488 & 340 \\%500 %O %1_1_K6000_N11166_basic_180723_13-16_k-500
%1 & 2 & 0.462 & 0.392 & 0.567 & 0.463 & 4493 & 122 \\%500 %O %1_2_K6000_N11166_basic_180723_13-15_k-500
%1 & 3 & 0.488 & 0.344 & 0.583 & 0.433 & 4485 & 156 \\ \hline %500 %O %1_3_K1000_N11166_basic_181104_15-26_k-500
%2 & 1 & 0.405 & 0.362 & 0.524 & 0.428 & 3447 & 66 \\%500 %O %2_1_K6000_N11166_basic_180723_13-11_k-500
%2 & 2 & 0.422 & 0.329 & 0.602 & 0.426 & 3441 & 96 \\%500 %O %2_2_K6000_N11166_basic_180723_13-11_k-500
%2 & 3 & 0.441 & 0.298 & 0.622 & 0.403 & 3445 & 132 \\ \hline %500 %O %2_3_K1000_N11166_basic_181015_00-18_k-500
%3 & 1 & 0.371 & 0.330 & 0.575 & 0.419 & 3447 & 78 \\%500 %O %3_1_K6000_N11166_basic_180727_03-42_k-500
%3 & 2 & 0.396 & 0.303 & 0.582 & 0.399 & 3444 & 113 \\%500 %O %3_2_K6000_N11166_basic_180723_13-19_k-500
%3 & 3 & 0.436 & 0.280 & 0.603 & 0.382 & 3444 & 142 \\ \hline \hline %500 %O %3_3_K6000_N11166_basic_180727_16-15_k-500
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.439 & 0.334 & 0.495 & 0.386 & 5605 & 168 \\
%\end{tabular}
%}
%\caption{Intrinsic evaluation results at $K = 500$. In the table headers, $s$ and $\delta$ are the parameters for positional and precedence features, respectively. \textit{BP} and \textit{BR} stand for BCubed Precision and BCubed Recall, respectively, and \textit{Cov} is the \textit{coverage}, i.e., the number of words that are active members of at least one cluster.}
%\label{tab:intr-500}
%\end{table}
%
%
%\begin{table}
%\small
%\centering
%\subtable[Transcriptions with stress (TS), $K=1000$ \label{subtab:intr-TS-1000}]{
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccccrr}
%$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline\hline
%0 & 1 & 0.448 & 0.481 & 0.370 & 0.418 & 11613 & 90 \\%1000 %TS %0_1_K1000_N12272_basic_181104_14-26_k-1000
%0 & 2 & 0.486 & 0.397 & 0.438 & 0.417 & 11759 & 106 \\%1000 %TS %0_2_K1000_N12272_basic_181104_14-25_k-1000
%0 & 3 & 0.376 & 0.340 & 0.426 & 0.379 & 11691 & 999 \\ \hline %1000 %TS %0_3_K1000_N12272_basic_181104_15-15_k-1000
%2 & 1 & 0.650 & 0.421 & 0.442 & 0.431 & 12144 & 986 \\%1000 %TS %2_1_K6000_N12272_basic_180621_23-57_k-1000
%2 & 2 & 0.464 & 0.356 & 0.504 & 0.417 & 12103 & 989 \\%1000 %TS %2_2_K6000_N12272_basic_180621_21-24_k-1000
%2 & 3 & 0.488 & 0.315 & 0.513 & 0.391 & 12111 & 107 \\ \hline %1000 %TS %2_3_K1000_N12272_basic_181014_23-53_k-1000
%4 & 1 & 0.457 & 0.299 & 0.536 & 0.384 & 12193 & 103 \\%1000 %TS %4_1_K1000_N12272_basic_181104_06-06_k-1000
%4 & 2 & 0.577 & 0.367 & 0.459 & 0.408 & 12011 & 591 \\%1000 %TS %4_2_K1000_N12272_basic_181104_04-36_k-1000
%4 & 3 & 0.473 & 0.273 & 0.519 & 0.358 & 12242 & 297 \\ \hline %1000 %TS %4_3_K1000_N12272_basic_181015_00-21_k-1000
%6 & 1 & 0.648 & 0.295 & 0.552 & 0.385 & 12139 & 995 \\%1000 %TS %6_1_K6000_N12271_basic_180616_13-06_k-1000
%6 & 2 & 0.655 & 0.333 & 0.499 & 0.400 & 11908 & 1000 \\%1000 %TS %6_2_K6000_N12272_basic_180620_02-37_k-1000
%6 & 3 & 0.703 & 0.333 & 0.497 & 0.399 & 11877 & 999 \\ \hline \hline%1000 %TS %6_3_K1000_N12272_basic_181104_07-21_k-1000
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.535 & 0.351 & 0.480 & 0.399 & 11983 & 605 \\
%\end{tabular}
%}
%\subtable[Transcriptions, no stress marking (TR), $K=1000$ \label{subtab:intr-TR-1000}]{
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccccrr}
%$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline\hline
%0 & 1 & 0.553 & 0.356 & 0.346 & 0.350 & 11936 & 427 \\
%0 & 2 & 0.615 & 0.302 & 0.516 & 0.381 & 12041 & 994 \\%1000 %TR %0_2_K6000_N12222_basic_180623_01-57_k-1000
%0 & 3 & 0.547 & 0.230 & 0.593 & 0.331 & 12168 & 860 \\ \hline %1000 %TR %0_3_K6000_N12222_basic_180626_18-27_k-1000
%2 & 1 & 0.786 & 0.326 & 0.522 & 0.401 & 12205 & 966 \\%1000 %TR %2_1_K6000_N12222_basic_180628_05-27_k-1000
%2 & 2 & 0.445 & 0.264 & 0.588 & 0.365 & 12202 & 133 \\%1000 %TR %2_2_K6000_N12222_basic_180626_18-27_k-1000
%2 & 3 & 0.551 & 0.252 & 0.323 & 0.283 & 12111 & 75 \\ \hline %1000 %TR %2_3_K1000_N12222_basic_181014_23-53_k-1000
%4 & 1 & 0.812 & 0.257 & 0.601 & 0.360 & 12203 & 432 \\%1000 %TR %4_1_K6000_N12222_basic_180619_21-27_k-1000
%4 & 2 & 0.436 & 0.258 & 0.612 & 0.363 & 12177 & 109 \\%1000 %TR %4_2_K6000_N12222_basic_180626_18-27_k-1000
%4 & 3 & 0.426 & 0.235 & 0.637 & 0.344 & 12136 & 113 \\ \hline %1000 %TR %4_3_K6000_N12222_basic_180621_01-45_k-1000
%6 & 1 & 0.705 & 0.234 & 0.643 & 0.343 & 12189 & 886 \\%1000 %TR %6_1_K6000_N12221_basic_180616_12-58_k-1000
%6 & 2 & 0.431 & 0.253 & 0.597 & 0.356 & 12152 & 114 \\%1000 %TR %6_2_K6000_N12222_basic_180626_17-48_k-1000
%6 & 3 & 0.446 & 0.271 & 0.569 & 0.367 & 12082 & 72 \\ \hline \hline%1000 %TR %6_3_K6000_N12222_basic_180621_01-59_k-1000
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.563 & 0.270 & 0.546 & 0.354 & 12133 & 432 \\
%\end{tabular}
%}
%
%\subtable[Orthographic data (O), $K=1000$ \label{subtab:intr-O-1000}]{
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccccrr}
%$s$ & $\delta$ & Purity &  BP & BR & F & Cov. & $K^{\prime}$ \\ \hline\hline
%0 & 1 & 0.565 & 0.348 & 0.152 & 0.212 & 11000 & 808 \\%1000 %O %0_1_K6000_N11166_basic_180727_16-17_k-1000
%0 & 2 & 0.427 & 0.295 & 0.248 & 0.269 & 11113 & 303 \\%1000 %O %0_2_K6000_N11166_basic_180727_16-17_k-1000
%0 & 3 & 0.439 & 0.280 & 0.326 & 0.302 & 11098 & 1000 \\ \hline %1000 %O %0_3_K6000_N11166_basic_180727_16-17_k-1000
%1 & 1 & 0.401 & 0.440 & 0.557 & 0.492 & 4490 & 1000 \\%1000 %O %1_1_K6000_N11166_basic_180727_03-24_k-1000
%1 & 2 & 0.385 & 0.384 & 0.571 & 0.459 & 4493 & 1000 \\%1000 %O %1_2_K6000_N11166_basic_180723_13-15_k-1000
%1 & 3 & 0.442 & 0.338 & 0.590 & 0.430 & 4489 & 385 \\ \hline %1000 %O %1_3_K1000_N11166_basic_181104_15-26_k-1000
%2 & 1 & 0.402 & 0.355 & 0.534 & 0.426 & 3447 & 1000 \\%1000 %O %2_1_K6000_N11166_basic_180727_03-16_k-1000
%2 & 2 & 0.596 & 0.328 & 0.602 & 0.424 & 3444 & 942 \\%1000 %O %2_2_K6000_N11166_basic_180723_13-11_k-1000
%2 & 3 & 0.443 & 0.295 & 0.623 & 0.400 & 3446 & 137 \\ \hline %1000 %O %2_3_K1000_N11166_basic_181015_00-18_k-1000
%3 & 1 & 0.361 & 0.325 & 0.596 & 0.421 & 3446 & 385 \\
%3 & 2 & 0.389 & 0.317 & 0.617 & 0.418 & 3444 & 101 \\%1000 %O %3_2_K6000_N11166_basic_180727_16-16_k-1000
%3 & 3 & 0.418 & 0.271 & 0.609 & 0.375 & 3443 & 155 \\ \hline \hline%1000 %O %3_3_K6000_N11166_basic_180727_16-15_k-1000
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.439 & 0.331 & 0.502 & 0.386 & 5613 & 601 \\
%\end{tabular}
%}
%\caption{Intrinsic evaluation results at $K = 1000$. In the table headers, $s$ and $\delta$ are the parameters for positional and precedence features, respectively. \textit{BP} and \textit{BR} stand for BCubed Precision and BCubed Recall, respectively, and \textit{Cov} is the \textit{coverage}, i.e., the number of words that are active members of at least one cluster.}
%\label{tab:intr-1000}
%\end{table}
%

 \begin{table}
%\scriptsize
\centering
%\setlength{\extrarowheight}{3pt}
%\caption{ Transcriptions with Stress Marking (TS)}
\subtable[Transcriptions with stress (TS), $K = 1000$\label{subtab:extr-1000-ts}]{
\setlength{\extrarowheight}{2pt}
\fontsize{11pt}{12pt}\selectfont
%\footnotesize
%\small
%\centering
\begin{tabular}{cc|ccc|ccc}
\toprule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{Four-stage process} & \multicolumn{3}{c}{Control} \\
$s$ & $\delta$ & P & R & F & P & R & F \\ \hline %\hline
0 & 1 & 0.40 & 0.37 & 0.39 & 0.78 & 0.60 & 0.68 \\
0 & 2 & 0.37 & 0.35 & 0.36 & 0.77 & 0.60 & 0.68 \\
0 & 3 & 0.50 & 0.39 & 0.44 & 0.77 & 0.60 & 0.67 \\\hline
2 & 1 & 0.50 & 0.48 & 0.49 & 0.77 & 0.60 & 0.67 \\
2 & 2 & 0.40 & 0.40 & 0.40 & 0.78 & 0.59 & 0.67 \\
2 & 3 & 0.46 & 0.42 & 0.44 & 0.78 & 0.59 & 0.67 \\\hline
4 & 1 & 0.54 & 0.48 & 0.51 & 0.78 & 0.59 & 0.67 \\
4 & 2 & 0.54 & 0.46 & 0.50 & 0.79 & 0.61 & 0.68 \\
4 & 3 & 0.48 & 0.38 & 0.42 & 0.78 & 0.59 & 0.68 \\ \hline
6 & 1 & 0.55 & 0.50 & 0.52 & 0.78 & 0.60 & 0.68 \\
6 & 2 & 0.58 & 0.49 & 0.53 & 0.77 & 0.59 & 0.67 \\ 
%0 & 1 & 0.40 & 0.37 & 0.39 & 0.78 & 0.60 & 0.68 \\
%0 & 2 & 0.37 & 0.35 & 0.36 & 0.77 & 0.60 & 0.68 \\
%0 & 3 & 0.50 & 0.39 & 0.44 & 0.77 & 0.60 & 0.67 \\
%2 & 1 & 0.50 & 0.48 & 0.49 & 0.77 & 0.60 & 0.67 \\
%2 & 2 & 0.40 & 0.40 & 0.40 & 0.78 & 0.59 & 0.67 \\
%2 & 3 & 0.46 & 0.42 & 0.44 & 0.78 & 0.59 & 0.67 \\
%4 & 1 & 0.54 & 0.48 & 0.51 & 0.78 & 0.59 & 0.67 \\
%4 & 2 & 0.54 & 0.46 & 0.50 & 0.79 & 0.61 & 0.68 \\
%4 & 3 & 0.48 & 0.38 & 0.42 & 0.78 & 0.59 & 0.68 \\
%6 & 1 & 0.55 & 0.50 & 0.52 & 0.78 & 0.60 & 0.68 \\
%6 & 2 & 0.58 & 0.49 & 0.53 & 0.77 & 0.59 & 0.67 \\
6 & 3 & 0.53 & 0.44 & 0.48 & 0.76 & 0.59 & 0.66 \\ \hline %\hline
\multicolumn{2}{r|}{\textit{Avgs:}}  & 0.49 & 0.43 & 0.46 & 0.77 & 0.60 & 0.67 \\
%\multicolumn{2}{r|}{\textit{Avgs:}} & 0.48 & 0.43 & 0.45 & 0.78 & 0.60 & 0.67 \\
\bottomrule
\end{tabular}
} \\
%\frame{
%\frametitle{Extrinsic Evaluation, $K = 1000$} %\label{subtab:extr-1000-0}]{
%\begin{table}
%
%\caption{ Transcriptions, No Stress Marking (TR)}
\subtable[Transcriptions with stress (TR), $K = 1000$\label{subtab:extr-1000-tr}]{
\setlength{\extrarowheight}{2pt}
\fontsize{11pt}{12pt}\selectfont
%\footnotesize
%\small
\centering
\begin{tabular}{cc|ccc|ccc}
 \toprule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{Four-stage process} & \multicolumn{3}{c}{Control} \\
$s$ & $\delta$ & P & R & F & P & R & F \\ \hline %\hline
0 & 1 & 0.36 & 0.47 & 0.40 & 0.61 & 0.68 & 0.62 \\
0 & 2 & 0.46 & 0.41 & 0.43 & 0.79 & 0.63 & 0.70 \\
0 & 3 & 0.50 & 0.37 & 0.43 & 0.80 & 0.64 & 0.71 \\ \hline 
2 & 1 & 0.50 & 0.46 & 0.48 & 0.81 & 0.65 & 0.72 \\
2 & 2 & 0.44 & 0.40 & 0.42 & 0.81 & 0.64 & 0.71 \\
2 & 3 & 0.31 & 0.56 & 0.40 & 0.40 & 0.73 & 0.52 \\ \hline
4 & 1 & 0.51 & 0.47 & 0.49 & 0.80 & 0.65 & 0.72 \\
4 & 2 & 0.46 & 0.42 & 0.44 & 0.80 & 0.64 & 0.71 \\
4 & 3 & 0.47 & 0.43 & 0.45 & 0.79 & 0.63 & 0.71 \\ \hline
6 & 1 & 0.53 & 0.49 & 0.51 & 0.81 & 0.64 & 0.72 \\
6 & 2 & 0.51 & 0.43 & 0.47 & 0.81 & 0.64 & 0.71 \\
6 & 3 & 0.59 & 0.46 & 0.52 & 0.79 & 0.64 & 0.71 \\ \hline %\hline
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.48 & 0.44 & 0.46 & 0.76 & 0.65 & 0.69 \\
 \bottomrule
\end{tabular}
} \\
%\caption{ Orthographic (O)}
\subtable[Orthographic (O), $K = 1000$\label{subtab:extr-1000-o}]{
%\footnotesize
%\small
\fontsize{11pt}{12pt}\selectfont
\setlength{\extrarowheight}{2pt}
\centering
\begin{tabular}{cc|ccc|ccc}
\toprule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{Four-stage process} & \multicolumn{3}{c}{Control} \\
$s$ & $\delta$ & P & R & F & P & R & F \\\hline %[8pt]  \hline % \midrule 
0 & 1 & 0.69 & 0.42 & 0.52 & 0.83 & 0.67 & 0.75 \\
0 & 2 & 0.67 & 0.31 & 0.42 & 0.84 & 0.68 & 0.75 \\
0 & 3 & 0.67 & 0.28 & 0.39 & 0.83 & 0.68 & 0.75 \\\hline %[5pt] \hline % \midrule
1 & 1 & 0.49 & 0.56 & 0.52 & 0.59 & 0.64 & 0.61 \\
1 & 2 & 0.53 & 0.48 & 0.51 & 0.60 & 0.64 & 0.62 \\
1 & 3 & 0.72 & 0.30 & 0.42 & 0.59 & 0.64 & 0.61 \\\hline %[2pt] \hline %\midrule
2 & 1 & 0.47 & 0.62 & 0.53 & 0.57 & 0.71 & 0.63 \\
2 & 2 & 0.46 & 0.59 & 0.52 & 0.58 & 0.72 & 0.64 \\
2 & 3 & 0.55 & 0.56 & 0.56 & 0.59 & 0.73 & 0.65 \\\hline %[8pt] \hline %\midrule
3 & 1 & 0.46 & 0.59 & 0.51 & 0.58 & 0.73 & 0.65 \\
3 & 2 & 0.43 & 0.56 & 0.49 & 0.60 & 0.70 & 0.65 \\
3 & 3 & 0.49 & 0.58 & 0.53 & 0.58 & 0.74 & 0.65 \\\hline 
 \multicolumn{2}{r|}{\textit{Avgs:}} & 0.55 & 0.49 & 0.49 & 0.65 & 0.69 & 0.66 \\\bottomrule
\end{tabular}
}
\caption{Extrinsic evaluation results at $K = 1000$}%(Second Pass: Cluster-membership threshold set at 0.8 instead of 0.5.)}
\label{tab:extr-results-1000}
\end{table}

\subsection{Extrinsic Results}
\label{sec:extr-results}
The extrinsic evaluation was itself a sort of experiment, one that sought to evaluate Multimorph according to its 
ability to positively influence a downstream application. It involved a four-stage procedure that is described in 
chapter~\ref{ch:eval}.
Basically, this process compressed Multimorph's input words by substituting original characters with atomic 
representations of morphs (i.e., morphs represented as single characters). The resulting file was then fed to 
Morfessor. The idea was to give Morfessor pre-analyzed (i.e., compressed) lists of input words and
 see if Morfessor performed better on them than on the original, untouched wordlists (i.e., the control wordlists) .
% the control group, which was in this case the original Hebrew wordlists, untouched by Multimorph.
%  
Because each
experimental combination of $s$ and $\delta$ values yielded a different word coverage, 
%---i.e., involved a somewhat different set of words---
each combination required its own control model. Thus, table~\ref{tab:extr-results-1000} shows control results for each valuation of $s$ and $\delta$.
%because the parameters $s$ and $\delta$ 
 


Table~\ref{tab:extr-results-1000} shows the results of the extrinsic evaluation at $K=1000$. 
Morfessor clearly did not perform 
better on the data that was preprocessed by Multimorph, as the control 
results were always better than the corresponding 
experimental results by a substantial margin. The closest margins between experimental and control results occur among the O models at $s = 1$ and $s = 2$. At $\langle{s}=2,\delta=3\rangle$, the experimental orthographic F-score was 0.56, and the corresponding control F-score was 0.65. 
%But even so, experimental and control results do seem to be \emph{comparable}. 
%that is,
%the experimental and control results do not seem large enough to suggest a fundamental inferiority in the 
%experimental models. 

One must bear in mind that this four-stage process is both new---it was executed for the first time in this project---and highly complex. The process's results in table~\ref{tab:extr-results-1000} are close enough to 
the control results at this early stage in its development to indicate promise, a potential for considerable improvement.
%comparable. the differences between the control and experimental do not appear large enough to   experimental and control results are 
%comparable. 
%Interestingly, the scores of the orthographic (O) models come closest to matching the control results.


%orthographic data that is inherently beneficial.

%The reader may have noticed that the tables contain the 
%results of many control datasets, one for each experimental model, in fact. This may seem strange 
%because the parameters $s$ and $\delta$ were irrelevant to the control data, untouched as it was 
%by Multimorph. This is indeed true, but the actual reason for the many control models is 
%that each experimental model yielded a different word coverage. The control datasets were made 
%to have the same words as their corresponding models covered. 

%\begin{table}[htb]
%\subtable[Transcriptions with stress (TS), $K = 1000$\label{subtab:extr-1000-ts}]{
%\small
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccc|ccc}
%\multicolumn{2}{c}{} & \multicolumn{3}{c}{4-stage process} & \multicolumn{3}{c}{Control} \\
%0 & 1 & 0.40 & 0.37 & 0.39 & 0.78 & 0.60 & 0.68 \\
%0 & 2 & 0.37 & 0.35 & 0.36 & 0.77 & 0.60 & 0.68 \\
%0 & 3 & 0.50 & 0.39 & 0.44 & 0.77 & 0.60 & 0.67 \\\hline
%2 & 1 & 0.50 & 0.48 & 0.49 & 0.77 & 0.60 & 0.67 \\
%2 & 2 & 0.40 & 0.40 & 0.40 & 0.78 & 0.59 & 0.67 \\
%2 & 3 & 0.46 & 0.42 & 0.44 & 0.78 & 0.59 & 0.67 \\\hline
%4 & 1 & 0.54 & 0.48 & 0.51 & 0.78 & 0.59 & 0.67 \\
%4 & 2 & 0.54 & 0.46 & 0.50 & 0.79 & 0.61 & 0.68 \\
%4 & 3 & 0.48 & 0.38 & 0.42 & 0.78 & 0.59 & 0.68 \\ \hline
%6 & 1 & 0.55 & 0.50 & 0.52 & 0.78 & 0.60 & 0.68 \\
%6 & 2 & 0.58 & 0.49 & 0.53 & 0.77 & 0.59 & 0.67 \\ \hline\hline
%\multicolumn{2}{r|}{\textit{Avgs:}} & 0.48 & 0.43 & 0.45 & 0.78 & 0.60 & 0.67 \\
%\end{tabular}
%}
%\subtable[Transcriptions with stress (TR), $K = 1000$\label{subtab:extr-1000-tr}]{
%\small
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccc|ccc}
%\multicolumn{2}{c}{} & \multicolumn{3}{c}{4-stage process} & \multicolumn{3}{c}{Control} \\
%0 & 1 & 0.36 & 0.47 & 0.40 & 0.61 & 0.68 & 0.62 \\
%0 & 2 & 0.46 & 0.41 & 0.43 & 0.79 & 0.63 & 0.70 \\
%0 & 3 & 0.50 & 0.37 & 0.43 & 0.80 & 0.64 & 0.71 \\ \hline
%2 & 1 & 0.50 & 0.46 & 0.48 & 0.81 & 0.65 & 0.72 \\
%2 & 2 & 0.44 & 0.40 & 0.42 & 0.81 & 0.64 & 0.71 \\
%2 & 3 & 0.31 & 0.56 & 0.40 & 0.40 & 0.73 & 0.52 \\ \hline
%4 & 1 & 0.51 & 0.47 & 0.49 & 0.80 & 0.65 & 0.72 \\
%4 & 2 & 0.46 & 0.42 & 0.44 & 0.80 & 0.64 & 0.71 \\
%4 & 3 & 0.47 & 0.43 & 0.45 & 0.79 & 0.63 & 0.71 \\ \hline
%6 & 1 & 0.53 & 0.49 & 0.51 & 0.81 & 0.64 & 0.72 \\
%6 & 2 & 0.51 & 0.43 & 0.47 & 0.81 & 0.64 & 0.71 \\
%6 & 3 & 0.59 & 0.46 & 0.52 & 0.79 & 0.64 & 0.71 \\ \hline\hline
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.48 & 0.44 & 0.46 & 0.76 & 0.65 & 0.69 \\
%\end{tabular}
%}
%\bigskip
%\subtable[Orthographic (O), $K = 1000$ \label{subtab:extr-1000-0}]{
%\small
%\setlength{\extrarowheight}{6pt}
%\begin{tabular}{cc|ccc|ccc}
%\multicolumn{2}{c}{} & \multicolumn{3}{c}{4-stage process} & \multicolumn{3}{c}{Control} \\
%$s$ & $\delta$ & Prc & R & F & Prc & R & F \\ \hline\hline
%0 & 1 & 0.69 & 0.42 & 0.52 & 0.83 & 0.67 & 0.75 \\
%0 & 2 & 0.67 & 0.31 & 0.42 & 0.84 & 0.68 & 0.75 \\
%0 & 3 & 0.67 & 0.28 & 0.39 & 0.83 & 0.68 & 0.75 \\ \hline
%1 & 1 & 0.49 & 0.56 & 0.52 & 0.59 & 0.64 & 0.61 \\
%1 & 2 & 0.53 & 0.48 & 0.51 & 0.60 & 0.64 & 0.62 \\
%1 & 3 & 0.72 & 0.30 & 0.42 & 0.59 & 0.64 & 0.61 \\ \hline
%2 & 1 & 0.47 & 0.62 & 0.53 & 0.57 & 0.71 & 0.63 \\
%2 & 2 & 0.46 & 0.59 & 0.52 & 0.58 & 0.72 & 0.64 \\
%2 & 3 & 0.55 & 0.56 & 0.56 & 0.59 & 0.73 & 0.65 \\ \hline
%3 & 1 & 0.46 & 0.59 & 0.51 & 0.58 & 0.73 & 0.65 \\
%3 & 2 & 0.43 & 0.56 & 0.49 & 0.60 & 0.70 & 0.65 \\
%3 & 3 & 0.49 & 0.58 & 0.53 & 0.58 & 0.74 & 0.65 \\ \hline\hline
% \multicolumn{2}{r|}{\textit{Avgs:}} & 0.55 & 0.49 & 0.49 & 0.65 & 0.69 & 0.66 \\
%\end{tabular}
%}
%\caption{Extrinsic evaluation results at $K = 1000$.}
%\label{tab:extr-results-1000}
%\end{table}

%\frame{
%\frametitle{Extrinsic Evaluation, $K = 1000$} %\label{subtab:extr-1000-0}]{

\section{Summary}
To sum up, in the qualitative analysis, we found clusters corresponding to roots, vowel patterns, as well as concatenative affixes. The orthographic (O) models produced more clusters corresponding to consonantal roots than the transcriptional models TS and TR, and in general, their clusters were more coherent as well. But since the orthographic representation favors consonants at the expense of vowels (see chapter~\ref{ch:experi}), this was perhaps to be expected. We also saw all types of models represented nonconcatenative phenomena through precedence features, and that at least two precedence features were necessary to represent a tri-consonantal root, whereas one precedence feature sufficed to represent most vowel patterns.

According to the intrinsic results, the O models performed at a level on par with the TS models. In the extrinsic results, the orthographic models were the best, coming closest to matching those of the control model.
The relative success of the orthographic models in the extrinsic evaluation reinforces the idea that there is something about the orthographic representation that is inherently and unexpectedly beneficial---that more and/or better information is present in the orthographic representation than what seems immediately apparent. The quantitative results, like the qualitative results, suggest that there are limits to the usefulness of positional features. At the same time, however, the affix-length setting $s=1$ 
%(the `affix length' parameter for positional features) 
tended to lead to substantially results better than $s=0$.

In the next chapter, we will consider the results from the perspective of the thesis as a whole.
